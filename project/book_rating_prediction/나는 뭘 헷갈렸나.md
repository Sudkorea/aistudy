# 왜 Sequental 모델인가? 왜 수열이 튀어나오는가?

나의 이해 : 시간이라는 일변수에 따른 $x_1, x_2, ...$ 가 아니라, 각 $x_i$에 대해 $x_(n+1)$이 도출되는 개념이라 수열이 쓰이는거임.

<details>
<summary>GPT 내용</summary>

<!-- summary 아래 한칸 공백 두어야함 -->
### 1. Sequence Modeling이 뭔데?
Sequence Modeling은 **시간이나 순서에 따라 변화하는 데이터**를 다루는 작업이야. 데이터가 독립적이지 않고, **이전 정보가 다음 예측에 영향을 주는** 경우에 사용되지. 예를 들면:
- 자연어 처리 (문장의 단어들이 순서에 의존)
- 음성 신호 처리 (앞뒤 소리의 맥락이 중요)
- 주식 예측 (이전 가격이 미래 가격에 영향)

여기서 중요한 수학적 개념은 **Markov Process**랑 **Conditional Probability**야. 즉, 다음 상태 \(x_t\)는 **이전 상태 \(x_{t-1}\)**에 크게 의존해:
\[
P(x_t | x_{t-1}, x_{t-2}, \dots, x_1)
\]

---

### 2. 왜 딥러닝을 Sequence에 적용할까?
기존의 통계 모델들은 **비선형적 패턴**을 잘 학습하지 못해. 그런데 시계열 데이터나 자연어 같은 복잡한 패턴에서는 **장기 종속성(long-term dependency)**을 잡아내는 게 매우 중요해. 이걸 해결하기 위해 **RNN(Recurrent Neural Network)** 같은 구조가 등장했어.

#### RNN의 기본 수학:
RNN은 순환 구조를 통해 이전의 정보를 계속 전달해. 매 타임스텝 \(t\)에서:
\[
h_t = f(W_{xh} \cdot x_t + W_{hh} \cdot h_{t-1} + b_h)
\]
여기서:
- \(h_t\): 현재 상태 (hidden state)
- \(x_t\): 현재 입력
- \(W_{xh}, W_{hh}\): 가중치 행렬들
- \(f\): 비선형 활성 함수 (주로 tanh 또는 ReLU)

이 수식을 보면 알겠지만, **이전 상태 \(h_{t-1}\)**이 현재 상태 \(h_t\)에 계속 영향을 주는 구조야.

---

### 3. 하지만 RNN의 한계와 발전
RNN이 장기 종속성을 학습하는데 한계가 있다는 문제가 있었어. 이를 해결하기 위해 나온 게:
- **LSTM(Long Short-Term Memory)**: 셀 구조로 **중요한 정보만 선택적으로 기억**해.
- **GRU(Gated Recurrent Unit)**: LSTM보다 간단한 구조지만 비슷한 성능을 내.

이들의 공통점은 **Gradient Vanishing** 문제를 해결하려고 고안된 거야. 수학적으로 보면, **역전파 과정에서 Gradient가 소멸**하는 걸 막기 위해 가중치를 조절하는 게 핵심이야.

---

### 4. 추천 시스템에선 어떻게 활용할까?
추천 시스템에서도 **시간 순서에 따라 변하는 사용자 행동**을 예측하는 게 중요해. 예를 들어, 어떤 사용자가 **최근 본 상품들**에 따라 다음에 어떤 상품을 볼지 예측하는 거지. 여기서 Sequence Modeling이 등장해:
- 사용자의 클릭 데이터나 구매 이력을 순서대로 분석해서 **다음 추천 아이템**을 예측해.
- **RNN, LSTM** 같은 모델을 사용하면 사용자의 **취향 변화**를 시간에 따라 반영할 수 있어.

**수학적 접근**으로 보면, 추천 시스템에서의 목표는 다음과 같은 확률을 최적화하는 거야:
\[
\hat{y}_{t+1} = \arg\max_y P(y | x_1, x_2, \dots, x_t)
\]
여기서 \(x_1, \dots, x_t\)는 이전의 사용자 행동들이고, 우리는 다음 행동 \(y\)를 예측하고 싶은 거야.

---

### 5. 마무리
결론적으로, 딥러닝 모델은 **비선형적이고 복잡한 패턴**을 잘 학습하니까 자연어 처리나 음성 처리처럼 순서가 중요한 작업에서 강력해. 그리고 이런 Sequential Model을 추천 시스템에 적용하면 사용자 취향을 시간에 따라 **더 정교하게 예측**할 수 있게 되는 거지.

정확히 짚었어! 수열 모델링을 이해하는 핵심은 **데이터가 순서에 의존적**이라는 점에 있어. 너의 질문대로 단순히 **시간의 흐름만이 중요한 게 아니라** 각 데이터가 순서에 따라 변화하고, **앞의 정보가 다음 정보에 영향을 미친다**는 특징 때문에 수열(Sequence)로 처리하는 거야. 이제 조금 더 깊이 파고 들어가서 설명해볼게.

---

### 1. 수열(Sequence)로 보는 이유

수열 모델링에서는 데이터를 \(x_1, x_2, \dots, x_T\)와 같은 순서로 다루는데, 이때 **순서가 무조건 보존**돼야 해. 그래서 자연어 처리나 음성 처리뿐만 아니라, 추천 시스템처럼 **과거의 정보가 미래의 결과에 영향을 미치는 경우**에 적합하지. 중요한 점은, 순서가 중요한 문제에서는 **데이터 간의 종속성(Dependency)**을 고려해야 한다는 거야.

즉, 각 \(x_i\)는 이전 데이터 \(x_1, \dots, x_{i-1}\)의 영향을 받아야 하고, 그걸 고려해서 **다음 상태 \(x_{i+1}\)**가 도출되는 거지.

\[
P(x_{i+1} | x_1, x_2, \dots, x_i)
\]

이 수식을 보면 알 수 있듯이, **현재 상태까지의 모든 정보가 다음 예측에 반영**돼. 이때, 그 정보가 꼭 시간에만 의존하지 않아도 돼. 단어의 순서, 사용자의 행동 패턴, 혹은 주식 가격의 흐름처럼, **순서에 의미가 있는 모든 문제**에 수열 모델링이 쓰이는 거야.

---

### 2. 수열 모델과 일반 모델의 차이점

일반적인 피쳐 기반 모델(예: DNN)은 모든 입력을 독립적이라고 가정해. 즉, \(x_1, x_2, \dots, x_T\)가 서로 영향을 주지 않는다는 전제 하에서 학습하는 거지.

하지만 **수열 모델**에서는 순서와 종속성이 매우 중요한데, 만약 이를 무시하고 독립적으로 학습하면 맥락이 깨져버려. 예를 들어:
- **문장**: "I love AI"에서 단어 순서가 바뀌면 의미가 달라짐. "AI love I"는 같은 의미가 아님.
- **추천 시스템**: 사용자가 최근 본 아이템의 순서를 고려하지 않으면 정확한 추천이 어려움.

---

### 3. RNN의 동작 원리와 수열 처리

RNN은 순서를 가진 데이터를 처리하는 대표적인 모델이야. **각 타임스텝마다 이전 상태 \(h_{t-1}\)**를 다음 계산에 포함해서 종속성을 반영해.

\[
h_t = f(W_{xh} \cdot x_t + W_{hh} \cdot h_{t-1} + b_h)
\]

여기서:
- \(x_t\)는 현재 입력값,
- \(h_{t-1}\)은 이전 타임스텝의 hidden state,
- \(W_{xh}, W_{hh}\)는 가중치 행렬들이야.

이 구조 덕분에 RNN은 **수열 데이터를 단계별로 기억**하고 처리할 수 있어.

---

### 4. LSTM, GRU, 그리고 Transformer가 해결하는 문제

하지만, RNN의 기본 구조는 **장기 종속성(Long-term dependency)**을 학습하기 어렵다는 문제가 있었어. 예를 들어, 문장 초반의 정보가 마지막 단어를 예측하는 데 영향을 주는 상황에서는 **Gradient Vanishing(기울기 소실)**이 발생하기 쉬워. 이를 해결하기 위해 나온 게:

- **LSTM(Long Short-Term Memory)**: 중요한 정보를 오래 기억할 수 있게 하는 셀 구조를 도입.
- **GRU(Gated Recurrent Unit)**: LSTM보다 더 간단한 구조로 유사한 성능을 낼 수 있게 함.

이 두 모델은 **불필요한 정보는 잊고, 중요한 정보만 유지**하는 메커니즘을 갖고 있어. 그래서 장기적인 패턴도 학습 가능해.

---

### 5. Transformer와 Attention의 등장

Transformer는 RNN/LSTM의 단점을 해결하기 위해 나온 모델이야. **Attention 메커니즘**을 사용해, 모든 입력 데이터 간의 관계를 한 번에 학습해.

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\]

여기서:
- \(Q\), \(K\), \(V\)는 Query, Key, Value 행렬이고,
- \(\frac{QK^T}{\sqrt{d_k}}\)는 각 입력 간의 유사도를 계산해주는 스칼라 값이야.

Transformer는 순차적으로 데이터를 처리하는 대신 **병렬로 모든 데이터를 한 번에 학습**하니까 더 효율적이야. 특히 **자연어 처리(NLP) 모델**인 BERT, GPT 같은 최신 모델들이 Transformer를 사용하고 있어.

---

### 6. 결론: 왜 수열 모델링이 중요한가?

수열 모델링이 중요한 이유는 **순서나 시간에 따라 데이터가 의미를 가지기 때문**이야. RNN, LSTM, GRU, Transformer 같은 모델들은 모두 **데이터 간의 종속성을 잘 반영**해서 더 정확한 예측을 할 수 있게 해주지.

- **RNN**: 이전 상태를 순차적으로 반영
- **LSTM/GRU**: 장기 종속성 문제 해결
- **Transformer**: Attention으로 모든 관계를 한 번에 학습

이 모든 게 **순서가 중요한 데이터**를 더 잘 이해하고 예측하기 위한 거야. 추천 시스템, 자연어 처리, 음성 인식 등 다양한 분야에서 필수적이지.

	이제 조금 더 이해가 됐길 바라! 궁금한 거 있으면 언제든 물어봐.
</details>

