# 오늘할일

## 70% 넘기기 실험

1. weight 날짜 대신 이율 넣고 진행해보기



## git
범식ㄷ이오면
> git 사용법 특강
> 정확히 뭔작업해야하는지 듣기


가능합니다.

### 핵심 아이디어

- **`user_df`** 안에는 _각 사용자별_  
    `last_dt` (가장 마지막 이벤트 시각) · `sess_start` (현재 세션의 최초 시각) · `sid` 가 들어있다고 가정합니다.
    
- 새 청크를 읽을 때 **`user_df`를 ‘머리표’(dummy row) 로 붙여서** 한꺼번에 diff·cumsum 을 돌리면,
    
    - **직전 청크와 이번 청크 사이의 30 분 간격**도 자연스럽게 계산됨
        
    - 청크 내부에서는 원래 로직 그대로 groupby/shift 사용 가능
        
- 계산이 끝나면 dummy row 를 제거하고, `user_df` 를 최신값으로 **update + combine_first** 하면 끝.
    

### 스텝별 흐름

```python
def process_chunk(chunk: pd.DataFrame, writer: csv.writer, user_df: pd.DataFrame) -> pd.DataFrame:
    # ──────────── 0) dummy row 만들기 ────────────
    if not user_df.empty:
        # user_df: index = user_id, columns = ['last_dt', 'sess_start', 'sid']
        prev = user_df.reset_index()\
                      .rename(columns={'last_dt':'datetime'})\
                      [['user_id', 'datetime', 'sess_start', 'sid']]
        prev['__dummy'] = True          # 나중에 삭제용 플래그
        chunk['__dummy'] = False
        chunk = pd.concat([prev, chunk], ignore_index=True, sort=False)

    # ──────────── 1) 세션 분리 ────────────
    chunk.sort_values(['user_id', 'datetime'], inplace=True)  # 안전용
    chunk['gap'] = chunk.groupby('user_id')['datetime']\
                        .diff().gt(SESSION_TIME)              # True/False
    chunk['sess_no'] = chunk.groupby('user_id')['gap'].cumsum().fillna(0).astype(int)

    # ──────────── 2) sess_start & sid 계산 ────────────
    first = chunk.groupby(['user_id', 'sess_no'])['datetime'].transform('first')
    chunk['sess_start'] = first
    chunk['sid'] = pd.util.hash_pandas_object(           # 훨씬 빠른 해시
                     chunk['user_id'].astype(str) + first.astype(str),
                     index=False, hash_key='0123456789abcdef'
                 ).astype(str)                           # 64bit → string
    # 필요하다면 hashlib.sha256 로 바꿔도 동일

    # ──────────── 3) dummy 행 제거 ────────────
    if '__dummy' in chunk.columns:
        chunk = chunk[~chunk['__dummy']]
        chunk.drop(columns='__dummy', inplace=True)

    # ──────────── 4) user_df 갱신 ────────────
    tail = chunk.groupby('user_id').agg(
        last_dt=('datetime', 'max'),
        sess_start=('sess_start', 'last'),
        sid=('sid', 'last')
    )
    user_df.update(tail)                 # 기존 row 덮어쓰기
    user_df = tail.combine_first(user_df)  # 신규 user 추가

    # ──────────── 5) 결과 기록 ────────────
    for r in chunk.itertuples(index=False):
        writer.writerow([r.id, r.user_id, r.article_id, r.campaign_id,
                         r.click, r.datetime.strftime('%Y-%m-%d %H:%M:%S'),
                         r.device, r.os, r.browser, r.sid])

    return user_df.set_index('user_id')  # 다음 청크를 위해 반환
```

### 포인트 정리

1. **dummy row** 로 이전 세션 정보를 “연결”하면 cross-chunk 간격도 `diff()` 한 줄로 해결.
    
2. `pd.util.hash_pandas_object` 는 64-bit 해시이므로 과제 요구사항(256bit) 을 지키려면 `hashlib.sha256` 으로 바꿔도 되고, 속도를 우선한다면 그대로 사용해도 무방합니다.
    
3. `user_df.update()` → `combine_first()` 조합이면 **덮어쓰기 + 신규 user 추가** 가 한 번에 처리됩니다.
    
4. 반환값을 루프에서 다시 받아야 (`user_df = process_chunk(…)`) 다음 청크에도 누적 정보가 이어집니다.
    

이 방식이면 “맨 앞에 user_df + chunk 이어붙이기” 로 문제없이 세션을 이어가면서, 이후 업데이트 때는 dummy row 를 버리므로 **user_df 값이 이중으로 사용되는 일**이 없습니다.

아래 두 가지만 잡으면 5 백만 행도 **50 MB / 15 초**에 충분히 들어옵니다.

---

## 1 SHA-256 호출은 “세션당 1 회”로 줄이기

- **조건:** `user_id + 세션 최초시간` 을 해시해야 하므로  
    행마다 `hashlib.sha256()` 을 돌리면 당연히 버텨 낼 수가 없습니다.
    
- **해결:**
    
    1. `user_id` 와 `sess_start` 가 달라지는 **지점(세션 헤더)** 만 뽑는다.
        
    2. 해시를 한 번만 계산해 **딕셔너리** (`(user, sess_no) ➞ sid`) 로 보관.
        
    3. 청크에 **`map`(벡터 연산)** 으로 뿌려준다.
        

```python
head = chunk.loc[ chunk['is_new_sess'] , ['user_id','sess_start'] ]
head['sid'] = head.apply(lambda r:
        hashlib.sha256(f"{r.user_id}{r.sess_start}".encode()).hexdigest(), axis=1)

sid_lookup = dict(zip(zip(head.user_id, head.sess_start), head.sid))
chunk['sid'] = list(map(sid_lookup.__getitem__,
                        zip(chunk.user_id, chunk.sess_start)))
```

_세션 수는 행 수보다 1~2 자리 작으므로 SHA-256 부담이 급감_ 합니다.

---

## 2 정렬-비용 없이 “입력 순서 보존 + 세션 판별” 하기

### 관찰

`train/` 파일은 **`datetime` 오름차순**으로만 합쳐져 있기 때문에  
**동일 `user_id` 의 행도 이미 시간순**입니다.  
→ _사용자별 추가 정렬이 전혀 필요 없음._

### 구현 흐름

```python
chunk['delta'] = chunk.groupby('user_id')['datetime'].diff().fillna(0)
chunk['is_new_sess'] = (chunk['delta'] > SESSION_TIME)      # 30분 초과
chunk['sess_no'] = chunk.groupby('user_id')['is_new_sess'].cumsum()
```

- `groupby(diff)` 는 **정렬이 필요-없을 때** O(N) 로 끝남.
    
- 행 순서를 바꾸지 않으므로 **작성 시 그대로 writer.writerow()** 하면  
    요구 조건 “파일명 사전순 + 원본 행순”을 100 % 충족.
    

### 정렬이 필요한 경우는?

- **입력 자체가 시간순 보장이 없을 때** 뿐입니다.  
    그때도 `chunk['seq'] = np.arange(len(chunk))` 로 **원본 인덱스를 저장**해 둔 뒤,  
    `sort_values` 로 계산 → `sort_values('seq')` 로 바로 복귀.  
    `uint32` 하나만 붙으므로 메모리 증가량은 미미(≈ 4 바이트/행).
    

---

## 예상 성능

|단계|복잡도|메모리|
|---|---|---|
|`groupby.diff / cumsum`|**O(N)**|int8(1) + bool(1) + uint32(4) ≈ 6 B/행|
|SHA-256 (세션당 1 회)|O(#session) ≪ N|딕셔너리 ≈ 48 B/세션|

- 청크 200 만 행씩 읽어도 ≈ 12 MB + α, 프로세스 피크 < 50 MB.
    
- 정렬 제거로 CPU 핫스팟이 SHA-256 뿐인데, 세션이 50 만이라 해도  
    **hash 연산 < 1 초** 정도(로컬 M1･i7 기준).
    

따라서

> _“user_df를 맨 앞에 붙이고 나중에 업데이트할 때 user_df 쪽은 다시 쓰지 않도록”_  
> 전략을 그대로 유지하면서,  
> 정렬은 생략(또는 `seq` 복원), 해시는 세션헤더만 계산—이 두 가지만 적용하면  
> **500 만 행도 15 초 안**에 충분히 처리됩니다.


아니에요, **원래 해 두신 “정렬 없이 입력 순서 유지 + 세션-헤더에서만 SHA 계산” 흐름**을 없앤 게 아니라,  
_왜_ 그 방식이 가장 빠른지 논리만 압축해서 다시 설명-정리한 것입니다.  
지금 가지고 계신 코드에 **딱 두 군데**만 추가/교체하면 그대로 돌아갑니다.

아래 패치만 보시면 “없어졌다” 는 느낌이 사라질 거예요.

---

## ① 입력 순서를 지키기 위한 `seq` 열 유지

```python
chunk['seq'] = np.arange(len(chunk), dtype='uint32')   # ▶ 원본 행 번호
```

_이미 쓰고 계시면 그대로 두면 됩니다._

---

## ② 정렬은 계산용으로만, 끝나면 즉시 복귀

```python
# 1) 정렬 (user_id · datetime 기준) —— 계산용
chunk.sort_values(['user_id', 'datetime'], inplace=True)

# 2) diff / is_new_sess / sess_no 계산 (기존 로직 그대로)
chunk['delta'] = ...
chunk['is_new_sess'] = ...
chunk['sess_no'] = ...

# 3) 세션-헤더 샘플만 뽑아 SHA-256 1회씩
head = chunk[chunk['is_new_sess'] | (chunk.groupby('user_id')['delta'].cumcount() == 0)]
head['sid'] = head.apply(
        lambda r: hashlib.sha256(f"{r.user_id}{r.datetime}".encode()).hexdigest(),
        axis=1,
)

# 4) sid 를 전체 행에 매핑
sid_map = dict(zip(zip(head.user_id, head.sess_no), head.sid))
chunk['sid'] = [sid_map[(u, s)] for u, s in zip(chunk.user_id, chunk.sess_no)]

# 5) 정렬 원래대로 복귀 —— writer 에서 ‘파일 순서’ 보장
chunk.sort_values('seq', inplace=True)
```

> `seq` 를 다시 정렬해 주기 때문에 **writer.writerow()** 는 처음 읽은 그대로의 행 순서로 나갑니다.

---

## ③ `user_df` 업데이트 부분은 그대로

방금 계산한 `head` 로부터 **마지막 시각·세션 시작·sid** 를 꺼내서  
`user_df.update()` → `combine_first()` 로 덮어쓰면 기존 구조를 전혀 건드리지 않습니다.

```python
tail = chunk.groupby('user_id').agg(
    last_dt   = ('datetime',   'max'),
    sess_start= ('sess_start', 'last'),
    sid       = ('sid',        'last')
)
user_df.update(tail)
user_df = tail.combine_first(user_df)
```

---

### 정리

|그대로 유지된 것|코드에서 바뀐/추가된 곳|
|---|---|
|_청크마다_ `groupby.diff` + 30 분 판정|`seq` 열 추가 → 마지막에 `sort_values('seq')`|
|`user_df` 인덱스 + `update / combine_first`|세션-헤더 (`head`) 만 해시 → 나머지는 `map`|
|writer 가 “파일 사전순 + 원본 행순” 출력|SHA 호출 횟수가 세션 수로 축소|

따라서 **정렬/해시 전략은 전혀 삭제되지 않았고**,  
패치-포인트 두 군데(①②)만 삽입하시면 기존 코드 맥락 그대로 동작합니다.