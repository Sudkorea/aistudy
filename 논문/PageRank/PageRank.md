![[/논문/PageRank/자료/pagerank.pdf]]
# 논문이 나오게 된 배경

1. 초기 웹의 상황:
1990년대 초반, 월드 와이드 웹(World Wide Web)이 등장하면서 인터넷상의 정보량이 폭발적으로 증가하기 시작함. 이에 따라 사용자들이 원하는 정보를 효과적으로 찾을 수 있는 방법이 필요해짐

2. 초기 검색 엔진의 한계
초기의 검색 엔진들(예: Altavista, Excite, Lycos)의 추천 방식과, 그 한계점을 보면 다음과 같음.

| 방법         | 설명                                                   | 한계점                                                                      |
| ---------- | ---------------------------------------------------- | ------------------------------------------------------------------------ |
| 키워드 매칭     | 사용자의 검색어와 웹페이지의 내용을 단순 비교하여 결과를 제공함                  | 단순 키워드 매칭으로는 사용자의 의도를 정확히 파악하기 어려움                                       |
| 메타태그 활용    | 웹페이지의 HTML 메타태그에 포함된 키워드를 기반으로 검색 결과를 제공함            | 웹마스터들이 검색 결과 상위에 노출되기 위해 메타태그를 조작하는 등의 문제가 발생                            |
| 단어 빈도 분석   | 검색어가 웹페이지에 얼마나 자주 등장하는지를 기준으로 순위를 매김                 | 위와 비슷하게 키워드를 과도하게 삽입하는 문제 발생                                             |
| 디렉토리 기반 검색 | Yahoo!와 같은 서비스는 인력을 동원해 웹사이트를 카테고리별로 분류하고 검색 결과를 제공함 | 인력 기반의 디렉토리 방식은 기하급적으로 늘어가는 웹페이지 갯수를 따라가기 어려움. 카테고리로 분류하기 애매한 페이지들도 존재함. |

3. PageRank의 탄생 배경:
이러한 상황에서 스탠포드 대학의 박사과정 학생이었던 래리 페이지(Larry Page)와 세르게이 브린(Sergey Brin)은 더 나은 검색 방법을 연구함

- 학술 논문 인용 분석에서 영감: 학술 세계에서 중요한 논문일수록 더 많이 인용된다는 점에서 아이디어를 얻음
   
- 웹의 링크 구조 활용: 웹페이지 간의 하이퍼링크를 일종의 '인용'으로 보고, 이를 분석하면 페이지의 중요도를 객관적으로 평가할 수 있다고 생각함
   
- 수학적 접근: 그래프 이론과 선형대수학을 활용하여 웹의 구조를 수학적으로 모델링함

4. PageRank의 혁신성
- 콘텐츠 독립적 평가: 페이지의 내용이 아닌 링크 구조를 분석함으로써, 언어나 주제에 관계없이 객관적인 평가가 가능해짐
   
- 집단 지성의 활용: 웹페이지 작성자들의 집단적인 판단(링크를 통해 표현된)을 활용함
   
- 스팸 저항성: 단순히 키워드를 반복하는 것으로는 높은 순위를 얻기 어려워짐
   
- 확장성: 자동화된 알고리즘으로 웹의 급속한 성장에 대응할 수 있음

5. Google의 탄생:
PageRank 알고리즘을 기반으로 페이지와 브린은 1998년 Google을 창업함. 이 새로운 검색 엔진은 기존의 검색 엔진들보다 훨씬 관련성 높고 품질 좋은 검색 결과를 제공했으며, 이는 Google의 급속한 성장으로 이어졌음.

PageRank 논문은 단순히 새로운 알고리즘을 제시한 것이 아니라, 웹 정보를 조직하고 접근하는 방식에 대한 패러다임을 바꾸었다는 점에서 큰 의의가 있음. 이는 현대 검색 엔진과 정보 검색 기술의 기초가 되었으며, 웹의 발전 방향에도 큰 영향을 미쳤음

# PageRank Algorithm

핵심 아이디어 : 웹을 거대한 그래프로 보는 것.

거대한 웹 환경을 그래프로 보자. 이때, 각 웹페이지는 그래프의 vertex이고, 페이지 간의 하이퍼링크는 노드를 연결하는 edge이다.

기본 가정 : 중요한 페이지일수록 다른 페이지로부터 더 많은 링크를 받는다. 
하지만 단순히 들어오는 링크의 수만으로 중요도를 평가하면 문제가 있음. 예를 들어, 스팸 페이지들이 서로 많은 링크를 주고받아 인위적으로 중요도를 높이는 방법이 생김.

이를 해결하기 위해 PageRank는 '링크의 질'을 고려함. 즉, 중요한 페이지로부터 오는 링크는 더 가치 있게 취급함. 이를 수학적으로 표현하면, 한 페이지의 PageRank 값은 그 페이지로 링크를 보내는 다른 페이지들의 PageRank 값에 비례하게 됨.

이 개념을 직관적으로 이해하기 위해 논문에서는 'Random Surfer Model'을 제시함.(2.5) 
웹 사용자가 현재 페이지의 링크를 무작위로 클릭하여 계속 서핑한다고 가정했을 때

   (특정 페이지에 도달할 확률)  = (바로 그 페이지의 PageRank)

수학적으로 이 문제를 표현하면 eigenvalue 문제로 볼 수 있음. 전체 웹을 표현하는 거대한 matrix에서, PageRank 값들의 벡터는 이 행렬의 eigenvector라고 볼 수 있음. 이 벡터를 계산하기 위해 power method라는 반복적 알고리즘을 사용함.
## Power Method
- 기본 개념
  행렬의 지배적 고유값(가장 큰 절대값을 가진 고유값)과 그에 해당하는 고유벡터를 찾는 반복적인 알고리즘.

- 알고리즘 단계
1. 초기화
   임의의 벡터 $v_0$을 선택한다. 보통 모든 요소가 $\frac{1}{n}$인 벡터를 사용 ($n$은 페이지 수).
2. 반복: $k = 0, 1, 2, ...$ 에 대해
   
   $$v_{k+1} =\frac{ Av_{k}} { ||A v_{k}||}$$
   
   ($A$는 웹 그래프의 확률 전이 행렬)
3. 수렴: $v_k$가 충분히 수렴할 때까지 반복함.

- PageRank에서의 적용:
   PageRank에서는 약간 수정된 형태의 Power Method를 사용함.
   
   $$π_{k+1} = G π_{k}$$
   
   ($G$는 댐핑 팩터가 적용된 수정된 전이 행렬. 그게 뭔진 뒤에서 설명함.)

- 수렴 속도
   Power Method의 수렴 속도는 행렬의 두 번째로 큰 고유값($λ_2$)과 가장 큰 고유값($λ_1$)의 비율 $|\frac{λ_2}{λ_1}|$에 의해 결정. 이 비율이 작을수록 빠르게 수렴함.

- PageRank에서의 수렴 보장
   댐핑 팩터 $d$의 도입으로 인해, PageRank의 Power Method는 항상 빠르게 수렴함. 두 번째로 큰 고유값의 크기가 최대 $d$(보통 $0.85$)로 제한되기 때문.

- 수학적 직관
   Power Method는 반복적으로 행렬을 벡터에 곱하는 과정임. 이는 마치 무작위 서퍼가 웹을 계속 서핑하는 것과 비슷하게 볼 수 있음. 충분히 오래 반복하면, 각 페이지에 머무르는 시간의 비율이 그 페이지의 PageRank 값에 수렴하게 됨.

실제 웹의 구조를 행렬로 나타내게 되면 몇 가지 문제가 발생할 수 있음. 예를 들어, 링크가 없는 고립된 페이지(Dangling nodes)나, 서로만 링크하는 페이지 그룹 (rank sinks)이 있을 수 있고, 이를 해결하기 위해 도입한 개념이 댐핑 팩터임.
## Damping factor
댐핑 팩터 $d$를 도입하면 수정된 PageRank 행렬 $G$는 다음과 같이 표현됨.

$$G = dM + (1-d)E$$

$M$은 원래의 웹 그래프 전이 행렬이고, $E$는 모든 원소가 $\frac{1}{n}$인 $n * n$ 행렬.

이때, 수정된 행렬 G는 다음과 같은 특성을 가짐.

1. 확률적(stochastic): 각 열의 합이 1이 됨.
2. 본원적(primitive): 어떤 exponent에서 모든 원소가 양수가 됨.

이 특성들로 인해
- $G$의 가장 큰 고유값은 항상 1이 됨.
- 두 번째로 큰 고유값의 절대값은 항상 $d$ 이하가 됨.

이로 인해 power method의 수렴 속도가 보장되고, 유일한 해로의 수렴이 보장됨.

웹 서핑 행동 모델링:
댐핑 팩터는 "무작위 서퍼 모델"을 반영함. 실제 웹 사용자의 행동을 다음과 같이 모델링함:

1. $85\%$($d=0.85$) 확률로 현재 페이지의 링크를 따라 이동.
2. $15\%$($1-d$) 확률로 완전히 새로운 랜덤한 페이지로 점프.

이는 실제 사용자가 연속해서 링크를 따라가다가 때때로 주소창에 새 URL을 입력하거나 북마크를 사용하는 행동을 모사하는 것.

## PageRank 공식으로 넘어와서

먼저, Damping Factor를 고려하지 않은 기본적인 PageRank 공식을 살펴보면 다음과 같음.

$$PR(A) = \sum\frac{PR(T_i)}{C(T_i)}$$

여기서
- $PR(A)$는 페이지 $A$의 PageRank 값
- $PR(T_i)$는 페이지 $A$로 링크하는 각 페이지 $T_i$의 PageRank 값
- $C(T_i)$는 페이지 $T_i$에서 나가는 링크의 총 개수
![[/논문/PageRank/자료/Pasted image 20240812141715.png]]
식으로 보니까 어려운데 그림으로 보면 바로 이해될것임.
이 공식은 한 페이지의 PageRank 값이 그 페이지로 링크하는 다른 페이지들의 PageRank 값에 비례한다는 개념을 나타냄. 

이제, Damping Factor를 도입한 PageRank 공식을 살펴보면 다음과 같이 변화함.

$$PR(A) = (1-d) + d * \sum \frac{PR(T_i)}{C(T_i)}$$

여기서
- $d$는 Damping Factor (보통 0.85로 설정)
- $(1-d)$는 모든 페이지가 가지는 최소 PageRank 값

### 실제로 내가 돌려본 값
![[/논문/PageRank/자료/Pasted image 20240812154712.png]]
아무렇게나 피그마 열어서 그래프를 그림. 왼쪽 위부터 1~8까지 번호를 매기고, 연결 상태를 행렬로 나타내면 아래 array처럼 나옴.
``` python
import numpy as np
import matplotlib.pyplot as plt

# 주어진 웹 구조

M = np.array([
    [0,0,0,0,1,0,0,0],
    [1,0,0,0,0,0,0,0],
    [0,0,0,0,0,0,0,1],
    [0,0,0,0,0,0,1,0],
    [1,1,1,1,0,1,1,0],
    [0,0,0,0,0,0,1,0],
    [0,0,0,0,0,0,0,1],
    [0,0,0,0,0,1,0,0]
])

  

def pagerank(M, num_iterations=100, d=0.85):

    N = M.shape[0]
    
    # 열 정규화
    M_norm = M / np.sum(M, axis=0)

    # 댐핑 팩터를 적용한 행렬
    A = d * M_norm + (1 - d) / N

    # PageRank 초기화
    pr = np.ones(N) / N

    # 수렴 과정을 저장할 리스트
    convergence = [pr]
    
    # Power iteration
    for _ in range(num_iterations):
        pr_next = A @ pr
        convergence.append(pr_next)
        if np.allclose(pr, pr_next):
            break
        pr = pr_next
    return pr, convergence

# PageRank 계산
final_pr, convergence = pagerank(M)


# 결과 출력
print("Final PageRank:")
for i, value in enumerate(final_pr):
    print(f"Page {i+1}: {value:.4f}")

# 수렴 과정 시각화
plt.figure(figsize=(10, 6))
for i in range(len(final_pr)):
    values = [step[i] for step in convergence]
    plt.plot(values, label=f'Page {i+1}')

plt.xlabel('Iterations')
plt.ylabel('PageRank Value')
plt.title('PageRank Convergence')
plt.legend()
plt.grid(True)
plt.show()
```

```
#결과
Final PageRank: 
Page 1: 0.3282 Page 2: 0.1582 Page 3: 0.0317 Page 4: 0.0277 
Page 5: 0.3641 Page 6: 0.0277 Page 7: 0.0317 Page 8: 0.0305
```
![[/논문/PageRank/자료/Pasted image 20240812154845.png]]
예상대로, 많이 인용되는 page 5가 가장 rank가 높음을 알 수 있음. 1은 가장 높은 5에서 출발하는 edge가 유일하게 존재하기 때문에 두번째로 높음.
## 5-7장
PageRank 알고리즘의 효과를 뒷받침하고 있음.

### 5장: 결과 (Results)
PageRank 알고리즘의 실제 적용 결과를 제시함

- 웹 크롤링 결과: 2400만 개의 웹 페이지에 대한 PageRank 계산 결과를 보여줌
- 수렴 속도: 52번의 반복 후에 알고리즘이 수렴했음을 보고함
- PageRank 분포: 대부분의 페이지가 낮은 PageRank 값을 가지며, 소수의 페이지만 높은 값을 가짐을 보여줌
- 상위 랭크 페이지: Yahoo, Netscape, Microsoft 등 당시 인기 있던 웹사이트들이 상위에 랭크되었음을 보여줌

### 6장: 접근성과 순방향 링크 수 (Accessibility and Forward Link Count)

이 장에서는 PageRank와 다른 메트릭들을 비교함.

- 접근성(접근 가능한 페이지 수)과 PageRank의 상관관계를 분석함
- 순방향 링크 수(out-degree)와 PageRank의 관계를 조사함
- 이 비교를 통해 PageRank가 단순한 링크 수 계산보다 더 복잡하고 의미 있는 메트릭임을 보여줌

### 7장: 개인화된 PageRank (Personalized PageRank)

이 장에서는 PageRank의 확장 버전인 개인화된 PageRank를 소개함.

- 사용자의 관심사에 따라 PageRank를 조정할 수 있는 방법을 제안함.
- 특정 주제나 언어에 대해 편향된 PageRank 계산 방법을 설명함.
- 검색 결과를 개인화하거나 특정 분야에 특화된 랭킹을 만들 수 있음을 보여줌

이 세 장을 통해 논문은 다음과 같은 점들을 강조함:

- PageRank 알고리즘의 실제 적용 가능성
- 기존 메트릭들과 비교했을 때의 PageRank의 우수성
- PageRank의 확장성과 다양한 응용 가능성

개인화된 PageRank는 사용자나 주제에 따라 편향된 랭킹을 제공하기 위해 설계됨.

기본 아이디어는, 표준 PageRank에서 무작위 점프할 때 모든 페이지에 균등한 확률을 주는 대신, 특정 페이지 세트에 더 높은 확률을 부여하는 것.

수정된 공식:
$$PR(A) = (1-d)v + d * \sum \frac{PR(T_i)}{C(T_i)}$$

여기서 $v$는 개인화 벡터로, 기본 PageRank의 균등 분포 대신 사용자 선호도를 반영한 확률 분포임.

예시:
사용자가 "스포츠"와 "기술" 주제를 선호한다고 가정해보자.

```python
# 간단한 개인화된 PageRank 예시
import numpy as np

# 웹 그래프 (5개 페이지)
M = np.array([
    [0, 1, 1, 0, 0],
    [1, 0, 1, 0, 0],
    [1, 0, 0, 1, 1],
    [0, 0, 1, 0, 1],
    [0, 0, 1, 1, 0]
])

# 개인화 벡터 (스포츠와 기술 페이지에 높은 가중치)
v = np.array([0.1, 0.1, 0.4, 0.3, 0.1])

d = 0.85  # 댐핑 팩터
n = len(M)

# 열 정규화
M_norm = M / M.sum(axis=0)

# PageRank 계산 함수
def personalized_pagerank(M, v, d, max_iter=100, tol=1e-6):
    pr = np.ones(n) / n
    for _ in range(max_iter):
        pr_next = (1-d) * v + d * M_norm.dot(pr)
        if np.sum(np.abs(pr_next - pr)) < tol:
            return pr_next
        pr = pr_next
    return pr

# 개인화된 PageRank 계산
personalized_pr = personalized_pagerank(M, v, d)
print("개인화된 PageRank:", personalized_pr)
```

이 예시에서:
- 5개의 페이지로 구성된 간단한 웹을 가정함.
- 개인화 벡터 v에서 스포츠와 기술 관련 페이지(3번, 4번)에 더 높은 가중치를 줌
- 결과적으로 이 사용자의 관심사를 반영한 PageRank 값이 계산됨.

실제 응용 사례:
1. 주제별 검색: 특정 주제에 관한 전문 검색 엔진 구현
2. 지역화된 검색: 사용자의 지리적 위치를 반영한 검색 결과 제공
3. 개인화된 추천 시스템: 사용자의 과거 행동을 기반으로 한 콘텐츠 추천

# 시간이 지나며
PageRank의 주요 한계점:

- 새 페이지 불이익: 새로 만들어진 페이지는 초기에 낮은 랭크를 가져 검색 결과에서 불리함.

- 링크 스팸에 취약: 의도적으로 많은 링크를 만들어 랭크를 조작할 수 있음.

- 콘텐츠 품질 무시: 실제 내용의 질보다 링크 구조에만 의존해 평가함.

- 시간적 요소 부재: 정보의 최신성을 고려하지 않음.

- 사용자 의도 반영 부족: 개별 사용자의 검색 의도나 관심사를 고려하지 못함.

Google의 발전 방향:

- 콘텐츠 분석 강화: 
   - 2003년 Florida 업데이트: 키워드 스터핑 같은 SEO 기법에 대응.
   - 2011년 Panda 업데이트: 콘텐츠 품질 평가 알고리즘 도입.

- 링크 품질 평가:
   - 2012년 Penguin 업데이트: 인위적인 링크 조작 탐지 및 페널티 부여.

- 의미 분석 도입:
   - 2013년 Hummingbird 업데이트: 자연어 처리 능력 향상.
   - 2019년 BERT 도입: 문맥을 고려한 언어 이해 모델 적용.

- 사용자 경험 중시:
   - 2015년 RankBrain: 머신러닝 기반 랭킹 시그널 도입.
   - 2021년 Core Web Vitals: 페이지 로딩 속도, 상호작용성 등 사용자 경험 요소 반영.

- 실시간 정보 반영:
   - 2009년 Caffeine 업데이트: 색인 생성 속도 개선으로 최신 정보 빠르게 반영.

- 다양한 콘텐츠 유형 지원:
   - 유니버설 검색 도입: 이미지, 뉴스, 동영상 등 다양한 형식의 결과를 통합 제공.

- 개인화:
   - 사용자의 검색 기록, 위치 등을 고려한 맞춤형 결과 제공.
