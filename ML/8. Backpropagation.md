## 기본 원리
- 신경망의 가중치를 최적화하는 핵심 알고리즘.
- 손실 함수의 그래디언트를 계산해 네트워크의 가중치를 효율적으로 조정함
- 연쇄 법칙(Chain Rule)을 사용해 계산 복잡도를 크게 줄임
- 출력층에서 시작해 입력층 방향으로 오차를 전파하면서 각 층의 가중치를 업데이트함

## 역전파의 중요성
- 딥러닝 모델 학습의 근간이 되는 알고리즘
- 대규모 신경망의 효율적인 학습을 가능하게 함.
- 그래디언트 소실 문제 등 학습 과정의 문제점을 이해하는 데 중요한 역할을 함

## 수학적 표현

### 주요 표기법
- $l$ : 층 인덱스 (입력층이 0, 출력층이 L)
- $w^l_{jk}$ : $l$층의 $k$번째 뉴런에서 $l+1$층의 $j$번째 뉴런으로의 가중치
- $a^l_j$ : $l$층의 $j$번째 뉴런의 활성화 값
- $z^l_j$ : $l$층의 $j$번째 뉴런의 가중합 (weighted sum)
- $\delta^l_j$ : $l$층의 $j$번째 뉴런의 오차 항 (error term)
- $C$ : 비용 함수 (Cost function)
- $f$ : 활성화 함수 (Activation function)

### 핵심 방정식

1. 출력층 오차
   $$\delta^L_j = \frac{\partial C}{\partial a^L_j} f'(z^L_j)$$
   - 이 식은 출력층에서의 오차를 계산함.
   - $\frac{\partial C}{\partial a^L_j}$는 출력값 변화에 따른 비용 함수의 변화율
   - $f'(z^L_j)$는 출력층 뉴런의 활성화 함수의 미분값

2. 은닉층 오차 (역전파 방정식)
   $$\delta^l_j = \sum_k w^{l+1}_{kj} \delta^{l+1}_k f'(z^l_j)$$
   - 이 식을 사용해 출력층에서 입력층 방향으로 오차를 전파함
   - $\sum_k w^{l+1}_{kj} \delta^{l+1}_k$는 다음 층의 오차를 현재 층으로 전파하는 항
   - $f'(z^l_j)$는 현재 층 뉴런의 활성화 함수 미분값

3. 가중치에 대한 비용 함수의 그래디언트
   $$\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$$
   - 이 식으로 각 가중치를 얼마나 조정해야 할지 계산함
   - $a^{l-1}_k$는 이전 층의 활성화 값
   - $\delta^l_j$는 현재 층의 오차 항

4. 편향에 대한 비용 함수의 그래디언트
   $$\frac{\partial C}{\partial b^l_j} = \delta^l_j$$
   - 이 식은 편향을 얼마나 조정해야 할지 나타냄
   - 편향의 그래디언트는 해당 뉴런의 오차 항과 같음

## 역전파 알고리즘의 단계
1. 순전파: 입력을 네트워크에 통과시켜 출력을 얻고
2. 손실 계산: 출력과 목표값 사이의 오차를 계산고
3. 역전파: 출력층부터 시작해 입력층 방향으로 오차를 전파함
4. 그래디언트 계산: 각 가중치와 편향에 대한 손실 함수의 그래디언트를 계산함
5. 파라미터 업데이트: 계산된 그래디언트를 사용해 가중치와 편향을 조정함

## 더 알면 좋긴 한데 좀 어려움

### 1. Hessian-Free 최적화
- 2차 미분 정보를 활용해 더 효율적인 최적화를 수행
- 큰 규모의 네트워크에서 특히 효과적임
- Hessian 행렬을 직접 계산하지 않고 Hessian-벡터 곱을 근사해 계산 복잡도를 줄임
- 곡률 정보를 이용해 더 정확한 방향으로 파라미터를 업데이트함

### 2. 순간 Hessian
- Hessian 행렬의 대각 요소만 고려해 계산 효율성을 높임
- AdaGrad, RMSProp 등 적응적 학습률 방법의 기반.
- 각 파라미터마다 개별적인 학습률을 적용할 수 있어 학습을 안정화시킴
- 그래디언트의 희소성이나 빈도에 따라 학습률을 자동으로 조절함

### 3. 자연 그래디언트
- 파라미터 공간의 리만 기하학을 고려한 최적화 방법
- 피셔 정보 행렬을 사용해 그래디언트를 조정
- 파라미터 공간의 곡률을 고려해 더 효과적인 최적화 경로를 찾음
- 확률 분포 간의 거리를 최소화하는 방식으로 학습을 진행함

### 4. 병렬 및 분산 역전파
- 대규모 네트워크에서 효율적인 학습을 위한 기법
- 모델 병렬화: 큰 모델을 여러 장치에 분산시켜 계산함
- 데이터 병렬화: 데이터를 여러 장치에 분산시켜 동시에 처리함
- 동기식, 비동기식 파라미터 업데이트 전략을 사용함
- 통신 오버헤드와 동기화 문제를 해결하는 게 핵심

### 5. 순환 신경망(RNN)에서의 역전파
- 시간에 따른 역전파(BPTT) 알고리즘을 사용함
- 긴 시퀀스에서 그래디언트 소실/폭발 문제가 발생함
- LSTM, GRU 같은 구조로 이 문제를 해결함
- 잘린 BPTT를 사용해 계산 효율성과 메모리 사용량을 개선함

### 6. 연속 시간 신경망에서의 역전파
- 미분 방정식으로 모델링된 신경망에서 그래디언트를 계산함
- Neural ODE 같은 최신 모델에 적용됨
- 연속적인 깊이를 가진 네트워크를 학습할 수 있음
- 수치적 미분 방법을 사용해 그래디언트를 근사

## 최적화 기법

1. 모멘텀
   - $v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta)$
   - $\theta = \theta - v_t$
   - 이전 그래디언트 정보를 활용해 현재 업데이트 방향을 결정
   - 지역 최소값을 벗어나는 데 도움을 줌

2. Nesterov 가속 그래디언트
   - $v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta - \gamma v_{t-1})$
   - $\theta = \theta - v_t$
   - 모멘텀의 개선된 버전으로, 미래의 위치를 예측해 그래디언트를 계산함
   - 수렴 속도가 더 빠르고 안정적임

3. Adam (Adaptive Moment Estimation)
   - $m_t = \beta_1 m_{t-1} + (1-\beta_1)\nabla_\theta J(\theta)$
   - $v_t = \beta_2 v_{t-1} + (1-\beta_2)(\nabla_\theta J(\theta))^2$
   - $\hat{m_t} = m_t / (1-\beta_1^t)$, $\hat{v_t} = v_t / (1-\beta_2^t)$
   - $\theta = \theta - \eta \hat{m_t} / (\sqrt{\hat{v_t}} + \epsilon)$
   - 1차 모멘트(평균)와 2차 모멘트(분산)를 모두 사용함
   - 학습률을 자동으로 조정하며, 희소한 그래디언트에도 효과적임