## 가중치 W를 어떻게 정할 것인가?

머신러닝은 데이터 기반의 접근이다.
- 우리는 모델의 형식을 디자인하고
- 랜덤하게 매개변수의 값을 설정함.
- 그 후, 학습 데이터 $x$를 입력해 라벨 $\hat{y}$를 예측함.
- 추정값 $\hat{y}$를 기준값 라벨 $y$와 비교하여 현재값이 얼마나 좋고 나쁜지를 비교함.
- 손실값에 따라 매개변수(W)를 업데이트하고 (이 부분을 어떻게 하는건가?)
- $\hat{y}\approx y$가 될 때까지 이 과정을 반복함.

**Mathematical optimization** (alternatively spelled _optimisation_) or **mathematical programming** is the selection of a best element, with regard to some criteria, from some set of available alternatives.

## 최적화 Primitive Ideas
- 완전 탐색
- 랜덤 탐색
- 시각화
- 그리디 탐색(각 변수 한개씩 해결)

## Gradient Descent(경사 하강법)

비용 함수 $J(\Theta)$를 최소화하는 방법

### 아이디어
- 현재 $\Theta$에서 $J(\Theta)$의 기울기 계산
- 음의 기울기 방향으로 작은 단계 이동
- 반복

### 업데이트 방정식
- 벡터 표기: $\Theta := \Theta - \alpha \nabla_\Theta J(\Theta)$
- 단일 매개변수: $\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\Theta)$

### 문제점
1. 비볼록 표면에서의 문제
2. 미분 가능한 함수에만 적용 가능
3. 수렴 속도가 느릴 수 있음
4. 
## Stochastic Gradient Descent (확률적 경사 하강법)
전체 데이터셋 대신 무작위 샘플 부분집합으로 그래디언트 계산

### 특징
1. 극단적 경우: 모든 샘플마다 파라미터 업데이트
2. 일반적 방법: $2^n$ 크기의 미니 배치 사용
3. 미니 배치 크기 선택
   - 작은 n: 안정성 증가
   - 큰 n: 수익 감소 법칙 적용

### 장점
1. 계산 효율성 향상
2. 지역 최적해 탈출 가능성

### 고려사항
- 최적의 미니 배치 크기는 문제, 데이터, 하드웨어에 따라 다름
