
## 역전파를 왜 쓰는걸까

신경망은 입력층, 출력층, 그리고 하나 이상의 은닉층으로 구성됨. 

입력과 출력 사이의 관계는 은닉층, 활성화 함수, 가중치 등으로 인해 비선형적일 수 있음.

학습 과정에서 훈련 데이터를 사용해 입력에 대한 출력을 계산하고, 그 결과를 바탕으로 가중치와 편향을 조정함. 이 조정에는 여러 방법이 있는데, 손실 함수를 계산하고 경사 하강법 등의 최적화 알고리즘을 적용하는 것이 대표적임.

역전파는 다층 신경망에서 출력층부터 입력층 방향으로 각 층의 가중치를 효율적으로 조정하는 알고리즘임. 이는 연쇄 법칙을 이용해 각 가중치가 최종 오차에 미치는 영향을 계산하고, 이를 바탕으로 가중치를 업데이트함.

## 그래디언트가 왜 중요할까?

ML에서 그래디언트가 되게 중요하게 다루어진다는 사실을 느끼고 있는데, 아직 그 이유가 잘 실감이 안남. 내 방식대로 그 이유를 추론해보겠음.

그래디언트가 중요한 이유 : 각 변수에 따른 편미분값이 그래디언트임. 당장 선형 모델만 생각해도, 기울기가 곧 가중치 W임. 비선형의 경우에도, 우리는 '어떤 함수를 따라갈것이다'라는 것을 맨 처음 가정하고, 각 변수의 가중치 W1, W2,... 등을 학습을 통해 구하는 것이 목적임. 따라서, train data를 통해 변수에 따른 출력값과, 그 변화량(미분값)을 따져서 가중치를 업데이트하는 과정은 필연적으로 있어야 하고, 가중치 업데이트에 그래디언트가 무조건 쓰이기 때문임.
 
몇 가지 보완할 점을 추가해 정리함:

1. 최적화 방향 제시: 그래디언트는 함수값이 가장 빠르게 증가하는 방향을 가리킴. 손실 함수를 최소화하는 과정에서 그래디언트의 반대 방향으로 이동하면 가장 효율적으로 최적점에 도달할 수 있음.

2. 효율적인 학습: 그래디언트를 이용한 경사 하강법은 계산 효율성이 높음. 특히 고차원 공간에서 최적화할 때 유용함.

3. 비선형 문제 해결: 비선형 모델에서도 그래디언트를 통해 국소적으로 선형 근사를 할 수 있어, 복잡한 함수의 최적화에 적용 가능함.

4. 역전파 알고리즘: 신경망에서 그래디언트는 역전파의 핵심임. 각 층의 가중치가 최종 오차에 미치는 영향을 계산하는 데 사용됨.

5. 학습률 조정: 그래디언트의 크기는 학습률 조정에 중요한 정보를 제공함. 이를 통해 적응적 학습률 방법들이 개발됨.

그래디언트는 단순히 가중치 업데이트에만 쓰이는 게 아니라, ML 알고리즘의 학습 과정 전반에 걸쳐 중요한 역할을 함.