## 정의
머신 러닝 모델의 성능을 정량화하는 함수

## 기본 개념
- 수식: $\mathcal{L}(\hat{y}, y)$
  * $\hat{y}$: 모델의 추정값
  * $y$: 실제 레이블 (기준값)
- 특성:
  * 양수를 출력
  * $\hat{y} = y$일 때 최소값 (보통 0)
  * $\hat{y}$와 $y$ 사이의 차이가 클수록 큰 값 출력

## Discriminative Model에서의 적용
- Ground Truth: $y \in \{+1, -1\}$
- 모델 예측: $\hat{y} \in \mathbb{R}$
- 분류 규칙:
  * $\hat{y} > 0$: 양성 클래스
  * $\hat{y} \leq 0$: 음성 클래스

## 마진 기반 손실
손실은 $y\hat{y}$에 따라 결정됨
- $y\hat{y} > 0$: 분류가 정확, 손실 감소
- $y\hat{y} < 0$: 분류가 부정확, 손실 증가

### 주요 마진 기반 손실 함수

1. 0/1 손실
   - 특징: 
     * 정확한 예측: 손실 0
     * 부정확한 예측: 일정한 손실 (보통 1)
   - 한계: 비연속적, 미분 불가능

2. 로그 손실
   - 특징:
     * 연속 함수, 전구간 미분 가능
     * 정확도에 따라 점진적으로 손실 감소
   - 장점: 확률로 해석 가능 ($p(y|x)$)

3. 지수 손실
   - 특징:
     * 연속 함수, 전구간 미분 가능
     * 오류에 대해 매우 큰 페널티
   - 단점: 아웃라이어에 민감

4. Hinge 손실
   - 특징:
     * 오류에 대해 선형적으로 페널티 증가
     * 작은 오차 범위 내 정답에도 약간의 페널티
   - 장점: 계산적으로 효율적

### 손실 함수 비교
1. 지수 손실
   - 장점: 정확한 예측에 대해 매우 작은 손실
   - 단점: 노이즈가 많은 데이터에 부적합

2. Hinge 손실 (SVM)
   - 장점: 계산 효율성이 높음

3. 로그 손실 (로지스틱 회귀)
   - 장점: 확률로 해석 가능하여 직관적

이 중 Hinge 손실, 로그 손실이 가장 널리 쓰임.
## 확률적 설정

### 이진 분류 문제
- 기준값: $y \in \{0, 1\}$
- 모델 예측: $\hat{y} \in [0, 1]$ (한 클래스의 확률)
- 다른 클래스의 확률: $1 - \hat{y}$
- 예시: 점수 차이에 시그모이드 함수 적용

### 다중 클래스 분류 (K > 2)
- 기준값: 원-핫 인코딩 벡터 (예: $y = [0, 0, 0, 0, 1, 0, 0]$)
- 모델 예측: K-1개의 점수 예측, 마지막 점수는 '1-합계'
- 예시: 소프트맥스 함수 (예측값이 0에서 1 사이, 합계가 1)

### 특징
- 손실 함수: 실제(GT) 확률 분포와 예측 확률 분포를 비교

## Cross Entropy
정보 이론에 기반한 개념으로, 한 분포를 사용하여 다른 분포를 인코딩하는 데 필요한 평균 비트 수를 측정

확률 기반 분류 모델에서 널리 사용되는 손실 함수로, 모델의 예측이 실제 분포에 얼마나 가까운지를 효과적으로 측정한다. 특히 로지스틱 회귀나 신경망의 소프트맥스 출력층에서 자주 사용된다.

### 수식

1. 일반적 정의 (다중 클래스):
   $$\mathcal{L} = -\frac{1}{N}\sum_{i=1}^N\sum_{k=1}^K y_{ik}\log{(\hat{y}_{ik})}$$

2. 이진 분류 정의:
   $$\mathcal{L} = -\frac{1}{N}\sum_{i=1}^N[y_i\log{(\hat{y}_i)} + (1-y_i)\log{(1-\hat{y}_i)}]$$

3. 간소화된 형태:
   $$\mathcal{L} = -\frac{1}{N}\sum_{i=1}^N \log{(\hat{y}_{iT_i})}$$
   여기서 $T_i$는 i번째 샘플의 실제 클래스

### 특성
- 예측 확률 $\hat{y} \in [0, 1]$
- 손실 함수 그래프: $y = -\log{x}$의 양의 영역
- 추정치가 1에 가까울 때: 손실 → 0
- 추정치가 0에 가까울 때: 손실 → $\infty$

### 해석
- "올바른 클래스에 대한 예측 확률"의 음의 로그 평균
- 모델의 예측이 실제 클래스에 얼마나 확신을 가지는지 측정
- 예측이 정확할수록 낮은 손실, 부정확할수록 높은 손실 부과

# Kullback-Leibler(KL) 발산
KL 발산은 두 확률 분포 간의 차이를 측정하는 방법으로, 정보 이론에서 중요한 개념이다. 하나의 확률 분포 P가 기준 확률 분포 Q와 얼마나 다른지를 수치화한다.

두 확률 분포 간의 차이를 정량화하는 강력한 도구로, 기계 학습, 정보 이론, 통계학 등 다양한 분야에서 중요하게 사용된다. 특히 모델의 학습과 평가, 정보 압축, 확률 분포의 근사 등에 광범위하게 적용된다.

## 수식
이산 확률 분포의 경우:
$$D_{KL}(P||Q) = \sum_{i} P(i) \log\left(\frac{P(i)}{Q(i)}\right)$$

연속 확률 분포의 경우:
$$D_{KL}(P||Q) = \int P(x) \log\left(\frac{P(x)}{Q(x)}\right) dx$$

여기서,
- P(i) 또는 P(x): 실제 분포
- Q(i) 또는 Q(x): 근사 또는 예측 분포

## 특성
1. 비대칭성: $D_{KL}(P||Q) \neq D_{KL}(Q||P)$
2. 비음수성: $D_{KL}(P||Q) \geq 0$
3. 동일 분포일 때 0: $D_{KL}(P||P) = 0$

## 구성 요소
KL 발산은 다음 두 항의 차이로 구성된다:

1. 교차 엔트로피: $H(P,Q) = -\sum_{i} P(i) \log(Q(i))$
2. 엔트로피: $H(P) = -\sum_{i} P(i) \log(P(i))$

따라서, $D_{KL}(P||Q) = H(P,Q) - H(P)$

## 응용
1. 기계 학습: 모델 학습 및 최적화
2. 정보 이론: 정보 압축 및 코딩
3. 통계학: 확률 분포 근사
4. 물리학: 열역학적 엔트로피 관련 연구

## 한계
1. 비대칭성으로 인해 거리 측정의 공리를 만족하지 않음
2. Q(i)=0일 때 P(i)≠0이면 정의되지 않음 (0으로 나누는 문제)

## 관련 개념
1. Jensen-Shannon 발산: KL 발산의 대칭적 버전
2. f-발산: KL 발산을 일반화한 개념

## Logit, Sigmoid, Softmax를 비교해보자.

| 특성       | Logit                               | Sigmoid                              | Softmax                                      |
| -------- | ----------------------------------- | ------------------------------------ | -------------------------------------------- |
| 정의       | $logit(p) = \log(\frac {p}  {1-p})$ | $σ(x) = \frac {1} {(1 + e^{-x})}$    | $softmax(x_i) = \frac{e^{x_i}} {Σ(e^{x_j})}$ |
| 입력 범위    | (0, 1)                              | (-∞, ∞)                              | (-∞, ∞)                                      |
| 출력 범위    | (-∞, ∞)                             | (0, 1)                               | (0, 1)                                       |
| 주요 용도    | 로그 오즈(log odds) 계산                  | 이진 분류                                | 다중 클래스 분류                                    |
| 특징       | - sigmoid의 역함수<br>- 확률을 로그 오즈로 변환   | - S자 형태의 곡선<br>- 입력을 0과 1 사이의 값으로 압축 | - 여러 클래스에 대한 확률 분포 생성<br>- 출력의 합이 1이 됨       |
| 미분       | 1 / (p * (1-p))                     | σ(x) * (1 - σ(x))                    | softmax(x_i) * (1 - softmax(x_i)) (대각 요소)    |
| 관련 손실 함수 | -                                   | 이진 교차 엔트로피                           | 범주형 교차 엔트로피                                  |