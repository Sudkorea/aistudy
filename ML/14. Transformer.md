``` Java
/* !!TODO!!
파일 너무 중구난방으로 저장되는거처럼 보인다. 
큰 주제 잡고 쭉 써내려가는게 더 나으려나??
*/
```
# RNN(Recurrent Neural Network)

RNN은 시계열 데이터를 처리하기 위한 신경망 모델임. 이 모델은 순차적인 데이터나 시간에 따라 변화하는 데이터를 다루는 데 특화되어 있음. 

## 작동 원리

RNN의 핵심은 hidden state라고 불리는 내부 메모리를 사용한다는 점임. 이 hidden state는 이전 시점의 정보를 저장하고 있어, 현재 입력과 함께 처리됨. 수식으로 표현하면 다음과 같음:

$$h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$

$$y_t = g(W_{hy}h_t + b_y)$$

여기서 $h_t$는 현재 시점의 hidden state, $x_t$는 현재 입력, $y_t$는 출력을 나타냄. $W$와 $b$는 각각 가중치와 편향을 의미하며, $f$와 $g$는 활성화 함수임.

## 장점

1. 가변적인 길이의 Input Sequence 처리 가능: RNN은 입력 시퀀스의 길이에 상관없이 동작할 수 있음. 이는 텍스트 분석이나 음성 인식과 같이 입력 길이가 일정하지 않은 태스크에 유용함.

2. 모델 크기의 효율성: 입력이 많아져도 모델의 크기가 증가하지 않음. 이는 RNN이 동일한 가중치를 반복적으로 사용하기 때문임. 이로 인해 메모리 사용이 효율적이며, 학습 파라미터의 수가 줄어듦.

3. 장기 의존성 처리 가능성: 이론적으로 $t$ 시점에서 수행된 계산은 여러 단계 이전의 정보를 사용할 수 있음. 이는 장기 의존성(long-term dependency)을 포착할 수 있는 가능성을 제공함.

4. 가중치 공유: 모든 시간 단계에 동일한 가중치가 적용됨. 이는 모델의 일반화 능력을 향상시키고, 다양한 길이의 시퀀스에 대해 동일한 모델을 사용할 수 있게 함.

## 단점

1. 계산 속도: Recurrent Computation이 느림. 각 시간 단계마다 순차적으로 계산을 수행해야 하므로, 병렬 처리가 어려움.

2. 병렬화의 어려움: Sequence output inference는 병렬화(parallelization)가 어려움. 이는 특히 긴 시퀀스를 처리할 때 성능 저하의 원인이 됨.

3. Vanishing Gradient Problem: 바닐라 RNN은 훈련 중 vanishing gradient problem에 취약함. 이는 역전파 과정에서 그래디언트가 점점 작아져, 장기 의존성을 학습하기 어렵게 만듦.

4. Long-range Dependence 모델링의 한계: 바닐라 RNN은 long-range dependence를 모델링하는 데 종종 실패함. 이는 vanishing gradient problem과 관련이 있으며, 먼 과거의 정보를 현재 시점까지 효과적으로 전달하지 못하는 문제를 야기함.

5. 정보 접근의 어려움: 실제로 여러 단계 이전의 정보에 접근하기 어려움. 시퀀스가 길어질수록 초기 정보가 점점 희석되어 손실됨.

## 단점을 보완하는 방법

1. LSTM (Long Short-Term Memory) / GRU (Gated Recurrent Unit): 
   이 두 모델은 게이트 메커니즘을 도입하여 vanishing gradient problem을 완화하고, long-range dependence를 더 잘 모델링할 수 있게 함. LSTM은 forget gate, input gate, output gate를 사용하여 정보의 흐름을 제어함. GRU는 reset gate와 update gate를 사용하여 LSTM보다 간단한 구조로 비슷한 효과를 냄.

2. Seq2seq model:
   Many-to-many RNN의 한계를 극복하기 위해 개발됨. 인코더-디코더 구조를 사용하여 입력 시퀀스를 고정된 크기의 벡터로 인코딩한 후, 이를 바탕으로 출력 시퀀스를 생성함. 이를 통해 입출력 시퀀스의 길이가 다른 경우에도 유연하게 대응할 수 있음.

3. Attention Model:
   LSTM/GRU도 매우 긴 시퀀스 처리에 한계가 있음. Attention 메커니즘은 디코더가 인코더의 모든 hidden state에 직접 접근할 수 있게 하여, 긴 시퀀스에서도 중요한 정보를 효과적으로 활용할 수 있게 함. 이는 기계 번역, 이미지 캡셔닝 등 다양한 분야에서 성능 향상을 이끌어냄.

4. Transformer:
   RNN의 순차적 계산 문제를 완전히 해결하기 위해 개발된 모델임. Self-attention 메커니즘을 사용하여 시퀀스의 모든 위치를 동시에 고려할 수 있으며, 병렬 처리가 가능해 학습 속도가 빠름. 또한 long-range dependence를 효과적으로 포착할 수 있음.

# Exploding / Vanishing Gradient Problem

심층 신경망, 특히 RNN에서 자주 발생하는 문제임. 이는 역전파 과정에서 그래디언트를 계산할 때 발생함.

## 문제의 원인

RNN에서 시간 단계 $t$에서의 손실을 $L_t$라고 할 때, 가중치 $W$에 대한 그래디언트는 다음과 같이 계산됨:

$$\frac{\partial L_t}{\partial W} = \sum_{i=1}^t \frac{\partial L_t}{\partial h_t} \frac{\partial h_t}{\partial h_i} \frac{\partial h_i}{\partial W}$$

여기서 $\frac{\partial h_t}{\partial h_i}$는 시간에 따른 그래디언트의 흐름을 나타냄. 이 항은 다음과 같이 계산됨:

$$\frac{\partial h_t}{\partial h_i} = \prod_{j=i+1}^t \frac{\partial h_j}{\partial h_{j-1}} = \prod_{j=i+1}^t \text{diag}(f'(h_{j-1}))W$$

여기서 $f'$는 활성화 함수의 미분임. 이 곱이 반복되면서 그래디언트가 점점 작아지거나(vanishing) 커질 수(exploding) 있음.

## 예시

시그모이드 함수를 활성화 함수로 사용하는 경우를 생각해보자. 시그모이드 함수의 미분값은 최대 0.25임. 만약 가중치 $W$의 최대 특이값이 4라고 가정하면:

$$\|\frac{\partial h_t}{\partial h_i}\| \leq (4 \times 0.25)^{t-i} = 1^{t-i} = 1$$

이 경우, 시간 간격이 커질수록 그래디언트는 기하급수적으로 작아짐. 반대로, 가중치가 크고 활성화 함수의 기울기가 1에 가까우면 그래디언트가 폭발할 수 있음.

## 영향

1. 학습 불안정: 그래디언트가 불안정해지면 가중치 업데이트가 불안정해지고, 학습이 수렴하지 않거나 발산할 수 있음.
2. 장기 의존성 학습 어려움: 그래디언트가 소실되면 먼 과거의 정보가 현재 시점에 영향을 미치기 어려워짐.
3. 학습 속도 저하: 그래디언트가 매우 작아지면 가중치 업데이트가 거의 일어나지 않아 학습이 매우 느려짐.

이러한 문제들로 인해 RNN의 성능이 저하되고, 특히 장기 의존성을 포착해야 하는 태스크에서 어려움을 겪게 됨. 이를 해결하기 위해 LSTM, GRU 등의 개선된 모델이 제안되었으며, 이들은 게이트 메커니즘을 통해 그래디언트의 흐름을 조절하여 문제를 완화함.

## Long Short Term Memory(LSTM)

LSTM은 RNN의 vanishing gradient 문제를 해결하기 위해 제안된 모델임. 기본적인 RNN과 달리 LSTM은 셀 상태(cell state)와 여러 게이트를 사용하여 정보의 흐름을 제어함.

### LSTM의 구조

1. 망각 게이트(Forget gate): 이전 정보를 얼마나 잊을지 결정함.
2. 입력 게이트(Input gate): 새로운 정보를 얼마나 저장할지 결정함.
3. 출력 게이트(Output gate): 셀 상태의 어떤 부분을 출력으로 내보낼지 결정함.
4. 셀 상태(Cell state): 장기 기억을 저장하는 컨베이어 벨트 역할을 함.

### LSTM의 수식

$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$
$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$
$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$
$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
$h_t = o_t * \tanh(C_t)$

여기서 $\sigma$는 시그모이드 함수, $*$는 요소별 곱셈을 나타냄.

### 예시

문장 "나는 한국어를 공부하고 있다"를 처리하는 LSTM을 생각해보자. 

1. "나는" 입력 시: 
   - 입력 게이트가 활성화되어 "나"에 대한 정보를 저장함.
   - 출력 게이트가 이 정보의 일부를 hidden state로 내보냄.

2. "한국어를" 입력 시:
   - 망각 게이트가 "나는"의 일부 정보를 잊음.
   - 입력 게이트가 "한국어"에 대한 정보를 저장함.
   - 셀 상태에 "나"와 "한국어"에 대한 정보가 함께 저장됨.

3. "공부하고" 입력 시:
   - 이전 정보와 "공부하고"를 결합하여 문맥을 이해함.

4. "있다" 입력 시:
   - 전체 문장의 의미를 파악하고 적절한 출력을 생성함.

이 과정에서 LSTM은 긴 시퀀스에 걸쳐 중요한 정보를 유지할 수 있음.

## Gated Recurrent Units (GRU)

GRU는 LSTM을 단순화한 버전으로, 비슷한 성능을 보이면서도 계산 복잡도가 낮음.

### GRU의 구조

1. 리셋 게이트(Reset gate): 이전 정보를 얼마나 무시할지 결정함.
2. 업데이트 게이트(Update gate): 이전 정보를 얼마나 유지하고 새 정보를 얼마나 받아들일지 결정함.

### GRU의 수식

$z_t = \sigma(W_z \cdot [h_{t-1}, x_t])$
$r_t = \sigma(W_r \cdot [h_{t-1}, x_t])$
$\tilde{h}_t = \tanh(W \cdot [r_t * h_{t-1}, x_t])$
$h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t$

여기서 $z_t$는 업데이트 게이트, $r_t$는 리셋 게이트를 나타냄.

### 예시

같은 문장 "나는 한국어를 공부하고 있다"를 처리하는 GRU를 생각해보자.

1. "나는" 입력 시:
   - 업데이트 게이트가 높게 활성화되어 이 정보를 강하게 저장함.

2. "한국어를" 입력 시:
   - 리셋 게이트가 이전 정보("나는")의 일부를 무시함.
   - 업데이트 게이트가 "한국어"에 대한 새로운 정보를 받아들임.

3. "공부하고" 입력 시:
   - 이전 상태와 현재 입력을 결합하여 hidden state를 업데이트함.

4. "있다" 입력 시:
   - 전체 문맥을 고려하여 최종 hidden state를 생성함.

GRU는 LSTM보다 간단한 구조로 비슷한 효과를 낼 수 있음.

## 두 모델 비교

1. 구조적 차이:
   - LSTM: 셀 상태, 입력 게이트, 망각 게이트, 출력 게이트를 가짐.
   - GRU: 리셋 게이트와 업데이트 게이트만을 가짐.

2. 파라미터 수:
   - LSTM이 GRU보다 더 많은 파라미터를 가짐. 이는 LSTM이 더 복잡한 패턴을 학습할 수 있게 하지만, 동시에 과적합의 위험도 높아짐.

3. 계산 효율성:
   - GRU는 게이트가 적어 계산이 더 빠르고 메모리 사용이 효율적임.

4. 성능:
   - 대부분의 태스크에서 두 모델의 성능은 비슷함. 
   - 매우 긴 시퀀스나 복잡한 태스크에서는 LSTM이 약간 우세할 수 있음.

5. 훈련 용이성:
   - GRU는 구조가 단순해 훈련이 더 쉬운 경향이 있음.

6. 적용 분야:
   - LSTM: 복잡한 시퀀스 모델링, 음성 인식, 기계 번역 등에 널리 사용됨.
   - GRU: 비교적 간단한 시퀀스 태스크, 텍스트 분류, 감성 분석 등에 효과적임.

# Machine Translation Problem

기계 번역은 한 언어의 텍스트를 다른 언어로 자동 변환하는 작업임. 이는 자연어 처리 분야에서 중요한 응용 사례 중 하나임.

## Many-to-many RNN

Many-to-many RNN은 입력 시퀀스와 출력 시퀀스의 길이가 다를 때 문제가 발생함. 

예시: "I love you"를 "나는 당신을 사랑합니다"로 번역하는 경우

1. 입력 길이 불일치: 영어 문장은 3개의 단어지만, 한국어 문장은 4개의 단어로 구성됨. 기본 RNN은 이러한 길이 차이를 처리하기 어려움.

2. 어순 차이: 영어는 SVO 구조, 한국어는 SOV 구조임. "love you"가 "당신을 사랑합니다"로 바뀌는데, 이는 단순한 단어 대 단어 매핑으로는 해결할 수 없음.

3. 문맥 이해의 어려움: "I"가 "나는"으로 번역되는데, 이는 문맥에 따라 달라질 수 있음 (예: 격식체에서는 "저는"으로 번역될 수 있음).

4. 출력 생성의 어려움: RNN은 각 시간 단계마다 출력을 생성해야 하는데, 입력과 출력의 길이가 다르면 이를 적절히 조절하기 어려움.

이러한 문제들로 인해 기본적인 many-to-many RNN은 복잡한 기계 번역 작업에 적합하지 않음.

## Encoder-Decoder Structure

Encoder-Decoder 구조는 이러한 문제를 해결하기 위해 제안된 모델임.

### Encoder

Encoder는 입력 시퀀스를 고정된 크기의 벡터로 압축함.

예시: "I love you" 번역
1. "I" 입력 → Encoder hidden state 갱신
2. "love" 입력 → hidden state 추가 갱신
3. "you" 입력 → 최종 hidden state 생성

이 최종 hidden state가 전체 입력 문장의 의미를 담고 있는 context vector가 됨.

### Decoder

Decoder는 Encoder에서 생성된 context vector를 받아 목표 언어로 번역을 생성함.

#### Auto-Regressive Generation

Auto-Regressive Generation은 이전에 생성된 출력을 다음 출력 생성의 입력으로 사용하는 방식임.

예시: "나는 당신을 사랑합니다" 생성
1. context vector + \<START> token → "나는" 생성
2. context vector + "나는" → "당신을" 생성
3. context vector + "당신을" → "사랑합니다" 생성
4. context vector + "사랑합니다" → \<END> token 생성

이 방식은 이전 출력을 고려하여 문맥에 맞는 자연스러운 번역을 생성할 수 있음.

#### Teacher Forcing

Teacher Forcing은 학습 시 모델의 예측 대신 실제 정답을 다음 입력으로 사용하는 기법임.

예시:
1. context vector + \<START> → "나는" 생성
2. context vector + "나는"(정답) → "당신을" 생성 (모델이 "나는" 대신 다른 것을 예측했더라도 정답인 "나는"을 사용)
3. context vector + "당신을"(정답) → "사랑합니다" 생성

이 방법은 학습 초기에 모델이 잘못된 예측을 할 때 발생할 수 있는 오류의 누적을 방지하고, 학습 속도를 높일 수 있음.

## Overall seq2seq Model

전체 seq2seq 모델은 Encoder와 Decoder를 결합한 구조임.

예시: "I love you" → "나는 당신을 사랑합니다" 번역 과정

1. Encoding:
   - "I" → h1
   - "love" → h2
   - "you" → h3 (final hidden state, context vector)

2. Decoding:
   - context vector + \<START> → "나는"
   - context vector + "나는" → "당신을"
   - context vector + "당신을" → "사랑합니다"
   - context vector + "사랑합니다" → \<END>

이 구조의 장점:
1. 가변 길이 입출력 처리: 입력과 출력의 길이가 달라도 처리 가능함.
2. 문맥 보존: Encoder가 전체 입력 문장의 의미를 context vector에 압축하여 Decoder에 전달함.
3. 유연한 번역: Decoder가 Auto-Regressive하게 출력을 생성하므로, 목표 언어의 문법과 어순에 맞는 자연스러운 번역이 가능함.

seq2seq 모델은 기계 번역뿐만 아니라 텍스트 요약, 대화 시스템 등 다양한 자연어 처리 태스크에 활용됨. 하지만 긴 문장에서 정보 손실이 발생할 수 있다는 한계가 있어, 이를 보완하기 위해 Attention 메커니즘 등이 추가로 도입되었음.

# Attention
![[/ML/stuff/Pasted image 20240818124109.png]]

Attention 메커니즘은 seq2seq 모델의 한계를 극복하기 위해 제안된 기법임. 이는 디코더가 출력을 생성할 때 인코더의 모든 hidden state를 참조할 수 있게 해주어, 긴 문장에서도 정보 손실을 줄이고 더 정확한 번역을 가능하게 함.

## Attention의 작동 원리

1. Alignment Score 계산: 디코더의 현재 hidden state와 인코더의 모든 hidden state 간의 유사도를 계산함.
2. Attention Weight 생성: Alignment Score를 정규화하여 각 인코더 hidden state의 중요도를 나타내는 가중치를 생성함.
3. Context Vector 생성: Attention Weight를 사용해 인코더 hidden state의 가중 평균을 계산하여 context vector를 생성함.
4. 출력 생성: Context vector와 디코더의 현재 hidden state를 결합하여 출력을 생성함.

## 예시: "I love you" → "나는 당신을 사랑합니다" 번역

### 1. 인코딩 단계

인코더가 입력 문장을 처리하여 hidden state를 생성함:
- "I" → h1
- "love" → h2
- "you" → h3

### 2. 디코딩 단계 (Attention 적용)

#### "나는" 생성
1. 디코더 초기 상태 s0와 인코더 hidden states (h1, h2, h3) 간 Alignment Score 계산:
   - $score(s0, h1) = 0.1$
   - $score(s0, h2) = 0.3$
   - $score(s0, h3) = 0.6$
2. Softmax 적용하여 Attention Weight 생성:
   - $α1 = 0.1, α2 = 0.3, α3 = 0.6$
3. Context Vector 계산:
   - $c1 = 0.1*h1 + 0.3*h2 + 0.6*h3$
4. c1과 s0를 결합하여 "나는" 생성

#### "당신을" 생성
1. 새로운 디코더 상태 s1와 인코더 hidden states 간 Alignment Score 계산:
   - $score(s1, h1) = 0.2$
   - $score(s1, h2) = 0.3$
   - $score(s1, h3) = 0.5$
2. Attention Weight 생성:
   - $α1 = 0.2, α2 = 0.3, α3 = 0.5$
3. 새로운 Context Vector 계산:
   - $c2 = 0.2*h1 + 0.3*h2 + 0.5*h3$
4. c2와 s1을 결합하여 "당신을" 생성

#### "사랑합니다" 생성
1. 디코더 상태 s2와 인코더 hidden states 간 Alignment Score 계산:
   - $score(s2, h1) = 0.1$
   - $score(s2, h2) = 0.7$
   - $score(s2, h3) = 0.2$
2. Attention Weight 생성:
   - $α1 = 0.1, α2 = 0.7, α3 = 0.2$
3. Context Vector 계산:
   - $c3 = 0.1*h1 + 0.7*h2 + 0.2*h3$
4. c3와 s2를 결합하여 "사랑합니다" 생성

## Attention의 장점

1. 정보 손실 감소: 긴 문장에서도 모든 단어의 정보를 활용할 수 있음.
   예: "I really love you very much"와 같은 긴 문장에서도 "very much"의 정보를 "사랑합니다"를 생성할 때 효과적으로 활용할 수 있음.

2. 단어 간 관계 포착: 서로 다른 위치의 단어들 사이의 관계를 학습할 수 있음.
   예: "bank of the river"에서 "bank"가 "강변"의 의미로 번역되어야 함을 포착할 수 있음.

3. 해석 가능성: Attention Weight를 시각화하여 모델의 결정 과정을 이해할 수 있음.
   예: "사랑합니다"를 생성할 때 "love"에 높은 가중치가 부여된 것을 확인할 수 있음.

4. 가변 길이 입력 처리: 입력 문장의 길이에 관계없이 효과적으로 정보를 처리할 수 있음.
   예: "I love you"와 "I absolutely adore you with all my heart"와 같이 길이가 다른 문장들도 동일한 구조로 처리 가능.

## Attention의 변형

1. Self-Attention: 같은 시퀀스 내의 다른 위치 간의 Attention을 계산함.
   예: "The animal didn't cross the street because it was too tired"에서 "it"이 "animal"을 가리킴을 포착할 수 있음.

2. Multi-Head Attention: 여러 개의 Attention을 병렬로 계산하여 다양한 관점에서 정보를 추출함.
   예: 한 헤드는 문법적 관계를, 다른 헤드는 의미적 관계를 포착할 수 있음.

3. Transformer: Attention만을 사용하여 RNN 구조를 완전히 대체한 모델.
   예: "BERT", "GPT" 등의 최신 언어 모델들이 Transformer 구조를 기반으로 함.

Attention 메커니즘은 기계 번역의 성능을 크게 향상시켰을 뿐만 아니라, 현대 자연어 처리의 근간이 되는 Transformer 구조의 핵심 요소가 되었음. 이를 통해 더 정확하고 자연스러운 번역, 더 효과적인 언어 이해와 생성이 가능해졌음.

# Transformers

 "Attention Is All You Need" 논문에서 제안된 혁신적인 신경망 구조임. 이 모델은 RNN이나 CNN을 사용하지 않고 오직 Attention 메커니즘만으로 시퀀스 데이터를 처리함. 

## Transformer의 구조

Transformer는 인코더와 디코더로 구성되며, 각각 여러 층의 동일한 구조를 가짐.

1. 인코더 구조:
   - Self-Attention 레이어
   - Feed-Forward Neural Network

2. 디코더 구조:
   - Masked Self-Attention 레이어
   - Encoder-Decoder Attention 레이어
   - Feed-Forward Neural Network

각 구성 요소에 대해 자세히 살펴보겠음.

## Self-Attention

Self-Attention은 입력 시퀀스 내의 각 요소가 다른 모든 요소와 어떻게 관련되는지를 계산함.

예시: "The animal didn't cross the street because it was too tired."

1. 각 단어에 대해 Query(Q), Key(K), Value(V) 벡터를 생성함.
2. 각 단어의 Q와 모든 단어의 K의 내적을 계산하여 Attention Score를 구함.
3. Attention Score를 정규화하고 Softmax를 적용하여 Attention Weight를 얻음.
4. Attention Weight와 V를 곱하여 각 단어의 새로운 표현을 생성함.

이 과정을 통해 "it"이 "animal"을 가리킨다는 것을 모델이 파악할 수 있음.

## Multi-Head Attention

Multi-Head Attention은 여러 개의 Self-Attention을 병렬로 수행함.

예시: 8-head attention을 사용하는 경우

1. 입력을 8개의 서로 다른 선형 변환을 통해 변환함.
2. 각 변환된 입력에 대해 별도의 Self-Attention을 수행함.
3. 8개의 Attention 결과를 연결(concatenate)하고 다시 선형 변환을 적용함.

이를 통해 모델은 다양한 관점에서 입력을 분석할 수 있음. 예를 들어, 한 헤드는 문법적 관계를, 다른 헤드는 의미적 관계를 포착할 수 있음.

## Positional Encoding

Transformer는 순차적인 구조가 없기 때문에, 입력의 순서 정보를 별도로 제공해야 함. 이를 위해 Positional Encoding을 사용함.

예시: "I love you"의 각 단어에 위치 정보를 더함
- "I" → "I" + positional_encoding(1)
- "love" → "love" + positional_encoding(2)
- "you" → "you" + positional_encoding(3)

이를ㄴㅋㅁㅋ 통해 모델은 각 단어의 상대적 위치를 파악할 수 있음.

## Feed-Forward Neural Networks

각 Attention 레이어 뒤에는 두 개의 선형 변환과 ReLU 활성화 함수로 구성된 Feed-Forward 네트워크가 있음.

예시: Attention 출력 x에 대해
1. $FFN(x) = max(0, xW1 + b1)W2 + b2$

이 레이어는 Attention에서 추출된 특징을 비선형적으로 변환하여 모델의 표현력을 높임.

## Layer Normalization과 Residual Connection

각 서브레이어(Attention, Feed-Forward) 뒤에는 Layer Normalization과 Residual Connection이 적용됨.

예시: Attention 레이어의 출력을 x, 입력을 Input이라고 할 때
- Output = LayerNorm(x + Input)

이는 모델의 학습을 안정화하고 깊은 네트워크에서의 그래디언트 흐름을 개선함.

## Masked Self-Attention

디코더의 첫 번째 레이어에서는 Masked Self-Attention을 사용함. 이는 미래의 토큰을 참조하지 못하게 하여 자기회귀적 생성을 가능하게 함.

예시: "나는 당신을 사랑합니다" 생성 시
- "나는" 생성 → "나는"만 참조 가능
- "당신을" 생성 → "나는", "당신을"까지만 참조 가능
- "사랑합니다" 생성 → "나는", "당신을", "사랑합니다"까지 참조 가능

## Transformer의 장점

1. 병렬 처리: Self-Attention은 모든 입력을 동시에 처리할 수 있어 학습 속도가 빠름.
2. 장거리 의존성 포착: 모든 단어 쌍 간의 관계를 직접 모델링하여 장거리 의존성을 잘 포착함.
3. 해석 가능성: Attention Weight를 시각화하여 모델의 결정 과정을 이해할 수 있음.

## Transformer의 응용

Transformer는 다양한 자연어 처리 태스크에서 뛰어난 성능을 보임.

1. 기계 번역: "The Transformer outperforms traditional seq2seq models in translation tasks."
   → "Transformer는 번역 작업에서 전통적인 seq2seq 모델을 능가합니다."

2. 텍스트 요약: 긴 문서를 압축하여 핵심 내용만 추출할 수 있음.

3. 언어 모델링: GPT (Generative Pre-trained Transformer) 시리즈가 Transformer의 디코더를 기반으로 함.

4. 양방향 언어 이해: BERT (Bidirectional Encoder Representations from Transformers)가 Transformer의 인코더를 기반으로 함.

현재 대부분의 최신 언어 모델들이 Transformer 구조를 기반으로 하고 있음