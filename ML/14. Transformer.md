``` Java
/* !!TODO!!
파일 너무 중구난방으로 저장되는거처럼 보인다. 
큰 주제 잡고 쭉 써내려가는게 더 나으려나??
*/
```
# RNN(Recurrent Neural Network)

RNN은 시계열 데이터를 처리하기 위한 신경망 모델임. 이 모델은 순차적인 데이터나 시간에 따라 변화하는 데이터를 다루는 데 특화되어 있음. 

## 작동 원리

RNN의 핵심은 hidden state라고 불리는 내부 메모리를 사용한다는 점임. 이 hidden state는 이전 시점의 정보를 저장하고 있어, 현재 입력과 함께 처리됨. 수식으로 표현하면 다음과 같음:

$$h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$

$$y_t = g(W_{hy}h_t + b_y)$$

여기서 $h_t$는 현재 시점의 hidden state, $x_t$는 현재 입력, $y_t$는 출력을 나타냄. $W$와 $b$는 각각 가중치와 편향을 의미하며, $f$와 $g$는 활성화 함수임.

## 장점

1. 가변적인 길이의 Input Sequence 처리 가능: RNN은 입력 시퀀스의 길이에 상관없이 동작할 수 있음. 이는 텍스트 분석이나 음성 인식과 같이 입력 길이가 일정하지 않은 태스크에 유용함.

2. 모델 크기의 효율성: 입력이 많아져도 모델의 크기가 증가하지 않음. 이는 RNN이 동일한 가중치를 반복적으로 사용하기 때문임. 이로 인해 메모리 사용이 효율적이며, 학습 파라미터의 수가 줄어듦.

3. 장기 의존성 처리 가능성: 이론적으로 $t$ 시점에서 수행된 계산은 여러 단계 이전의 정보를 사용할 수 있음. 이는 장기 의존성(long-term dependency)을 포착할 수 있는 가능성을 제공함.

4. 가중치 공유: 모든 시간 단계에 동일한 가중치가 적용됨. 이는 모델의 일반화 능력을 향상시키고, 다양한 길이의 시퀀스에 대해 동일한 모델을 사용할 수 있게 함.

## 단점

1. 계산 속도: Recurrent Computation이 느림. 각 시간 단계마다 순차적으로 계산을 수행해야 하므로, 병렬 처리가 어려움.

2. 병렬화의 어려움: Sequence output inference는 병렬화(parallelization)가 어려움. 이는 특히 긴 시퀀스를 처리할 때 성능 저하의 원인이 됨.

3. Vanishing Gradient Problem: 바닐라 RNN은 훈련 중 vanishing gradient problem에 취약함. 이는 역전파 과정에서 그래디언트가 점점 작아져, 장기 의존성을 학습하기 어렵게 만듦.

4. Long-range Dependence 모델링의 한계: 바닐라 RNN은 long-range dependence를 모델링하는 데 종종 실패함. 이는 vanishing gradient problem과 관련이 있으며, 먼 과거의 정보를 현재 시점까지 효과적으로 전달하지 못하는 문제를 야기함.

5. 정보 접근의 어려움: 실제로 여러 단계 이전의 정보에 접근하기 어려움. 시퀀스가 길어질수록 초기 정보가 점점 희석되어 손실됨.

## 단점을 보완하는 방법

1. LSTM (Long Short-Term Memory) / GRU (Gated Recurrent Unit): 
   이 두 모델은 게이트 메커니즘을 도입하여 vanishing gradient problem을 완화하고, long-range dependence를 더 잘 모델링할 수 있게 함. LSTM은 forget gate, input gate, output gate를 사용하여 정보의 흐름을 제어함. GRU는 reset gate와 update gate를 사용하여 LSTM보다 간단한 구조로 비슷한 효과를 냄.

2. Seq2seq model:
   Many-to-many RNN의 한계를 극복하기 위해 개발됨. 인코더-디코더 구조를 사용하여 입력 시퀀스를 고정된 크기의 벡터로 인코딩한 후, 이를 바탕으로 출력 시퀀스를 생성함. 이를 통해 입출력 시퀀스의 길이가 다른 경우에도 유연하게 대응할 수 있음.

3. Attention Model:
   LSTM/GRU도 매우 긴 시퀀스 처리에 한계가 있음. Attention 메커니즘은 디코더가 인코더의 모든 hidden state에 직접 접근할 수 있게 하여, 긴 시퀀스에서도 중요한 정보를 효과적으로 활용할 수 있게 함. 이는 기계 번역, 이미지 캡셔닝 등 다양한 분야에서 성능 향상을 이끌어냄.

4. Transformer:
   RNN의 순차적 계산 문제를 완전히 해결하기 위해 개발된 모델임. Self-attention 메커니즘을 사용하여 시퀀스의 모든 위치를 동시에 고려할 수 있으며, 병렬 처리가 가능해 학습 속도가 빠름. 또한 long-range dependence를 효과적으로 포착할 수 있음.

# Exploding / Vanishing Gradient Problem

그래디언트 소실/폭발 문제는 심층 신경망, 특히 RNN에서 자주 발생하는 문제임. 이는 역전파 과정에서 그래디언트를 계산할 때 발생함.

## 문제의 원인

RNN에서 시간 단계 $t$에서의 손실을 $L_t$라고 할 때, 가중치 $W$에 대한 그래디언트는 다음과 같이 계산됨:

$$\frac{\partial L_t}{\partial W} = \sum_{i=1}^t \frac{\partial L_t}{\partial h_t} \frac{\partial h_t}{\partial h_i} \frac{\partial h_i}{\partial W}$$

여기서 $\frac{\partial h_t}{\partial h_i}$는 시간에 따른 그래디언트의 흐름을 나타냄. 이 항은 다음과 같이 계산됨:

$$\frac{\partial h_t}{\partial h_i} = \prod_{j=i+1}^t \frac{\partial h_j}{\partial h_{j-1}} = \prod_{j=i+1}^t \text{diag}(f'(h_{j-1}))W$$

여기서 $f'$는 활성화 함수의 미분임. 이 곱이 반복되면서 그래디언트가 점점 작아지거나(vanishing) 커질 수(exploding) 있음.

## 예시

시그모이드 함수를 활성화 함수로 사용하는 경우를 생각해보자. 시그모이드 함수의 미분값은 최대 0.25임. 만약 가중치 $W$의 최대 특이값이 4라고 가정하면:

$$\|\frac{\partial h_t}{\partial h_i}\| \leq (4 \times 0.25)^{t-i} = 1^{t-i} = 1$$

이 경우, 시간 간격이 커질수록 그래디언트는 기하급수적으로 작아짐. 반대로, 가중치가 크고 활성화 함수의 기울기가 1에 가까우면 그래디언트가 폭발할 수 있음.

## 영향

1. 학습 불안정: 그래디언트가 불안정해지면 가중치 업데이트가 불안정해지고, 학습이 수렴하지 않거나 발산할 수 있음.
2. 장기 의존성 학습 어려움: 그래디언트가 소실되면 먼 과거의 정보가 현재 시점에 영향을 미치기 어려워짐.
3. 학습 속도 저하: 그래디언트가 매우 작아지면 가중치 업데이트가 거의 일어나지 않아 학습이 매우 느려짐.

이러한 문제들로 인해 RNN의 성능이 저하되고, 특히 장기 의존성을 포착해야 하는 태스크에서 어려움을 겪게 됨. 이를 해결하기 위해 LSTM, GRU 등의 개선된 모델이 제안되었으며, 이들은 게이트 메커니즘을 통해 그래디언트의 흐름을 조절하여 문제를 완화함.

## Long Short Term Memory(LSTM)
(이 부분에 예시를 들어 자세한 설명.)

## Gated Recurrent Units
(이 부분에 예시를 들어 자세한 설명.)

## 두 모델 비교

#