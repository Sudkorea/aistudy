# 선형회귀분석

## 회귀분석의 정의
회귀분석은 관찰된 연속형 변수들에 대해 두 변수 사이의 모형을 구한 뒤 적합도를 측정하는 분석 방법이다.

## 변수
- 변수: 값이 변하는 데이터 요소 또는 속성
- 독립변수: 결과의 원인이 되는 변수
- 종속변수: 독립변수에 종속된 변수

## 선형회귀분석의 정의
선형회귀분석은 종속 변수와 하나 이상의 독립변수 간의 관계를 선형적으로 모델링하는 통계적 방법이다. 주로 독립변수의 값을 기반으로 종속변수의 값을 예측하기 위해 사용된다.

## 선형회귀 방정식

$$y = mx + b$$


- $y$: 종속변수
- $x$: 독립변수
- $m$: 회귀계수 (기울기)
- $b$: $y$절편

## 선형회귀의 가정
1. 선형성: 종속변수와 독립 변수 간의 관계는 선형적이어야 함
2. 독립성: 관측값들은 서로 독립적이어야 함
3. 등분산성: 오류의 분산이 일정해야 함
4. 정규성: 오류가 정규분포를 따라야 함

## 선형회귀의 역사적 배경
- 영국 유전학자 프랜시스 골턴이 부모-자녀의 키 상관관계 연구를 통해 개념을 발전시킴
- 골턴은 부모와 자녀의 키 사이에 선형적 관계가 있으며, 키가 전체 평균으로 회귀하려는 경향이 있다는 가설을 세움
- 이러한 '평균으로의 회귀' 현상으로 인해 '회귀분석'이라는 이름이 붙게 됨

## '선형회귀'라는 이름의 유래
- '선형'은 변수 간의 관계가 직선적임을 나타냄
- '회귀'는 골턴의 연구에서 관찰된 평균으로 돌아가려는 경향을 의미함
- 따라서 '선형회귀'는 변수 간의 선형적 관계를 모델링하며, 평균을 중심으로 한 예측을 수행하는 분석 방법을 지칭하게 됨

## 다중선형회귀
- 정의: 여러 독립 변수를 포함하도록 선형 회귀를 확장한 모델
- 방정식: $y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$
  * $y$: 종속변수
  * $x_1, x_2, ..., x_n$: 독립변수들
  * $b_0$: y절편
  * $b_1, b_2, ..., b_n$: 각 독립변수의 회귀계수

## 최소제곱법(OLS: Ordinary Least Squares) 방법
- 목적: 관측값과 예측값의 차이(잔차)의 제곱합을 최소화하는 매개변수 추정
- 비용 함수:
  $$Cost(m, b) = \sum_{i=1}^n{(y_i-(mx_i+b))^2}$$
  * $y_i$: 실제 관측값
  * $mx_i + b$: 모델의 예측값
- 장점: 계산이 간단하고 해석이 용이함
- 단점: 이상치에 민감할 수 있음

## 모델 평가 지표

### 1. 평균 절대오차(MAE: Mean Absolute Error)
- 정의: 예측값과 실제값의 차이의 절대값 평균
- 수식: $MAE = \frac{1}{n}\sum_{i=1}^n |y_i - \hat{y}_i|$
- 특징: 이상치에 덜 민감하며, 오차의 크기를 직관적으로 이해하기 쉬움

### 2. 평균제곱오차(MSE: Mean Squared Error)
- 정의: 예측값과 실제값 차이의 제곱의 평균
- 수식: $MSE = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2$
- 특징: 큰 오차에 더 큰 가중치를 부여하며, 미분 가능하여 최적화에 유용함

### 3. 제곱근 평균 제곱 오차(RMSE: Root Mean Squared Error)
- 정의: MSE의 제곱근
- 수식: $RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2}$
- 특징: MSE와 같은 특성을 가지며, 원본 데이터와 같은 단위로 해석 가능

### 4. 결정계수($R^2$)
- 정의: 모델이 설명하는 종속변수의 분산 비율
- 수식: $R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}$
  * $\bar{y}$: y의 평균
- 범위: 0 ~ 1 (1에 가까울수록 모델의 설명력이 높음)
- 특징: 모델의 적합도를 나타내며, 다중회귀에서는 조정된 R-squared 사용 권장

# Nearest Neighbor

## 정의
Nearest Neighbor(최근접 이웃)는 분류와 회귀에 사용되는 간단하면서도 효과적인 비모수적 기계학습 알고리즘이다.

## 기본 개념
- 새로운 데이터 포인트의 클래스나 값을 예측할 때, 훈련 데이터셋에서 가장 가까운 데이터 포인트(들)을 찾아 그 정보를 활용한다.
- '가까움'은 주로 유클리드 거리로 측정하지만, 다른 거리 측정 방법도 사용 가능하다.

## 종류

### 1. k-Nearest Neighbors (k-NN)
- k개의 가장 가까운 이웃을 고려하여 결정을 내린다.
- 분류: 다수결 투표 방식으로 클래스 결정
- 회귀: k개 이웃의 평균값으로 예측

### 2. 1-Nearest Neighbor
- k-NN의 특수한 경우로, 가장 가까운 하나의 이웃만 고려한다.

## 장점
1. 구현이 간단하고 이해하기 쉽다.
2. 훈련 과정이 없어 빠르게 적용 가능하다.
3. 비모수적 방법으로, 데이터의 분포에 대한 가정이 필요 없다.

## 단점
1. 대규모 데이터셋에서 예측 시간이 오래 걸릴 수 있다.
2. 차원의 저주에 취약하다.
3. 특성의 스케일에 민감하다.

## 주요 매개변수
1. k: 고려할 이웃의 수
2. 거리 측정 방법: 유클리드 거리, 맨하탄 거리 등
3. 가중치 함수: 거리에 따른 이웃의 영향력 조절

## 적용 분야
- 추천 시스템
- 패턴 인식
- 이상 감지
- 컴퓨터 비전

## 성능 개선 방법
1. 특성 스케일링
2. 차원 축소 기법 적용
3. 앙상블 방법과 결합

## 관련 알고리즘
- KD-Tree, Ball-Tree: 효율적인 최근접 이웃 검색을 위한 자료 구조
- Locality Sensitive Hashing (LSH): 대규모 데이터셋에서의 근사 최근접 이웃 검색
