# Linear Classifier

## 정의
선형 분류기는 입력 특성의 선형 조합을 사용하여 클래스를 분류하는 알고리즘이다.

## 작동 원리
1. 훈련 시: 훈련 데이터에서 각 클래스에 대한 템플릿(가중치 벡터) 학습
2. 테스트 시: 새로운 예제와 학습된 템플릿 간의 유사도 비교

## K-NN과의 비교
- 유사점: 둘 다 거리(유사도) 비교 기반
- 차이점: 
  * 선형 분류기: K개의 클래스 템플릿과만 비교
  * K-NN: N개의 모든 훈련 예제와 비교

## 매개변수적 모델의 장점
1. 공간 효율성
   - 학습 완료 후 가중치 행렬 W만 저장
2. 계산 효율성
   - 테스트 시 단일 행렬-벡터 곱(Wx)으로 예제 평가 가능
   - K-NN보다 빠른 예측 속도

## 한계
1. 제한성 부재
   - 과적합 가능성 존재
2. 해석의 어려움
   - 특히 고차원 데이터에서 각 특성의 영향 파악이 어려움

## 수식
결정 규칙: $\text{class} = \arg\max_k(W_k^T x)$
- $W_k$: k번째 클래스의 가중치 벡터
- $x$: 입력 특성 벡터

# Sigmoid Function

## 정의
시그모이드 함수는 S자 형태의 곡선을 그리는 함수로, 로지스틱 함수라고도 불린다. 선형 분류기의 출력을 확률로 변환하거나, 신경망에서 비선형성을 도입하는 데 자주 사용되는 중요한 함수다.

## 수식
$$\sigma(s) = \frac{1}{1 + e^{-s}}$$

## 특성
- 입력값 $s$: 두 점수의 차이 ($s = s_1 - s_2$)
- 출력 범위: 0에서 1 사이
- $s$가 큰 양수: 1로 수렴
- $s$가 큰 음수: 0으로 수렴
- $s = 0$일 때: 0.5

## 용도
1. 이진 분류에서 확률 출력
2. 신경망의 활성화 함수
3. 로지스틱 회귀에서 핵심 함수

## 장점
1. 비선형성 도입
2. 출력의 범위가 제한됨 (0~1)
3. 미분 가능하며, 미분 형태가 간단함

## 한계
1. 깊은 신경망에서 기울기 소실 문제 발생 가능
2. 출력이 0 또는 1에 정확히 도달하지 않음
