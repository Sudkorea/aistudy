## Perceptron (퍼셉트론)
- 가장 단순한 형태의 인공 신경망
- 입력값과 가중치의 선형 결합을 계산하고 활성화 함수를 통과시켜 출력 생성
- 이진 분류 문제에 주로 사용

## Neural Network with a Single Layer (단층 신경망)
- 입력층과 출력층으로만 구성된 신경망
- 선형 분리 가능한 문제만 해결 가능
- XOR 문제와 같은 비선형 문제 해결 불가

## MLP (Multi-Layer Perceptron, 다층 퍼셉트론)
- 입력층, 하나 이상의 은닉층, 출력층으로 구성
- 비선형 문제 해결 가능
- 복잡한 패턴 학습 가능

### 활성 함수 (Activation Function)
Multi-linear layers의 선형성을 비선형으로 만드는 함수

#### 주요 활성 함수 종류
1. Sigmoid
   - 수식

$$f(x) = \frac{1}{1 + e^{-x}}$$

   - 특징: 출력 범위 (0, 1), 기울기 소실 문제 있음

2. tanh (하이퍼볼릭 탄젠트)
   - 수식 

$$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

   - 특징: 출력 범위 (-1, 1), Sigmoid보다 기울기 소실 문제가 덜함

3. ReLU (Rectified Linear Unit)
   - 수식

$$f(x) = max(0, x)$$

   - 특징: 계산 효율적, 기울기 소실 문제 완화, 음수 입력에 대해 0 출력

4. Leaky ReLU
   - 수식: $f(x) = max(0.01x, x)$
   - 특징: ReLU의 "죽은 뉴런" 문제 해결

5. ELU (Exponential Linear Unit)
   - 수식: $f(x) = x \text{ if } x > 0 \text{ else } \alpha(e^x - 1)$
   - 특징: 음수 입력에 대해 부드러운 기울기 제공

## Computing Gradients (그래디언트 계산)
Gradient Descent (GD)에서는 손실 함수에 대한 각 매개변수의 그래디언트를 계산해야 함

### 그래디언트의 역할
- 각 매개변수가 손실에 미치는 영향을 quantify
- 매개변수 업데이트 방향과 크기 결정

### 계산 방법
1. 순전파 (Forward Propagation)
   - 입력에서 출력까지 계산 수행
   - 각 층의 활성화 값 저장

2. 역전파 (Backpropagation)
   - 출력층에서 입력층 방향으로 그래디언트 계산
   - 체인 룰(Chain Rule) 적용

3. 매개변수 업데이트
   - 계산된 그래디언트를 사용하여 가중치와 편향 조정

### 주의사항
- 기울기 소실/폭발 문제 고려
- 적절한 학습률 선택
- 정규화 기법 적용 (예: L1, L2 정규화)
