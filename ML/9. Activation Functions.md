non-linear한 함수를 어떻게 활성화할 것인지 결정하는 과정에서 나온 개념

1. Sigmoid
   - 수식:
     $$f(x) = \frac{1}{1 + e^{-x}}$$
   - 출력 범위: (0, 1)
   - 기울기 소실: 입력이 크거나 작을 때 기울기가 0에 가까워짐.
     - 이는 역전파 과정에서 그래디언트가 점점 작아져 학습이 어려워지는 문제를 야기함.
   - 출력 값들이 zero-centered하지 않음. 이는 다음 층의 입력에 편향을 줄 수 있음.
   - exp() 연산이 계산 비용이 높음.

2. tanh (하이퍼볼릭 탄젠트)
   - 수식:
     $$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
   - 출력 범위: (-1, 1)
   - Zero-centered 출력을 가짐. 이는 다음 층의 학습에 도움이 됨.
   - 기울기 소실이 여전히 발생함. 입력의 절댓값이 큰 경우 문제가 됨.

3. ReLU (Rectified Linear Unit)
   - 수식:
     $$f(x) = max(0, x)$$
   - 양수 영역에서 기울기가 1로 일정하여 saturate되지 않음.
   - 연산이 매우 효율적임. 단순한 임계값 비교로 구현 가능함.
   - Sigmoid나 tanh보다 빠르게 수렴함. 기울기가 큰 영역이 넓어 학습이 빠름.
   - 출력값이 zero-centered되지 않음. 이는 다음 층의 입력에 편향을 줄 수 있음.
   - Dead ReLU Problem: 입력값이 음수면 뉴런이 비활성화되어 학습에 참여하지 않는 문제가 발생함.
   - x=0에서 미분 불가능함. 실제로는 왼쪽 또는 오른쪽 미분값 중 하나를 선택하여 사용함.

4. Leaky ReLU
   - 수식:
     $$f(x) = max(0.01x, x)$$
   - ReLU의 "죽은 뉴런" 문제를 해결함. 음수 입력에 대해 작은 기울기를 가짐.
   - 학습 가능한 파라미터 α를 도입한 Parametric ReLU도 존재함.

5. ELU (Exponential Linear Unit)
   - 수식:
     $$f(x) = \begin{cases}
     x & \text{ if } x \geq 0 \\ \alpha(e^x - 1) & \text{ else }
     \end{cases}$$
   - 특징: 음수 입력에 대해 부드러운 기울기를 제공함. 이는 노이즈에 대한 robustness를 높임.
   - ReLU와 달리 평균 활성화가 0에 가까워 편향 이동 효과를 줄임.
   - exp 연산이 계산 비용이 높음. 이는 학습 속도를 늦출 수 있음.

각 활성화 함수는 장단점이 있으며, 문제의 특성과 네트워크 구조에 따라 적절한 함수를 선택해야 함. 최근에는 ReLU와 그 변형들이 많이 사용되는 추세임.