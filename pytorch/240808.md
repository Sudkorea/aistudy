## 선형 회귀모델의 테스트 방법

 ### 데이터 분할
- 훈련 세트(Training set): 모델 학습에 사용
- 검증 세트(Validation set): 하이퍼파라미터 튜닝에 사용
- 테스트 세트(Test set): 최종 모델 평가에 사용

일반적인 비율: 60% 훈련, 20% 검증, 20% 테스트
   
   ```python
   from sklearn.model_selection import train_test_split
   
   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
   X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)
   ```

### 모델 평가 지표
앞에서 다룬 MSE 등등

   ```python
   from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
   import numpy as np
   
   mse = mean_squared_error(y_true, y_pred)
   mae = mean_absolute_error(y_true, y_pred)
   r2 = r2_score(y_true, y_pred)
   rmse = np.sqrt(mse)
   ```

### 교차 검증 (Cross-validation):
데이터를 여러 부분집합으로 나누어 반복적으로 평가하는 방법
   
   ```python
   from sklearn.model_selection import cross_val_score
   from sklearn.linear_model import LinearRegression
   
   model = LinearRegression()
   scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')
   ```

### 잔차 분석 (Residual Analysis):
모델의 가정을 검증하고 이상치를 확인하는 방법
   
   ```python
   import matplotlib.pyplot as plt
   
   residuals = y_true - y_pred
   plt.scatter(y_pred, residuals)
   plt.xlabel('Predicted values')
   plt.ylabel('Residuals')
   plt.title('Residual Plot')
   plt.show()
   ```

### 학습 곡선 (Learning Curves):
과적합 또는 과소적합 여부를 판단하는 데 도움
   
   ```python
   from sklearn.model_selection import learning_curve
   
   train_sizes, train_scores, val_scores = learning_curve(
       model, X, y, cv=5, scoring='neg_mean_squared_error',
       train_sizes=np.linspace(0.1, 1.0, 10))
   
   plt.plot(train_sizes, -train_scores.mean(axis=1), label='Training error')
   plt.plot(train_sizes, -val_scores.mean(axis=1), label='Validation error')
   plt.xlabel('Training set size')
   plt.ylabel('Mean Squared Error')
   plt.legend()
   plt.show()
   ```

### 특성 중요도 분석:
각 특성의 영향력을 평가
   
   ```python
   coefficients = model.coef_
   feature_importance = pd.DataFrame({'feature': X.columns, 'importance': abs(coefficients)})
   feature_importance = feature_importance.sort_values('importance', ascending=False)
   ```

### 신뢰 구간 계산:
예측의 불확실성을 quantify
   
   ```python
   from scipy import stats
   
   predictions = model.predict(X_test)
   mse = mean_squared_error(y_test, predictions)
   std_error = np.sqrt(mse)
   confidence_interval = stats.norm.interval(0.95, loc=predictions, scale=std_error)
   ```

### 예측성능의 통계적 검정:
다른 모델과의 성능 차이가 통계적으로 유의미한지 검정
   
   ```python
   from scipy import stats
   
   t_statistic, p_value = stats.ttest_rel(errors_model1, errors_model2)
   ```

## `.eval()`
모델을 평가 모드로 설정하는 데 사용된다. 이 메서드는 nn.Module을 상속받은 모든 모델에서 사용할 수 있다.

### 목적
- 모델을 추론(inference) 또는 평가 모드로 전환한다.
- 학습 시 사용되는 특정 레이어나 기능들을 비활성화한다.

### 영향을 받는 주요 레이어
- Dropout 레이어: 비활성화됨
- BatchNorm 레이어: 고정된 평균과 분산 사용

### 사용 방법
```python
model.eval()
```

### 내부 작동
- model.training = False로 설정됨
- 이는 모델 내의 모든 모듈에 전파됨

### with torch.no_grad()와의 조합
- 그래디언트 계산을 비활성화하여 메모리 사용량을 줄이고 연산 속도를 높임
```python
model.eval()
with torch.no_grad():
   predictions = model(test_data)
```

### 주의사항
- 평가 후 다시 학습할 때는 .train() 메서드로 학습 모드로 전환해야 함
```python
model.train()
```

### 세부 영향
- Dropout: 모든 뉴런을 활성화하고, 각 활성화 값에 dropout 비율을 곱함
- BatchNorm: 학습 중 계산된 이동 평균과 분산을 사용
- 기타 레이어 (예: RNN, LSTM): 일부 내부 동작이 변경될 수 있음

### 수학적 의미
- Dropout의 경우: $E[output] = p * (learned\_weight * input)$
 여기서 p는 유지 확률(1 - dropout_rate)
- BatchNorm의 경우: 

$$\frac{x - E[x]}{\sqrt{Var[x] + ε}}$$

여기서 $E[x]$와 $Var[x]$는 학습 중 계산된 이동 평균과 분산

### 코드 예시
   ```python
   def evaluate(model, test_loader):
       model.eval()  # 평가 모드 설정
       total_loss = 0
       correct = 0
       with torch.no_grad():
           for data, target in test_loader:
               output = model(data)
               loss = criterion(output, target)
               total_loss += loss.item()
               pred = output.argmax(dim=1, keepdim=True)
               correct += pred.eq(target.view_as(pred)).sum().item()
       return total_loss / len(test_loader), correct / len(test_loader.dataset)
   ```

### 성능 영향
- 일반적으로 .eval() 모드에서 모델의 추론 속도가 더 빠름
- 메모리 사용량도 줄어들 수 있음 (특히 torch.no_grad()와 함께 사용 시)

### 모델 저장 및 로드
- 모델의 .eval() 상태는 저장되지 않음
- 모델을 로드한 후 필요에 따라 .eval() 또는 .train()을 명시적으로 호출해야 함

## Batch
데이터셋의 전체 또는 일부 샘플의 집합. 모델 학습 시 한 번에 처리되는 데이터의 단위다

### 배치 크기의 영향
- 큰 배치: 안정적인 그래디언트 추정, 병렬 처리에 유리, 메모리 많이 사용
- 작은 배치: 불안정한 그래디언트, 일반화 성능 향상 가능, 메모리 적게 사용
### 미니 배치 경사하강법
전체 데이터셋을 여러 개의 작은 배치로 나누어 학습하는 방법이다. 각 미니 배치마다 모델 파라미터를 업데이트한다.

수학적으로, 미니 배치 경사하강법의 파라미터 업데이트 규칙은 다음과 같다:

$$θ = θ - η * ∇J(θ)$$

($θ$는 모델 파라미터, $η$는 학습률, $∇J(θ)$는 미니 배치에 대한 비용 함수의 그래디언트)
장점: 배치와 SGD의 장점 결합, 병렬 처리 가능
단점: 배치 크기 선택이 중요

#### 수학적 배경
목적 함수 $J(θ)$의 그래디언트를 미니배치 $B$에 대해 근사한다:

$$∇J(θ) ≈ \frac{1}{|B|}* \sum\limits_{(x,y)∈B}∇l(x, y, θ)$$

여기서 $l(x, y, θ)$는 개별 샘플의 손실 함수다.

#### 미니배치 선택 전략
- 무작위 선택: 각 에폭마다 데이터를 섞고 미니배치로 나눈다.
- 순차적 선택: 데이터 순서대로 미니배치를 구성한다.

#### 미니배치 크기 선택
- 일반적으로 2의 거듭제곱 (32, 64, 128 등)을 사용한다.
- GPU 메모리와 모델 크기를 고려해 선택한다.
- 너무 작으면 학습이 불안정하고, 너무 크면 일반화 성능이 떨어질 수 있다.

#### 적응적 학습률 방법
미니배치 경사하강법의 성능을 개선하기 위해 Adam, RMSprop 등의 최적화 알고리즘을 사용할 수 있다. 이들은 미니배치별로 학습률을 조정한다.

1. Dataset 클래스 정의:
```python
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, x, y):
        self.x = x
        self.y = y
    
    def __len__(self):
        return len(self.x)
    
    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]
```

2. DataLoader 사용:
```python
from torch.utils.data import DataLoader

dataset = CustomDataset(x_data, y_data)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
```

여기서 `batch_size`는 미니 배치의 크기를 지정한다. `shuffle=True`는 각 에폭마다 데이터를 섞어 과적합을 방지한다.

3. 학습 루프:
```python
for epoch in range(num_epochs):
    for batch_x, batch_y in dataloader:
        # 순전파
        predictions = model(batch_x)
        loss = criterion(predictions, batch_y)
        
        # 역전파
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

이 구조를 사용하면 미니 배치 단위로 효율적인 학습이 가능하다. 메모리 사용을 최적화하고 학습 속도를 향상시킬 수 있다.

### `from torch.utils.data import DataLoader, TensorDataset`
#### Dataset 클래스 메서드

| 메서드 | 설명 |
|--------|------|
| `__init__(self, ...)` | 데이터셋 초기화 |
| `__len__(self)` | 데이터셋의 총 샘플 수 반환 |
| `__getitem__(self, idx)` | 인덱스에 해당하는 샘플 반환 |

#### DataLoader 클래스 메서드 및 매개변수

| 메서드/매개변수 | 설명 |
|-----------------|------|
| `__init__(dataset, batch_size=1, shuffle=False, ...)` | DataLoader 초기화 |
| `__iter__()` | 이터레이터 반환 |
| `__next__()` | 다음 배치 데이터 반환 |
| `num_workers` | 데이터 로딩에 사용할 서브프로세스 수 |
| `collate_fn` | 배치 데이터 생성 방식 지정 함수 |
| `pin_memory` | Tensor를 CUDA 고정 메모리에 할당 |
| `drop_last` | 마지막 불완전한 배치 삭제 여부 |

#### 자주 사용되는 Dataset 변형 및 유틸리티 함수:

| 함수/클래스                             | 설명            |
| ---------------------------------- | ------------- |
| `torchvision.transforms`           | 이미지 변형 및 증강   |
| `torch.utils.data.random_split()`  | 데이터셋을 무작위로 분할 |
| `torch.utils.data.Subset()`        | 데이터셋의 부분집합 생성 |
| `torch.utils.data.ConcatDataset()` | 여러 데이터셋 연결    |
| `torch.utils.data.TensorDataset()` | 텐서로부터 데이터셋 생성 |
# 이진 분류 모델
## 로지스틱 회귀
### 개념
이진 분류 문제를 해결하기 위한 통계적 방법이다. 입력 변수와 출력 변수(0 또는 1) 사이의 관계를 모델링한다.

### 너무 공부만 하면 재미 없다. 역사를 좀 짚고 가자.

| 연도        | 인물                                                | 사건                                |
| --------- | ------------------------------------------------- | --------------------------------- |
| 1805      | Adrien-Marie Legendre                             | 최소제곱법 발표                          |
| 1809      | Carl Friedrich Gauss                              | 최소제곱법의 기본 원리 발표                   |
| 1821      | Gauss                                             | 최소제곱법의 최적성 증명                     |
| 1844      | Pierre François Verhulst                          | 로지스틱 함수 도입 (인구 성장 모델링)            |
| 1886      | Francis Galton                                    | 회귀(regression)라는 용어 도입            |
| 1901      | Karl Pearson                                      | 상관 계수와 주성분 분석 개발                  |
| 1922      | Ronald Fisher                                     | 최대우도추정법 개발                        |
| 1931      | Jerzy Neyman                                      | 신뢰구간 개념 도입                        |
| 1936      | Fisher                                            | 선형판별분석(LDA) 개발                    |
| 1943      | Warren McCulloch, Walter Pitts                    | 인공 신경망에 시그모이드 함수 도입               |
| 1951      | David Cox                                         | 비례위험모형 개발                         |
| 1958      | Cox                                               | 로지스틱 회귀를 통계학에 도입                  |
| 1960s-70s | 여러 연구자                                            | 로지스틱 회귀가 의학, 사회과학 분야에서 널리 사용됨     |
| 1970      | Arthur Dempster, Nan Laird, Donald Rubin          | EM 알고리즘 개발                        |
| 1972      | Nelder, Wedderburn                                | 일반화 선형 모형(GLM) 제안                 |
| 1980s     | 여러 연구자                                            | 로지스틱 회귀가 기계학습 분야에서 주요 알고리즘으로 자리잡음 |
| 1986      | David Rumelhart, Geoffrey Hinton, Ronald Williams | 오차역전파 알고리즘 개발, 신경망 학습에 로지스틱 함수 사용 |
| 1990s-현재  | 여러 연구자                                            | 로지스틱 회귀의 변형 및 개선 (예: L1, L2 정규화)  |

가우스 저양반은 빠지질 않네

로지스틱 함수:
- 1844년 Pierre François Verhulst가 인구 성장을 모델링하기 위해 도입했다.
- 생태학, 화학, 물리학 등 다양한 분야에서 사용되었다.

로지스틱 회귀:
- 1958년 David Cox가 통계학에 도입했다.
- 1960년대와 1970년대에 걸쳐 의학 및 사회과학 분야에서 널리 사용되기 시작했다.

시그모이드 함수:
- 19세기 말 신경과학자들이 뉴런의 활성화 함수로 사용하기 시작했다.
- 1943년 Warren McCulloch와 Walter Pitts가 인공 신경망에 도입했다.

### 수학적 모델

$$y = σ(w^T x + b)$$

- $y$는 예측 확률 ($0 \leq y \leq 1$)
- $x$는 입력 특성 벡터
- $w$는 가중치 벡터
- $b$는 편향
- $σ$는 시그모이드 함수

로지스틱 회귀 모델은 선형 회귀를 확장한 것이다. 선형 회귀에서

$$y = w^T x + b$$

이 모델을 0과 1 사이의 확률로 변환하기 위해 시그모이드 함수를 적용한다

$$P(y=1|x) = σ(w^T x + b)$$

이는 로그 오즈(log-odds) 또는 로짓(logit) 함수의 역함수다

$$log(\frac{p}{1-p}) = w^T x + b$$

여기서 $p$는 양성 클래스($y=1$)에 속할 확률이다.

### 시그모이드 함수

$$σ(z) = \frac{1}{ (1 + e^{-z})}$$

- 범위가 (0, 1)이다.
- 단조 증가한다.
- S자 형태의 곡선이다.
- 미분 가능하다: $σ'(z) = σ(z)(1 - σ(z))$

이러한 특성들이 이진 분류에 적합하다.

### 로지스틱 회귀와 시그모이드 함수의 관계

로지스틱 회귀는 선형 모델과 시그모이드 함수를 결합한 것이다. 이는 다음과 같은 이점을 제공한다:

- 확률 해석
  출력을 확률로 해석할 수 있다.
- 비선형성
  선형 결정 경계를 비선형으로 변환한다.
- 미분 가능성
  경사 하강법을 사용한 최적화가 가능하다.

### 결정 경계
예측 확률이 0.5를 넘으면 클래스 1로, 그렇지 않으면 클래스 0으로 분류한다.

### 비용 함수 (로그 손실 또는 이진 교차 엔트로피)
'로그 손실'이라는 용어는 주로 Kaggle 같은 데이터 과학 경연에서 사용되며, 수학적으로 부르는 이름이 이진 교차 엔트로피임.

#### 역사적 배경
교차 엔트로피 개념은 정보 이론에서 유래했다. Claude Shannon이 1948년에 발표한 논문 "A Mathematical Theory of Communication"에서 처음 소개됐다. 이후 기계학습 분야에서 손실 함수로 널리 사용되기 시작했다.

#### 이론
이진 교차 엔트로피는 두 확률 분포 간의 차이를 측정하는 방법이다. 로지스틱 회귀에서는 모델의 예측 확률과 실제 레이블 간의 차이를 측정하는 데 사용된다.

#### 정의

$$H(y, p) = -[y log(p) + (1-y) log(1-p)]$$

$y$ : 실제 레이블 (0 또는 1)
$p$ : 모델이 예측한 양성 클래스의 확률

$m$개의 샘플에 대한 평균 이진 교차 엔트로피는

$$J(w,b) = -\frac{1}{m}\sum [y^i log(h_w(x^i)) + (1-y^i) log(1-h_w(x^i))]$$

- $m$은 훈련 샘플 수
- $y^i$는 $i$번째 샘플의 실제 레이블
- $h_w(x^i)$는 $i$번째 샘플에 대한 모델의 예측

#### 특징
- 비대칭성: 오차의 방향에 따라 페널티가 다르다.
- 볼록 함수: 전역 최소값을 보장한다.
- 최대 우도 추정과의 연관성: 음의 로그 우도를 최소화하는 것과 동일하다.

#### 로지스틱 회귀에서의 사용
로지스틱 회귀에서 이진 교차 엔트로피를 최소화하는 것은 조건부 우도를 최대화하는 것과 같다. 이는 모델 파라미터의 최적 추정치를 제공한다.

#### 그래디언트

$$\frac{∂H}{∂θ} = (\hat{y} - y) * x$$

($θ$는 모델 파라미터, $\hat{y}$는 예측값, $y$는 실제값, $x$는 입력 특성)
이 그래디언트는 경사 하강법을 통한 모델 학습에 사용된다.
### 가능도 (Likelihood)
가능도는 주어진 데이터가 특정 확률 모델에서 나왔을 가능성을 나타내는 함수다. 파라미터 $θ$를 가진 모델에서 데이터 $x$가 관측될 확률을 $P(x|θ)$라고 할 때, 가능도 $L(θ|x)$는 이와 같다

$$L(θ|x) = P(x|θ)$$

### 최대 가능도 추정 (MLE)
MLE는 관측된 데이터를 가장 잘 설명하는 모델 파라미터를 찾는 방법이다. 수학적으로:

$$θ_{MLE} = argmax_θ * L(θ|x)$$

로그 가능도를 사용하면 계산이 더 쉬워지므로, 보통 로그 가능도를 최대화한다

$$θ_{MLE} = argmax_θ *log L(θ|x)$$

### 이진 교차 엔트로피를 유도하는 방법
#### 이진 분류에서의 가능도
이진 분류 문제에서, 각 데이터 포인트 $x_i$에 대해 $y_i ∈ \{0, 1\}$인 레이블이 있다. 로지스틱 회귀 모델은 $P(y_i = 1|x_i, θ) = σ(θ^T x_i)$로 표현된다. 여기서 $σ$는 시그모이드 함수다.

단일 데이터 포인트의 가능도

$$P(y_i|x_i, θ) = [σ(θ^T x_i)]^{y_i} [1 - σ(θ^T x_i)]^{1-y_i}$$

전체 데이터셋의 가능도

$$L(θ) = \prod\limits_{i} P(y_i|x_i, θ)$$

#### 로그 가능도

$$log L(θ) = \sum\limits_{i} [y_i log(σ(θ^T x_i)) + (1-y_i) log(1 - σ(θ^T x_i))]$$

#### 이진 교차 엔트로피 유도
MLE는 로그 가능도를 최대화하는 것이다. 이는 음의 로그 가능도를 최소화하는 것과 같다:

$$-log L(θ) = -\sum\limits_{i} [y_i log(σ(θ^T x_i)) + (1-y_i) log(1 - σ(θ^T x_i))]$$

이 식의 우변은 이진 교차 엔트로피의 정의와 정확히 일치한다. 따라서 이진 교차 엔트로피를 최소화하는 것은 로그 가능도를 최대화하는 것과 같다.

결론적으로, 로지스틱 회귀에서 이진 교차 엔트로피를 손실 함수로 사용하는 것은 MLE 원칙을 따르는 것과 동일하다. 이는 모델의 예측이 실제 데이터 분포에 가장 잘 맞도록 파라미터를 조정하는 것을 의미한다.


### 최적화
경사 하강법을 사용해 비용 함수를 최소화한다.

$$w := w - α \times \frac {∂J}{∂w}$$

$$b := b - α \times \frac {∂J}{∂b}$$

($α$는 학습률)

### 정규화 
과적합을 방지하기 위해 L1(Lasso) 또는 L2(Ridge) 정규화를 사용할 수 있다.

| 장점                                                      | 단점                                                 |
| ------------------------------------------------------- | -------------------------------------------------- |
| - 해석이 쉽다<br>- 구현이 간단하다<br>- 과적합 위험이 낮다<br>- 확률을 직접 출력한다 | - 비선형 결정 경계를 모델링할 수 없다<br>- 고차원 데이터에서 성능이 떨어질 수 있다 |

### 다중 클래스 확장
소프트맥스 회귀를 통해 다중 클래스 분류로 확장할 수 있다.

### 평가 지표
- 정확도 (Accuracy)
- 정밀도 (Precision)
- 재현율 (Recall)
- F1 점수
- ROC 곡선과 AUC



### PyTorch에서 사용하기

1. `import`
```python
import torch
import torch.nn as nn
import torch.optim as optim
```

2. 데이터 준비
데이터를 텐서로 변환하는 일련의 과정

3. 모델 정의:
```python
class LogisticRegression(nn.Module):
    def __init__(self, input_dim):
        super(LogisticRegression, self).__init__()
        self.linear = nn.Linear(input_dim, 1)
    
    def forward(self, x):
        return torch.sigmoid(self.linear(x))
```

4. 모델 초기화:
```python
model = LogisticRegression(input_dim=X.shape[1])
```

5. 손실 함수와 옵티마이저 정의:
```python
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
```

6. 학습 루프:
```python
num_epochs = 1000
for epoch in range(num_epochs):
    outputs = model(X)
    loss = criterion(outputs, y)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

7. 예측:
```python
with torch.no_grad():
    predicted = model(X)
    predicted = predicted > 0.5
```
