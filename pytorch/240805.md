# Pytorch
## Pytorch란?
- 간편한 딥러닝 API를 제공하며, 머신러닝 알고리즘을 구현하고 실행하기 위한 확장성이 뛰어난 멀티플랫폼 프로그래밍 인터페이스(Raschka, Liu & Mirjalili, 2022)
- FAIR(Facebook AI Research) 연구소의 연구원과 엔지니어들에 의해 개발된 오픈소스 딥러닝 프레임워크. 2016.9에 처음 출시함.
	- PyTorch는 Torch라는 Lua 기반의 프레임워크를 Python으로 재구현한 것으로, 동적 계산 그래프를 지원하는 것이 큰 특징임.
- 사용자 친화성을 강조하고 복잡한 작업도 쉽게 처리할 수 있도록 개발되어, 학계와 산업계의 많은 머신러닝 연구자들과 실무자들이 딥러닝 솔루션을 개발하기 위해 PyTorch를 사용하고 있음.
- PyTorch의 주요 장점 중 하나는 '즉시 실행(eager execution)' 모드로, 코드를 한 줄씩 실행하며 즉시 결과를 확인할 수 있어 디버깅이 용이함.
- GPU 가속을 지원하여 대규모 데이터셋에 대한 학습을 빠르게 수행할 수 있음.
- TorchScript를 통해 모델을 C++환경에서도 실행할 수 있어 production 환경에서의 활용도가 높음.
- 커뮤니티가 활성화되어 있어 다양한 pre-trained 모델과 라이브러리를 쉽게 찾아 사용할 수 있음.

## Tensor
### Tensor란?
- PyTorch의 핵심 데이터 구조. NumPy의 다차원 배열과 유사한 형태로 데이터를 표현함.
- Q : DL을 위한 매서드들이 많이 정의된 C++의 vector같은건가?
  A : C++의 vector보다는 더 복잡하고 기능이 많은 구조. 다차원 배열에 더 가깝다고 볼 수 있음.
	- 다차원 배열: tensor는 1차원뿐만 아니라 여러 차원의 데이터를 쉽게 다룰 수 있음.
	- GPU 지원: tensor는 GPU에서 연산을 수행할 수 있어서 딥러닝 작업을 빠르게 처리할 수 있음.
	- 자동 미분: PyTorch의 tensor는 자동 미분 기능을 지원함. 이건 딥러닝에서 역전파를 계산할 때 아주 중요.
	- 특화된 연산: 딥러닝에 필요한 다양한 수학적 연산들이 이미 구현되어 있음.
	DL 최적화 객체라고 이해하면 될듯.

### n-dim tensor

| Dim | Method                          |
| --- | ------------------------------- |
| 0   | torch.tensor(a)                 |
| 1   | torch.tensor(\[a,b,...])        |
| 2   | torch.tensor(\[\[a,b],\[c,d\]]) |
| ... | ...                             |
이런 식으로, torch.tensor() 안에 원하는 차원 list 넣으면 알아서 변환시켜줌.
### Tensor에 쓰이는 데이터 타입
- 정수형
	- 8비트 부호 있는/없는 정수, 16비트 부호 있는 정수, 32비트 부호 있는 정수, 64비트 부호 있는 정수 유형으로 구분함.
- 실수형
	- 32비트 부동 소수점 수, 64비트 부동 소수점 수의 유형 등으로 구분함.
	- 고정 소수점 수로 표현하면, 한 자리수(0~9)당 1byte를 차지하므로 메모리 낭비가 심함. 따라서, 가수부와 지수부로 나누어서 숫자를 표현하고, 이 방식을 부동소수점수라고 부름.
	- 신경망의 수치계산에서 사용됨.

정수를 정리하면 다음과 같음.

| dtype          | range of value | Method                     |
| -------------- | -------------- | -------------------------- |
| 8bit 부호 없는 정수  | 0~255          | torch.uint8                |
| 8bit 부호 있는 정수  | -128~127       | torch.int8                 |
| 16bit 부호 있는 정수 | -32768~32767   | torch.int16 or torch.short |
| 32bit 부호 있는 정수 | -2^31~2^31-1   | torch.int32 or torch.int   |
| 64bit 부호 있는 정수 | -2^63~2^63-1   | torch.int64 or torch.long  |

실수는 비트가 부호/지수부/가수부 세 파트로 나뉘어짐.

| dtype          | sign/exponent/mantissa | Method                        |
| -------------- | ---------------------- | ----------------------------- |
| 16bit 부동 소수점 수 | 1/7/8                  | torch.float16 or torch.half   |
| 32bit 부동 소수점 수 | 1/8/23                 | torch.float32 or torch.float  |
| 64bit 부동 소수점 수 | 1/11/52                | torch.float64 or torch.double |

이외에 쓰이는 types
- Boolean:
    - torch.bool: True/False 값을 저장하는 데 사용
- Complex:
    - torch.complex64: 32비트 실수부와 32비트 허수부로 구성
    - torch.complex128: 64비트 실수부와 64비트 허수부로 구성
- BFloat16:
    - torch.bfloat16: Brain Floating Point, 16비트 부동 소수점의 변형
    - 기계 학습에 최적화된 형식으로, float32와 같은 동적 범위를 가지지만 정밀도는 낮음
#### Type Casting
``` python
i = torch.tensor([2,3,4], dtype = torch.int8)

j = i.float()
k = i.double()
```
위와 같이 형변환이 가능함.

### Methods
#### Calculation

| Category | Method            | Explanation                 |     |
| -------- | ----------------- | --------------------------- | --- |
| 텐서 생성    | torch.tensor()    | 데이터로부터 텐서 생성                |     |
|          | torch.zeros()     | 모든 원소가 0인 텐서 생성             |     |
|          | torch.ones()      | 모든 원소가 1인 텐서 생성             |     |
|          | torch.rand()      | 0과 1 사이의 균일 분포에서 랜덤하게 생성    |     |
|          | torch.randn()     | 표준 정규 분포에서 랜덤하게 생성          |     |
|          | torch.arange()    | 시작값부터 끝값까지 일정 간격의 1차원 텐서 생성 |     |
| 텐서 조작    | .reshape()        | 텐서의 형태 변경                   |     |
|          | .view()           | 텐서의 형태 변경 (메모리 공유)          |     |
|          | .squeeze()        | 차원이 1인 차원 제거                |     |
|          | .unsqueeze()      | 지정된 위치에 1인 차원 추가            |     |
|          | .transpose()      | 두 차원을 교환                    |     |
|          | .permute()        | 모든 차원의 순서 변경                |     |
| 텐서 연산    | .add(), +         | 덧셈                          |     |
|          | .sub(), -         | 뺄셈                          |     |
|          | .mul(), *         | 원소별 곱셈                      |     |
|          | .div(), /         | 나눗셈                         |     |
|          | .matmul(), @      | 행렬 곱셈                       |     |
|          | .pow()            | 거듭제곱                        |     |
| 통계       | .mean()           | 평균 계산                       |     |
|          | .sum()            | 합계 계산                       |     |
|          | .max()            | 최댓값 계산                      |     |
|          | .min()            | 최솟값 계산                      |     |
|          | .std()            | 표준편차 계산                     |     |
| 인덱싱      | .index_select()   | 특정 인덱스의 값 선택                |     |
|          | .masked_select()  | 마스크 조건에 맞는 값 선택             |     |
| 변환       | .to()             | 데이터 타입 또는 디바이스 변경           |     |
|          | .cuda()           | GPU로 텐서 이동                  |     |
|          | .cpu()            | CPU로 텐서 이동                  |     |
| 그래디언트    | .backward()       | 역전파 수행                      |     |
|          | .grad             | 그래디언트 값 접근                  |     |
|          | .requires_grad_() | 그래디언트 계산 여부 설정              |     |

외적, 내적도 있음

| Calculation | Method        | Explanation                       |
| ----------- | ------------- | --------------------------------- |
| 내적          | torch.dot()   | 두 1차원 벡터의 내적 계산                   |
|             | torch.inner() | 다차원 텐서에 대한 내적 계산 (1차원 벡터에도 사용 가능) |
| 외적          | torch.cross() | 두 3차원 벡터의 외적 계산                   |
|             | torch.outer() | 두 1차원 벡터의 외적 계산 (결과는 2차원 텐서)      |

#### Others
| Method               | Explanation                        |
| -------------------- | ---------------------------------- |
| .dim()               | 텐서의 차원 수 반환                        |
| .size()              | 텐서의 크기(shape) 반환                   |
| .shape               | 텐서의 크기(shape)를 나타내는 속성             |
| .numel()             | 텐서의 전체 원소 개수 반환                    |
| .dtype               | 텐서의 데이터 타입을 나타내는 속성                |
| .device              | 텐서가 저장된 디바이스(CPU/GPU)를 나타내는 속성     |
| .is_cuda             | 텐서가 CUDA 디바이스에 있는지 확인              |
| .is_floating_point() | 텐서가 부동소수점 타입인지 확인                  |
| .stride()            | 각 차원을 따라 다음 요소로 이동하는 데 필요한 단계 수 반환 |
| .ndimension()        | .dim()과 동일                         |
| .element_size()      | 각 요소의 바이트 크기 반환                    |
| .nelement()          | .numel()과 동일                       |
#### 구체적인 사용 예시

1. torch.zeros(), torch.ones()
   - 모든 요소가 0 또는 1인 텐서를 생성
   - 예시:
     ```python
     a = torch.zeros([3, 2, 4])  # 3x2x4 크기의 3차원 텐서, 모든 요소가 0
     b = torch.ones(5)  # 길이가 5인 1차원 텐서, 모든 요소가 1
     ```

2. torch.zeros_like(), torch.ones_like()
   - 주어진 텐서와 같은 크기의 새 텐서를 생성하며, 모든 요소를 0 또는 1로 채움
   - 예시:
     ```python
     original = torch.rand(3, 4)
     c = torch.zeros_like(original)  # original과 같은 크기(3x4)의 텐서, 모든 요소가 0
     ```

3. torch.rand()
   - [0, 1] 구간의 균등 분포에서 랜덤하게 값을 추출하여 텐서를 생성
   - 예시:
     ```python
     d = torch.rand(2, 3)  # 2x3 크기의 텐서, 각 요소는 0과 1 사이의 랜덤 값
     ```

4. torch.randn()
   - 평균이 0이고 표준편차가 1인 정규 분포에서 랜덤하게 값을 추출하여 텐서를 생성
   - 예시:
     ```python
     e = torch.randn(3, 2)  # 3x2 크기의 텐서, 각 요소는 표준 정규 분포에서 추출된 값
     ```

5. torch.arange(start=, end=, step=)
   - 시작값부터 끝값(포함하지 않음)까지 일정한 간격으로 값을 생성
   - 예시:
     ```python
     f = torch.arange(1, 10, 2)  # [1, 3, 5, 7, 9]를 포함하는 1차원 텐서
     ```

1. torch.linspace(start, end, steps)
   - 시작값부터 끝값(포함)까지 지정된 개수만큼 균등하게 나눈 값들로 텐서를 생성
   - 예시:
     ```python
     g = torch.linspace(0, 1, 5)  # [0.0000, 0.2500, 0.5000, 0.7500, 1.0000]
     ```

7. torch.eye(n)
   -  n x n 크기의 단위 행렬(대각선이 1이고 나머지가 0인 행렬)을 생성
   - 예시:
     ```python
     h = torch.eye(3)  # 3x3 단위 행렬
     ```

8. torch.full(size, fill_value)
   - 지정된 크기의 텐서를 생성하고 모든 요소를 지정된 값으로 채움
   - 예시:
     ```python
     i = torch.full((2, 2), 3.14)  # 2x2 텐서, 모든 요소가 3.14
     ```

#### 초기화되지 않은 Tensor
``` c++
#include <vector>

using namespace std;

int main() {
	v = vector<int>(3);
}
```
이런거임. 왜 초기화되지 않은걸 사용하냐?
- 성능 향상: 텐서를 생성하고 곧바로 다른 값으로 덮어씌울 예정이라면, 초기화 과정을 건너뛰어 성능을 향상시킬 수 있음
- 메모리 사용 최적화: 대규모 텐서의 경우, 불필요한 초기화로 인한 메모리 사용량 증가를 피할 수 있음
``` python
# 초기화되지 않은 텐서 생성
q = torch.empty(3, 4)

# 생성된 텐서에 값 채우기
q.fill_(3.0)

# 또는 다른 연산 결과로 덮어쓰기
q = torch.matmul(torch.randn(3, 2), torch.randn(2, 4))
```

#### list, numpy로 Tensor 생성하는 법
- list : torch.tensor()
  ```python
# 1차원 리스트
list_1d = [1, 2, 3, 4]
tensor_1d = torch.tensor(list_1d)

# 2차원 리스트
list_2d = [[1, 2], [3, 4], [5, 6]]
tensor_2d = torch.tensor(list_2d)
```
- numpy : torch.from_Numpy(u)
```python
import numpy as np

# NumPy 배열 생성
numpy_array = np.array([1, 2, 3, 4])

# NumPy 배열로부터 텐서 생성
tensor_from_numpy = torch.from_numpy(numpy_array)

# 실수형으로 변환
tensor_float = tensor_from_numpy.float()
```
주의사항 및 추가 정보:
- NumPy 배열로부터 생성된 텐서는 기본적으로 NumPy 배열의 데이터 타입을 따름. 따라서 실수형 NumPy 배열을 사용하면 실수형 텐서가 생성됨
- `torch.from_numpy()`로 생성된 텐서는 NumPy 배열과 메모리를 공유함. 즉, 하나를 변경하면 다른 하나도 변경됨.
- NumPy 배열의 데이터 타입에 따라 적절한 PyTorch 데이터 타입으로 자동 변환됨. 예를 들어, `np.float64`는 `torch.float64`로 변환됨

```python
# NumPy 배열과 텐서 간의 메모리 공유 예시
numpy_array = np.array([1, 2, 3])
tensor = torch.from_numpy(numpy_array)
numpy_array[0] = 5
print(tensor)  # tensor([5, 2, 3])
```

| 메소드 | 설명 | 예시 |
|--------|------|------|
| torch.tensor() | 데이터로부터 텐서 생성 | torch.tensor([1, 2, 3]) |
| torch.zeros() | 모든 원소가 0인 텐서 생성 | torch.zeros(3, 4) |
| torch.ones() | 모든 원소가 1인 텐서 생성 | torch.ones(2, 3) |
| torch.rand() | 0~1 균일 분포에서 랜덤 텐서 생성 | torch.rand(3, 3) |
| torch.randn() | 표준 정규 분포에서 랜덤 텐서 생성 | torch.randn(2, 4) |
| torch.arange() | 시작값부터 끝값까지 순차적 텐서 생성 | torch.arange(0, 10, 2) |
| torch.linspace() | 시작값부터 끝값까지 균등 간격 텐서 생성 | torch.linspace(0, 1, 5) |
| torch.eye() | 단위 행렬(Identity matrix) 생성 | torch.eye(3) |
| torch.empty() | 초기화되지 않은 텐서 생성 | torch.empty(2, 3) |
| torch.full() | 특정 값으로 채워진 텐서 생성 | torch.full((2, 2), 3.14) |
| torch.from_numpy() | NumPy 배열로부터 텐서 생성 | torch.from_numpy(np_array) |
#### CPU와 GPU
1. 기본 생성 위치:
   - PyTorch에서 텐서는 기본적으로 CPU에 생성됨

2. GPU로 이동:
   - CUDA가 지원되는 환경에서는 텐서를 GPU로 이동시킬 수 있음
   - 이는 `to()` 메소드나 `cuda()` 메소드를 사용해 수행할 수 있음

| Method                           | Explanation          |
| -------------------------------- | -------------------- |
| torch.cuda.is_available()        | CUDA 사용 가능 여부 확인     |
| torch.cuda.device_count()        | 사용 가능한 GPU 개수 반환     |
| torch.cuda.current_device()      | 현재 선택된 GPU 번호 반환     |
| torch.cuda.get_device_name()     | 지정된 GPU의 이름 반환       |
| tensor.to('cuda')                | 텐서를 GPU로 이동          |
| tensor.cuda()                    | 텐서를 GPU로 이동 (간편 메소드) |
| torch.tensor(..., device='cuda') | GPU에 직접 텐서 생성        |

3. GPU에 직접 생성:
   - 텐서를 생성할 때 바로 GPU에 생성하는 것도 가능함

예시 코드:

```python
import torch

# CPU에 텐서 생성 (기본)
cpu_tensor = torch.randn(3, 3)

# GPU 사용 가능 여부 확인
if torch.cuda.is_available():
    # CPU 텐서를 GPU로 이동
    gpu_tensor = cpu_tensor.to('cuda')
    # 또는
    gpu_tensor = cpu_tensor.cuda()

    # GPU에 직접 텐서 생성
    direct_gpu_tensor = torch.randn(3, 3, device='cuda')

    print(f"CPU 텐서 위치: {cpu_tensor.device}")
    print(f"GPU로 이동된 텐서 위치: {gpu_tensor.device}")
    print(f"GPU에 직접 생성된 텐서 위치: {direct_gpu_tensor.device}")
else:
    print("CUDA를 사용할 수 없습니다.")
```

주의할 점:
- GPU 사용 여부 확인: `torch.cuda.is_available()`을 통해 GPU 사용 가능 여부를 항상 체크하는 것이 좋음
- 연산 속도: GPU에서의 연산이 항상 빠른 것은 아님. 작은 크기의 데이터나 간단한 연산의 경우 CPU가 더 빠를 수 있음
- 메모리 관리: GPU 메모리는 제한적이므로, 큰 텐서를 다룰 때는 메모리 사용량에 주의해야 함
- 디바이스 일치: 연산을 수행할 때 모든 텐서가 같은 디바이스(CPU 또는 GPU)에 있어야 함

##### 근데 왜 갑자기 GPU?? GPU를 AI 연구 및 개발에 사용하는 이유와 배경
1. 병렬 처리 능력:
   - GPU는 수천 개의 작은 코어로 구성되어 있어 동시에 많은 연산을 처리할 수 있음.
   - 딥러닝의 행렬 연산과 같은 병렬화 가능한 작업에 매우 적합함.

2. 그래픽 처리에서 AI로의 전환:
   - GPU는 원래 그래픽 렌더링을 위해 설계됨.
   - 그래픽 처리와 딥러닝 연산이 유사한 병렬 처리 특성을 가짐을 발견함.
   - NVIDIA가 CUDA를 개발하여 GPU를 일반 연산에 활용할 수 있게 됨.

3. 높은 메모리 대역폭:
   - GPU는 CPU보다 훨씬 높은 메모리 대역폭을 가짐.
   - 대량의 데이터를 빠르게 처리할 수 있어 딥러닝 학습에 유리함.

4. 비용 효율성:
   - 동일한 성능의 CPU 클러스터보다 GPU가 더 저렴하고 전력 효율이 높음.

5. 딥러닝 프레임워크의 지원:
   - PyTorch, TensorFlow 등의 프레임워크가 GPU 연산을 잘 지원함.
   - 개발자가 쉽게 GPU를 활용할 수 있는 환경을 제공함.

6. 지속적인 발전:
   - NVIDIA 등의 회사가 AI 연구용 GPU를 지속적으로 개발하고 있음.
   - 텐서 코어 등 AI 특화 기능을 추가하여 성능을 개선함.
##### CPU, GPU, TPU의 차이 정리

| 특성 | CPU | GPU | TPU |
|------|-----|-----|-----|
| 전체 구조 | 범용 처리 장치 | 그래픽 및 병렬 처리 특화 | AI 연산 특화 |
| 코어 수 | 적음 (수~수십 개) | 많음 (수천 개) | 매우 많음 (수만 개) |
| 클럭 속도 | 높음 | 중간 | 낮음 |
| 병렬 처리 | 제한적 | 뛰어남 | 매우 뛰어남 |
| 메모리 대역폭 | 낮음 | 높음 | 매우 높음 |
| 유연성 | 매우 높음 | 중간 | 낮음 |
| 주요 용도 | 일반적인 컴퓨팅 작업 | 그래픽 렌더링, 딥러닝 | 딥러닝 특화 |
| 프로그래밍 난이도 | 쉬움 | 중간 | 어려움 |
| 전력 효율 | 낮음 | 중간 | 높음 |
| 가격 | 저렴 | 중간 | 고가 |
| 강점 | 범용성, 복잡한 연산 | 병렬 처리, 행렬 연산 | AI 연산 최적화 |
| 약점 | 병렬 처리 제한적 | 일반 연산 비효율적 | 특정 AI 연산에 제한적 |
| 대표 제조사 | Intel, AMD | NVIDIA, AMD | Google |
| 소프트웨어 지원 | 매우 광범위 | 널리 지원됨 | 제한적 |
| 학습 곡선 | 낮음 | 중간 | 높음 |
| 시장 점유율 | 매우 높음 | 높음 | 낮음 (성장 중) |

추가로 알면 좋을 내용:
1. CPU는 복잡한 순차적 작업에 적합하며, 운영체제 관리와 같은 다양한 작업을 처리할 수 있음.
2. GPU는 CUDA나 OpenCL과 같은 병렬 컴퓨팅 프레임워크를 통해 일반 연산에도 활용됨.
3. TPU는 Google에서 개발한 AI 특화 칩으로, TensorFlow와 같은 특정 프레임워크에 최적화되어 있음.
4. 최근에는 CPU, GPU, TPU의 장점을 결합한 하이브리드 아키텍처도 연구되고 있음.
5. 엣지 컴퓨팅의 발전으로 모바일 기기나 IoT 장치에서도 AI 연산을 위한 특화 칩이 사용되기 시작함.
