# Linear Regression
## 의미
독립 변수와 종속 변수 간의 선형 관계를 모델링하는 통계적 방법. 이 방법은 데이터 포인트들을 가장 잘 나타내는 직선(또는 고차원에서는 평면)을 찾는 것을 목표로 함

## 활용 예
- 부동산 가격 예측
   - 집의 크기, 방 개수, 위치 등을 이용해 주택 가격을 예측
- 판매 예측
   - 광고 지출, 경제 지표 등을 바탕으로 향후 판매량을 예측
- 의료 분야
   - 환자의 나이, 혈압, 콜레스테롤 수치 등을 이용해 특정 질병 위험을 평가
- 농업
   - 강수량, 온도 등의 기후 데이터를 이용해 작물 수확량을 예측
- 금융
   - 다양한 경제 지표를 이용해 주식 가격 변동을 예측
- 교육
   - 공부 시간, 출석률 등을 이용해 학생의 시험 성적을 예측

## 과정
| 단계 | 과정 | 설명 | 주요 메서드/클래스 |
|------|------|------|-------------------|
| 1 | 데이터 준비 | 데이터를 PyTorch 텐서로 변환 | `torch.FloatTensor()` |
| 2 | 데이터셋 생성 | 텐서를 PyTorch 데이터셋으로 변환 | `TensorDataset()` |
| 3 | 데이터로더 생성 | 배치 처리를 위한 데이터로더 설정 | `DataLoader()` |
| 4 | 모델 정의 | 선형 회귀 모델 클래스 정의 | `nn.Module`, `nn.Linear()` |
| 5 | 손실 함수 정의 | 평균 제곱 오차(MSE) 손실 함수 설정 | `nn.MSELoss()` |
| 6 | 옵티마이저 정의 | 확률적 경사 하강법(SGD) 옵티마이저 설정 | `optim.SGD()` |
| 7 | 모델 훈련 | 에폭을 반복하며 모델 파라미터 업데이트 | `model()`, `loss.backward()`, `optimizer.step()` |
| 8 | 학습 곡선 시각화 | 에폭에 따른 손실 변화를 그래프로 표시 | `matplotlib.pyplot.plot()` |
| 9 | 모델 평가 | 테스트 데이터로 모델 성능 평가 | `model.eval()`, `torch.no_grad()` |
| 10 | 결과 시각화 | 실제 값과 예측 값을 scatter plot으로 비교 | `matplotlib.pyplot.scatter()`, `pyplot.plot()` |
| 11 | 모델 저장 | 훈련된 모델을 파일로 저장 | `torch.save()` |

## 상관관계 분석
### 정의
두 변수 간의 선형적 관계의 강도와 방향을 측정하는 통계적 방법. 이는 한 변수의 변화가 다른 변수의 변화와 어떻게 연관되어 있는지를 파악하는 데 사용됨.

### 파악할 수 있는 것
   - 변수 간 관계의 강도: 얼마나 강하게 연관되어 있는지
   - 관계의 방향: 양의 관계인지 음의 관계인지
   - 선형성: 관계가 선형적인지 여부
   - 이상치 탐지: 전반적인 패턴에서 벗어난 데이터 포인트 식별
   - 다중 변수 간 관계: 여러 변수 간의 상호작용 파악

### 수식
피어슨 상관계수(Pearson correlation coefficient)로 표현됨.
$$r = \frac{\sum{(x_i-\bar{x})(y_i-\bar{y})}}{\sqrt{\sum{(x_i-\bar{x})^2}\sum{(y_i-\bar{y})^2}}}$$
(이때, $-1\leq r \leq 1$)
- $r = 1$: 완벽한 양의 선형 관계
- $r = -1$: 완벽한 음의 선형 관계
- $r = 0$: 선형 관계 없음

- $|r| > 0.7$: 강한 상관관계
- $0.3 < |r| ≤ 0.7$: 중간 정도의 상관관계
- $|r| ≤ 0.3$: 약한 상관관계

## 코드 표현
`np.corrcoef(x, t)`

## 선형 회귀 모델 학습
### 정의
직선 $y=wx+b$의 기울기(가중치) $w$와 $y$절편(bias) $b$를 찾는 과정.

### 신경망 관점에서 선형 회귀 모델을 살펴보면 매우 간단한 형태의 신경망으로 이해할 수 있음.

#### 구조
- 선형 회귀는 단일 뉴런(노드)를 가진 가장 간단한 형태의 신경망으로 볼 수 있음
- 입력 레이어: 특성(feature)들이 입력으로 들어옴
- 출력 레이어: 하나의 뉴런으로 구성되며, 이 뉴런은 입력의 가중합(weighted sum)을 계산함

#### 수학적 표현
- $y = w1x1 + w2x2 + ... + wnxn + b$
- 여기서 $w$는 가중치(weight), $x$는 입력 특성, $b$는 편향(bias)

#### 활성화 함수
- 선형 회귀에서는 활성화 함수를 사용하지 않거나, 항등 함수(identity function)를 사용
- 이는 $f(x) = x$ 형태로, 입력을 그대로 출력으로 전달함

#### 손실 함수
- 주로 평균 제곱 오차(Mean Squared Error, MSE)를 사용함
- $$MSE = \frac{Σ(y_{true} - y_{pred})^2}{n}$$

#### 최적화
- 경사 하강법(Gradient Descent)을 사용하여 가중치와 편향을 업데이트함
- 이는 더 복잡한 신경망에서 사용하는 역전파(backpropagation) 알고리즘의 가장 단순한 형태

#### 신경망과의 관계
   - 선형 회귀는 단일 레이어 퍼셉트론의 특수한 경우로 볼 수 있음
   - 비선형 활성화 함수를 추가하고 여러 레이어를 쌓으면 더 복잡한 신경망으로 확장될 수 있음

이제 코드를 작성하러 가볼까
## torch.nn.Module
PyTorch에서 신경망 모델을 만들 때 기본이 되는 클래스. 이 클래스를 상속받아 우리만의 모델을 정의할 수 있음.

### 모델 구조 정의
#### `__init__` 
클래스의 생성자. 객체가 생성될 때 호출되며, 모델의 구조와 레이어를 정의하는 데 사용함
- 모델의 레이어와 구성요소를 초기화
- `super().__init__()`을 호출하여 nn.Module의 초기화를 수행
- 모델의 파라미터를 정의
#### `forward`
입력 데이터가 모델을 통과하는 방식, 즉 순전파(forward propagation)를 정의함
- 입력 데이터를 받아 모델의 출력을 계산
- 각 레이어와 활성화 함수를 순서대로 적용
- PyTorch가 자동으로 호출하므로, 직접 호출할 필요가 없음

### 자동 파라미터 관리
- nn.Module은 모델의 학습 가능한 파라미터를 자동으로 추적
- parameters() 메서드를 통해 모든 파라미터에 쉽게 접근할 수 있음

### GPU 지원
- to() 메서드를 사용하여 모델을 쉽게 GPU로 이동할 수 있음

### 저장 및 불러오기
- state_dict() 메서드로 모델의 상태를 쉽게 저장하고 불러올 수 있음

### 평가 모드
- train()과 eval() 메서드로 학습 모드와 평가 모드를 전환할 수 있음

### 선형 회귀 모델을 nn.Module을 상속받아 구현하는 예

```python
import torch
import torch.nn as nn

class LinearRegression(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)
    
    def forward(self, x):
        return self.linear(x)

# 모델 생성
input_dim = 1
output_dim = 1
model = LinearRegression(input_dim, output_dim)

# 모델 파라미터 확인
print(list(model.parameters()))

# 입력 데이터로 예측
x = torch.tensor([[1.0], [2.0], [3.0]])
y_pred = model(x)
print(y_pred)
```
#### `self.linear`
PyTorch의 `nn.Linear` 모듈의 선형 변환(또는 완전 연결 계층)을 수행하는 레이어를 나타내는 인스턴스. `forward` 메서드에서

	`return self.linear(x)`

이 줄은 입력 `x`에 선형 변환을 적용함. `self.linear`는 `__init__`에서 정의된 선형 레이어를 참조

#### `nn.Linear`
- `nn.Linear(input_dim, output_dim)`은 입력 차원(`input_dim`)을 출력 차원(`output_dim`)으로 변환하는 선형 변환을 수행
- 내부적으로 가중치 행렬 `W`와 편향 벡터 `b`를 가지고 있어, `y = Wx + b` 형태의 연산을 수행함
- 이 가중치와 편향은 학습 가능한 파라미터로, 모델 학습 과정에서 최적화됨.

선형 회귀의 경우, 보통 `input_dim`과 `output_dim`이 모두 1이 되어 `y = wx + b` 형태의 단순 선형 방정식을 표현하게 됨

## 오차
모델의 예측값과 실제값의 차이. 선형회귀에서 각 데이터 포인트 $i$에 대한 오차 $e_i$는 다음과 같이 정의됨:
$$e_i = y_i - ŷ_i$$
($y_i$는 실제값, $ŷ_i$는 모델의 예측값)

## 손실 함수
손실 함수는 모델의 성능을 단일 숫자로 요약한다. 선형회귀에서 주로 사용되는 손실 함수는 평균 제곱 오차(Mean Squared Error, MSE)다.
$$MSE = \frac{Σ(y_i - ŷ_i)^2}{n}$$($n$은 데이터 포인트의 수)
- 가장 널리 사용되는 손실 함수다.
- 미분이 쉽고 수학적으로 다루기 편해 최적화에 유리하다.
- 큰 오차에 더 큰 페널티를 부여하므로, 이상치에 민감하다.
- 회귀 문제에서 기본적으로 사용되며, 특히 정규 분포를 따르는 오차를 가정할 때 적합하다.

PyTorch에서 MSE 손실 함수는 다음과 같이 사용할 수 있다:

```python
criterion = nn.MSELoss()
loss = criterion(y_pred, y_true)
```

MSE 외에 다른 손실 함수들

1. 평균 절대 오차(Mean Absolute Error, MAE):
	   $$MAE = \frac{Σ|y_i - ŷ_i|}{n}$$
- 이상치에 덜 민감한 평가가 필요할 때:
	- MAE는 MSE(평균 제곱 오차)에 비해 이상치의 영향을 덜 받는다.
	- 이는 오차를 제곱하지 않고 절대값을 사용하기 때문이다.
- 오차의 크기를 직관적으로 해석해야 할 때:
	- MAE는 원본 데이터와 같은 단위를 가져 해석이 쉽다.
	- 예: 주택 가격 예측에서 MAE가 $10,000이면, 평균적으로 예측이 실제 가격과 $10,000 차이난다고 해석할 수 있다.
- 중앙값(median) 예측이 중요할 때:
	- MAE를 최소화하는 것은 조건부 중앙값을 추정하는 것과 동일하다.
	- 이는 MSE가 조건부 평균을 추정하는 것과 대비된다.
- 금융 분야에서의 예측:
	- 금융 시계열 예측에서 MAE는 자주 사용된다.
	- 특히 주가 예측이나 리스크 관리에서 유용하다.
- 시계열 예측 평가:
	- 시계열 모델의 성능을 평가할 때 MAE는 자주 사용되는 지표 중 하나다.
- 로버스트 회귀(Robust Regression):
	- MAE를 손실 함수로 사용하는 회귀는 이상치에 덜 민감한 로버스트 모델을 만든다.
- 비대칭적 비용 함수가 필요없을 때:
	- MAE는 양의 오차와 음의 오차를 동등하게 취급한다.

2. 평균 절대 비율 오차(Mean Absolute Percentage Error, MAPE):
	$$MAPE =\frac{ Σ|(y_i - ŷ_i) / y_i|}{n}* 100\%$$
	- 다른 규모의 데이터셋 간 예측 정확도를 비교할 때 유용하다.
	- 실제값이 0에 가까울 때 문제가 발생할 수 있어, 이런 경우를 피해야 한다.
	- 주로 수요 예측, 판매량 예측 등 비즈니스 및 경제 분야에서 자주 사용된다.

4. Huber Loss: MSE와 MAE의 특성을 결합한 손실 함수다. 이상치에 덜 민감하다.
	- MSE와 평균 절대 오차(MAE)의 특성을 결합한 손실 함수다.
	- 이상치에 덜 민감하면서도 MSE의 장점을 유지한다.
	- 델타(δ) 파라미터를 통해 MSE와 MAE 사이의 균형을 조절할 수 있다.
	- 이상치가 있는 회귀 문제에서 유용하다. 예를 들어, 센서 데이터 분석이나 금융 데이터 예측 등에서 사용된다.

손실 함수의 선택은 문제의 특성과 데이터의 분포에 따라 달라진다. MSE는 이상치에 민감하지만, 미분이 쉽고 수학적으로 다루기 편해 가장 널리 사용된다.

모델 학습 시 이 손실 함수를 최소화하는 방향으로 파라미터를 조정한다. 이 과정에서 경사 하강법(Gradient Descent)같은 최적화 알고리즘이 사용된다.

## 경사하강법
### 이론
함수의 최솟값을 찾는 최적화 알고리즘이다. 머신러닝에서는 주로 손실 함수를 최소화하는 데 사용된다. 

수학적으로, 우리는 함수 $J(θ)$의 최솟값을 찾고자 한다.(여기서 $θ=$모델의 파라미터) 경사하강법의 핵심 아이디어는 함수의 기울기(gradient)를 사용해 파라미터를 반복적으로 업데이트하는 것이다.
$$θ(new) = θ(old) - α * ∇J(θ)$$
($α$는 학습률)

#### Taylor 전개를 이용한 이론적 근거
경사하강법의 이론적 근거는 테일러 전개에서 찾을 수 있다. 다변수 함수 $J(θ)$의 1차 테일러 전개는 다음과 같다:
$$J(θ + Δθ) ≈ J(θ) + ∇J(θ)^T * Δθ + O(||Δθ||^2)$$
여기서 $O(||Δθ||^2)$는 2차 이상의 고차항을 나타낸다. $Δθ$가 충분히 작다면 이 항은 무시할 수 있다.

우리의 목표는 $J(θ + Δθ) < J(θ)$가 되는 $Δθ$를 찾는 것이다. 이를 위해 $Δθ = -α∇J(θ)$로 선택하면:
$$J(θ - α∇J(θ)) ≈ J(θ) - α||∇J(θ)||^2$$
이는 $α > 0$일 때 $J(θ)$보다 작은 값을 갖는다. 이것이 경사하강법의 수학적 기초다.
   
### 작동 원리
경사하강법의 작동 과정을 시간 순서대로 표로 나타내면 다음과 같다:

| 단계  | 설명                                          |
| --- | ------------------------------------------- |
| 1   | 파라미터 $θ$를 초기화한다.                            |
| 2   | 현재 파라미터에서의 손실 함수 값 $J(θ)$를 계산한다.            |
| 3   | 손실 함수의 그래디언트 $∇J(θ)$를 계산한다.                 |
| 4   | 파라미터를 업데이트한다: $θ(new) = θ(old) - α * ∇J(θ)$ |
| 5   | 새로운 손실 함수 값을 계산한다.                          |
| 6   | 종료 조건을 만족할 때까지 2-5 단계를 반복한다.                |

### 학습률 $α$
학습률은 각 반복에서 파라미터를 얼마나 크게 업데이트할지 결정하는 하이퍼파라미터다. 

큰 학습률
   - 장점: 빠른 학습 가능
   - 단점: 최솟값을 지나치거나 발산할 위험

작은 학습률
   - 장점: 안정적인 수렴
   - 단점: 학습 속도가 느리고 지역 최솟값에 갇힐 수 있음

최적의 학습률 선택은 어렵고, 보통 실험적으로 결정한다. 학습률 스케줄링(learning rate scheduling)을 통해 학습 과정에서 학습률을 동적으로 조정할 수도 있다.

convex function의 경우, 적절한 학습률을 선택하면 전역 최솟값으로의 수렴이 보장된다.

### 경사하강법에서 나타난 문제점

#### 대규모 데이터셋에서의 계산 비용 문제
- 전체 데이터셋에 대해 그래디언트를 계산하는 것은 계산 비용이 매우 크다.
- 시간 복잡도: $O(n)$, 여기서 $n$은 데이터 포인트의 수다.
- 메모리 요구사항도 데이터셋 크기에 비례하여 증가한다.

#### 로컬 미니마 문제
- non-convex function의 경우, 경사하강법이 지역 최솟값에 갇힐 수 있다.
- 이는 특히 딥러닝에서 중요한 문제다. 왜냐하면 신경망의 손실 함수는 일반적으로 매우 복잡한 비볼록 함수이기 때문이다.
- 수학적으로, 로컬 미니마는 $∇J(θ) = 0$이면서 헤시안 행렬이 양의 준정부호(positive semidefinite)인 지점이다.

## 확률적 경사하강법(SGD)
위의 두 문제를 동시에 해결하기 위해 도입되었다.

계산 비용 감소
- 매 반복마다 무작위로 선택된 일부 데이터(미니배치)만을 사용한다.
- 시간 복잡도: $O(m)$, 여기서 $m$은 미니배치 크기로 보통 $m << n$이다.
- 이는 대규모 데이터셋에서 훨씬 효율적이다.

로컬 미니마 회피
- SGD의 확률적 특성으로 인해 노이즈가 발생한다.
- 이 노이즈는 때때로 로컬 미니마에서 벗어나는 데 도움을 준다.
- 수학적으로, SGD의 업데이트 규칙은 다음과 같다  $$θ(t+1) = θ(t) - α∇J_i(θ(t))$$여기서 $∇J_i$는 $i$번째 샘플(또는 미니배치)에 대한 그래디언트다.

SGD의 수렴성
- 적절한 조건 하에서, SGD는 전체 데이터셋을 사용하는 경사하강법과 동일한 최솟값으로 수렴한다는 것이 증명되어 있다.
- 이는 확률적 근사(stochastic approximation) 이론에 기반한다.

SGD는 이러한 특성으로 인해 대규모 머신러닝, 특히 딥러닝에서 표준적인 최적화 알고리즘이 되었다. 다만, 학습률 선택이 더욱 중요해지고, 수렴 경로가 불안정할 수 있다는 단점도 있다.

### 이론
SGD의 이론적 근거는 확률적 근사 이론에 기반한다. 핵심 아이디어는 전체 데이터셋의 기댓값을 추정하는 것이다.

손실 함수를 다음과 같이 표현할 수 있다
$$J(θ) = E[J_i(θ)] ≈ (1/n) * Σ J_i(θ)$$
여기서 $J_i(θ)$는 $i$번째 샘플의 손실이다.

SGD는 이 기댓값을 단일 샘플(또는 미니배치)로 근사한다:
$$∇J(θ) ≈ ∇J_i(θ)$$
### 경사하강법과 SGD의 차이점

경사하강법
$$θ(t+1) = θ(t) - α * ∇J(θ(t))$$

확률적 경사하강법
$$θ(t+1) = θ(t) - α * ∇J_i(θ(t))$$

($i$는 각 반복에서 무작위로 선택된 샘플(또는 미니배치)의 인덱스)

| 특성 | 경사하강법 | 확률적 경사하강법 |
|------|------------|-------------------|
| 데이터 사용 | 전체 데이터셋 | 샘플 또는 미니배치 |
| 계산 비용 | O(n) | O(1) 또는 O(m), m은 미니배치 크기 |
| 메모리 요구사항 | 높음 | 낮음 |
| 수렴 경로 | 안정적 | 불안정, 노이즈 있음 |
| 로컬 미니마 회피 | 어려움 | 상대적으로 용이 |
| 병렬화 | 쉬움 | 어려움 (순차적 특성 때문) |
| 수렴 속도 | 느림 | 초기에 빠름, 후기에 느림 |

### PyTorch에서의 사용 방법

`torch.optim` 모듈을 통해 다양한 최적화 알고리즘을 제공한다.

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
```

## Epoch
### 정의
전체 데이터셋에 대해 한 번의 완전한 학습 주기를 의미한다. 즉, 훈련 데이터셋의 모든 샘플이 한 번씩 모델의 학습에 사용되었을 때 1 epoch가 완료된 것이다.

### 주요 포인트
- 데이터 사용: 1 epoch 동안 모든 훈련 데이터가 한 번씩 사용된다.
- 반복 횟수: epoch 수는 전체 데이터셋을 몇 번 반복해서 학습할지를 나타낸다.
- 배치와의 관계: 한 epoch는 여러 배치(또는 미니배치)로 구성될 수 있다.

### epoch의 중요성
- 과적합 방지: epoch 수 조절로 과적합을 제어할 수 있다.
- 학습 진행 평가: 각 epoch 후 검증 세트로 성능을 평가하여 학습 진행을 모니터링한다.
- 학습률 조정: 일정 epoch마다 학습률을 조정하는 전략을 사용할 수 있다.

## 데이터 표준화
### 정의
학창시절에 지겹도록 한 표준정규분포화시키기. 

### Q : 근데 이게 값만 줄어든다는거 외에 다른 의미가 있음?
A :
- 특성 스케일 통일:
	- 서로 다른 단위나 범위를 가진 특성들을 동일한 스케일로 변환한다.
	- 이는 모든 특성이 학습 과정에서 동등하게 중요하게 취급되도록 한다.
- 학습 안정성 향상:
	- 그래디언트 계산 시 수치적 안정성을 제공한다.
	- 특히 신경망에서 활성화 함수의 포화를 방지할 수 있다.
- 최적화 속도 개선:
	- 표준화된 데이터는 보통 더 빠른 수렴을 가능하게 한다.
	- 특성 간 스케일 차이로 인한 최적화 경로의 왜곡을 줄인다.
- 정규화 효과:
	- L2 정규화와 같은 가중치 감소 기법의 효과를 일관되게 만든다.
- 모델 일반화 향상:
	- 과적합 위험을 줄이고, 모델의 일반화 성능을 향상시킬 수 있다.

### PyTorch에서 데이터 표준화 방법

PyTorch는 여러 방법으로 데이터 표준화를 수행할 수 있음

a) 수동 계산:
```python
mean = torch.mean(data, dim=0)
std = torch.std(data, dim=0)
normalized_data = (data - mean) / std
```

b) torchvision.transforms 사용 (이미지 데이터):
```python
from torchvision import transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])
```

c) sklearn과 함께 사용:
```python
from sklearn.preprocessing import StandardScaler
import numpy as np

scaler = StandardScaler()
normalized_data = scaler.fit_transform(data.numpy())
normalized_data = torch.from_numpy(normalized_data).float()
```

d) 배치 정규화 층 사용:
```python
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(10, 20),
    nn.BatchNorm1d(20),
    nn.ReLU()
)
```

배치 정규화는 신경망 내부에서 각 층의 출력을 정규화하는 방법으로, 학습 중 데이터 분포의 변화에 적응할 수 있다.

이러한 표준화 방법들은 단순히 값의 크기를 줄이는 것 이상으로, 모델의 학습 과정과 성능에 중요한 영향을 미친다. 적절한 표준화는 모델의 수렴 속도를 높이고, 더 좋은 일반화 성능을 얻는 데 도움을 준다.