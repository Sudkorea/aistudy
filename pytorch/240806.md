# Tensor를 다루는 방법들
## Indexing & Slicing
Numpy에서 하는 사용법과 유사함.
- Indexing : Tensor의 특정 위치에서 찾는거
- Slicing : 부분 집합으로 자르는거
index는 python list와 동일함. 0부터 시작하고, -1 등 음수 가능하고, \[a:b]는 마지막 b번째를 포함하지 않고 등등..

## Tensor의 모양을 변경하는 법

### `view()` 
tensor의 모양을 바꿀 때 가장 많이 쓰이는 방법임. 메모리를 새로 할당하지 않고 기존 데이터의 뷰만 바꾸는 거라 효율적이기 때문

#### 사용법
``` python
f = torch.arange(12)
g = f.view(4,3) # == f.view(4, -1)
h = f.view(3,2,2) # == f.view(3,2,-1)
```
-1은 wildcard로, 두개가 -1값을 가질 수 없음을 유의.

이때, Tensor의 메모리가 연속적으로 할당되지 않으면 에러남.
```python
c = torch.tensor([[0,1,2], [3,4,5]])
d = c[:,:2]
```
이런 식으로 slicing한 상황을 가정하자. 이 경우, d는 메모리가 연속적으로 할당되어있지 않음. 

연속성 확인 및 강제시키는 방법:

```python
if not tensor.is_contiguous():
    tensor = tensor.contiguous()
reshaped = tensor.view(new_shape)
```

(메모리 주소가 등차수열꼴일 경우, 연속적이라 판단하여 view가 작동함.)
### `flatten()`
PyTorch에서 텐서의 차원을 줄이는 데 사용됨. 주로 다차원 텐서를 더 낮은 차원으로 평탄화할 때 씀.
#### 기본 사용법

1. 텐서 메서드로 사용:
   ```python
   flattened_tensor = tensor.flatten(start_dim=0, end_dim=-1)
   ```

2. torch 함수로 사용:
   ```python
   flattened_tensor = torch.flatten(tensor, start_dim=0, end_dim=-1)
   ```

#### 매개변수
- start_dim: 평탄화를 시작할 차원 (기본값: 0)
- end_dim: 평탄화를 끝낼 차원 (기본값: -1, 즉 마지막 차원)

#### 주요 사용 패턴

1. 전체 텐서를 1차원으로 평탄화:
   ```python
   x = torch.randn(2, 3, 4)
   y = x.flatten()  # 또는 torch.flatten(x)
   print(y.shape)  # torch.Size([24])
   ```

2. 마지막 n개 차원만 평탄화:
   ```python
   x = torch.randn(2, 3, 4, 5)
   y = x.flatten(2)  # 마지막 2개 차원 평탄화
   print(y.shape)  # torch.Size([2, 3, 20])
   ```

3. 처음 n개 차원만 평탄화:
   ```python
   x = torch.randn(2, 3, 4, 5)
   y = x.flatten(0, 2)  # 처음 3개 차원 평탄화
   print(y.shape)  # torch.Size([24, 5])
   ```

4. 중간 차원들 평탄화:
   ```python
   x = torch.randn(2, 3, 4, 5, 6)
   y = x.flatten(1, 3)  # 1번째부터 3번째 차원까지 평탄화
   print(y.shape)  # torch.Size([2, 60, 6])
   ```

#### 주의사항
- end_dim을 생략하면 자동으로 마지막 차원(-1)까지 평탄화됨.
- 음수 인덱스 사용 가능: -1은 마지막 차원, -2는 뒤에서 두 번째 차원을 의미함.
- flatten은 새로운 뷰를 반환하므로, 원본 텐서의 데이터를 공유함. 메모리 효율적.

#### 활용 팁
1. 신경망 모델에서 Convolution 레이어 출력을 Fully Connected 레이어에 연결할 때 자주 사용:
   ```python
   conv_output = torch.randn(32, 64, 7, 7)  # (배치, 채널, 높이, 너비)
   flattened = conv_output.flatten(1)  # (32, 3136)
   ```

2. 배치 처리 시 배치 차원은 유지하고 나머지만 평탄화할 때:
   ```python
   batch_data = torch.randn(10, 3, 224, 224)  # (배치, 채널, 높이, 너비)
   flattened_batch = batch_data.flatten(1)  # (10, 150528)
   ```

3. PyTorch 1.11.0부터는 flattened() 메서드도 사용 가능. 이는 원본을 변경하지 않는 버전

### `reshape()`
view()와 비슷한데, 메모리가 연속적이지 않아도 사용할 수 있음. 필요하면 메모리를 새로 할당
```python
k = torch.randn(4, 4)
l = k.reshape(2, 8)  # k가 연속적이지 않아도 동작함
```

### `transpose()`
지정한 두 차원을 서로 바꿈. 행렬을 전치할 때 많이 씀.
```python
m = torch.randn(2, 3)
n = m.transpose(0, 1)  # 결과는 shape (3, 2)
```

### `squeeze()`
크기가 1인 차원을 제거하여 불필요한 차원을 없앨 때 유용함.
```python
o = torch.randn(1, 2, 1, 3)
p = o.squeeze()  # 결과는 shape (2, 3)
p = torch.squeeze(o, dim = 0) # 결과는 shape (2, 1, 3)
```

### `unsqueeze()`
지정한 위치에 크기가 1인 새 차원을 추가함. 차원을 늘려야 할 때 씀.
```python
q = torch.randn(3, 4)
r = q.unsqueeze(0)  # 결과는 shape (1, 3, 4)
r = torch.unsqueeze(q, dim=0) # """
```

### `stack()`

여러 개의 텐서를 새로운 차원으로 쌓아서 하나의 텐서로 만드는 함수. 주로 torch.stack()으로 사용함.

#### 기본 사용법
```python
result = torch.stack(tensors, dim=0)
```

여기서 tensors는 쌓을 텐서들의 리스트나 튜플이고, dim은 새로 생성할 차원의 인덱스임

#### 주요 특징

1. 입력 텐서들의 크기가 모두 같아야 함.
2. 새로운 차원이 추가되기 때문에, 결과 텐서의 차원 수는 입력 텐서보다 1 더 많아짐.
3. 기본적으로 dim=0. 즉, 첫 번째 차원으로 쌓음.

#### 예시

1. 기본 사용:
   ```python
   t1 = torch.tensor([1, 2, 3])
   t2 = torch.tensor([4, 5, 6])
   t3 = torch.tensor([7, 8, 9])
   result = torch.stack((t1, t2, t3))
   print(result)
   # tensor([[1, 2, 3],
   #         [4, 5, 6],
   #         [7, 8, 9]])
   ```

2. 차원 지정:
   ```python
   result = torch.stack((t1, t2, t3), dim=1)
   print(result)
   # tensor([[1, 4, 7],
   #         [2, 5, 8],
   #         [3, 6, 9]])
   ```

3. 2D 텐서 쌓기:
   ```python
   t1 = torch.tensor([[1, 2], [3, 4]])
   t2 = torch.tensor([[5, 6], [7, 8]])
   result = torch.stack((t1, t2))
   print(result.shape)  # torch.Size([2, 2, 2])
   ```

#### 주의사항

1. 메모리 사용: stack()은 새로운 메모리를 할당하므로, 큰 텐서를 다룰 때는 메모리 사용에 주의해야 함

2. cat()과의 차이: cat()은 기존 차원을 따라 연결하지만, stack()은 새 차원을 만듦

3. 성능: 많은 수의 작은 텐서를 쌓을 때는 list comprehension과 함께 사용하면 효율적일 수 있음
   ```python
   result = torch.stack([torch.randn(3, 4) for _ in range(100)])
   ```

4. GPU 사용: GPU에서 실행 중이라면, 모든 입력 텐서가 같은 디바이스에 있어야 함
### `permute()`
여러 차원을 한 번에 재배열할 때 씀. transpose()의 일반화된 버전이라고 볼 수 있음.
```python
x = torch.randn(2, 3, 5)
y = x.permute(2, 0, 1)  # 결과는 shape (5, 2, 3)
```
차원의 순서를 원하는 대로 바꿀 수 있음. 인자로 새로운 차원 순서를 지정하면 됨.
주의할 점은 permute 후에는 메모리가 연속적이지 않을 수 있어서, 필요하면 contiguous() 호출해야 함.

### 매서드들 총정리

| Method      | main function  | 메모리 연속성 필요 | 새 메모리 할당 | example                       |
| ----------- | -------------- | ---------- | -------- | ----------------------------- |
| view()      | 텐서 모양 변경       | Y          | N        | `tensor.view(2, -1)`          |
| flatten()   | 지정 차원 평탄화      | N          | N        | `tensor.flatten(start_dim=1)` |
| reshape()   | 텐서 모양 변경       | N          | 조건부      | `tensor.reshape(2, 8)`        |
| transpose() | 두 차원 교환        | N          | N        | `tensor.transpose(0, 1)`      |
| squeeze()   | 크기 1인 차원 제거    | N          | N        | `tensor.squeeze()`            |
| unsqueeze() | 새 차원 추가        | N          | N        | `tensor.unsqueeze(0)`         |
| stack()     | 텐서들을 새 차원으로 쌓기 | N          | Y        | `torch.stack([t1, t2])`       |
| permute()   | 여러 차원 재배열      | N          | N        | `tensor.permute(2, 0, 1)`     |

## Tensor를 연결하는 법
### `cat()`

cat()은 concatenate의 줄임말로, 주어진 차원을 따라 텐서들을 연결함. stack()과 달리 새로운 차원을 만들지 않음

#### 기본 사용법
```python
result = torch.cat(tensors, dim=0)
```

#### 주요 특징

1. 연결하려는 차원을 제외한 다른 모든 차원의 크기가 같아야 함
2. 새로운 차원을 만들지 않고, 기존 차원을 따라 연결함
3. default : dim=0

#### cat() 사용 예시

1. 1D 텐서 연결:
   ```python
   t1 = torch.tensor([1, 2, 3])
   t2 = torch.tensor([4, 5, 6])
   result = torch.cat((t1, t2))
   print(result)  # tensor([1, 2, 3, 4, 5, 6])
   ```

2. 2D 텐서 연결 (dim=0):
   ```python
   t1 = torch.tensor([[1, 2], [3, 4]])
   t2 = torch.tensor([[5, 6], [7, 8]])
   result = torch.cat((t1, t2), dim=0)
   print(result)
   # tensor([[1, 2],
   #         [3, 4],
   #         [5, 6],
   #         [7, 8]])
   ```

3. 2D 텐서 연결 (dim=1):
   ```python
   result = torch.cat((t1, t2), dim=1)
   print(result)
   # tensor([[1, 2, 5, 6],
   #         [3, 4, 7, 8]])
   ```

#### `cat()`과 `reshape()` 함께 사용하기

1. 다른 차원의 텐서 연결하기
```python
import torch

a = torch.tensor([[1, 2], [3, 4]])  # shape: (2, 2)
b = torch.tensor([5, 6, 7, 8])      # shape: (4,)

# b를 (2, 2) 형태로 reshape
b_reshaped = b.reshape(2, 2)

# a와 b_reshaped를 연결
result = torch.cat((a, b_reshaped), dim=0)

print(result)
# tensor([[1, 2],
#         [3, 4],
#         [5, 6],
#         [7, 8]])
```

2. 3차원 텐서 다루기
```python
c = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])  # shape: (2, 2, 2)
d = torch.tensor([9, 10, 11, 12])                       # shape: (4,)

# d를 (1, 2, 2) 형태로 reshape
d_reshaped = d.reshape(1, 2, 2)

# c와 d_reshaped를 연결
result = torch.cat((c, d_reshaped), dim=0)

print(result.shape)  # torch.Size([3, 2, 2])
print(result)
# tensor([[[ 1,  2],
#          [ 3,  4]],
#         [[ 5,  6],
#          [ 7,  8]],
#         [[ 9, 10],
#          [11, 12]]])
```

3. 여러 차원에서 연결하기
```python
e = torch.tensor([[1, 2], [3, 4]])  # shape: (2, 2)
f = torch.tensor([5, 6])            # shape: (2,)

# f를 (2, 1) 형태로 reshape
f_reshaped = f.reshape(2, 1)

# e와 f_reshaped를 dim=1에서 연결
result = torch.cat((e, f_reshaped), dim=1)

print(result)
# tensor([[1, 2, 5],
#         [3, 4, 6]])
```

주의사항:
- reshape() 사용 시 원하는 shape의 원소 개수가 원본 텐서의 원소 개수와 일치해야 함.
- cat()으로 연결할 때는 연결하려는 차원을 제외한 다른 차원들의 크기가 일치해야 함.
- -1을 사용해 자동으로 크기를 계산하게 할 수 있음. 예: `tensor.reshape(-1, 2)`
#### `stack()`과 `cat()`의 비교

1. 차원 변화:
   - stack(): 새로운 차원 추가
   - cat(): 기존 차원을 따라 연결, 차원 수 유지

2. 입력 텐서 크기:
   - stack(): 모든 입력 텐서의 크기가 같아야 함
   - cat(): 연결하는 차원을 제외한 다른 차원의 크기만 같으면 됨

3. 사용 사례:
   - stack(): 배치 만들기, 시계열 데이터 처리
   - cat(): 특징 벡터 합치기, 모델 출력 연결하기
### `expand()`
텐서의 크기를 확장할 때 씀. 새로운 메모리를 할당하지 않고, 기존 데이터를 재사용해서 크기를 늘림.
```python
x = torch.tensor([[1], [2], [3]])
y = x.expand(-1, 4)  # 결과는 shape (3, 4)
print(y)
# tensor([[1, 1, 1, 1],
#         [2, 2, 2, 2],
#         [3, 3, 3, 3]])
```
-1을 사용하면 해당 차원의 크기를 그대로 유지함. 

1. 브로드캐스팅(broadcasting)^[[[pytorch/개념정리/브로드캐스팅]]]과 관련이 깊음. PyTorch의 자동 브로드캐스팅 기능을 이해하면 expand()를 더 잘 활용할 수 있음.
2. view()나 reshape()와 함께 자주 사용됨. 데이터 구조를 변경할 때 유용
3. 음수 인덱스 사용 가능: x.expand(-1, -1, 3)처럼 쓰면 마지막 차원만 3으로 확장하고 나머지는 그대로 유지

#### 주의사항:
- expand() 후 결과를 수정하면 원본 데이터도 변경될 수 있음. 이걸 피하려면 .clone()을 같이 써야 함.
- expand는 새 메모리를 할당하지 않아서 메모리 효율적이지만, 확장된 차원에 대해 연산을 수행하면 예상치 못한 결과가 나올 수 있어, 주의해서 사용해야 함.
### `repeat()`
텐서를 반복해서 새로운 텐서를 만듦. expand()와 달리 실제로 데이터를 복제함.

```python
x = torch.tensor([1, 2, 3])
y = x.repeat(2, 3)  # 결과는 shape (2, 9)
print(y)
# tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],
#         [1, 2, 3, 1, 2, 3, 1, 2, 3]])
```

1. 차원 확장: repeat()는 필요하면 자동으로 새로운 차원을 추가함
   ```python
   z = x.repeat(4, 1)  # 결과는 shape (4, 3)
   print(z)
   # tensor([[1, 2, 3],
   #         [1, 2, 3],
   #         [1, 2, 3],
   #         [1, 2, 3]])
   ```

2. 부분 반복: 일부 차원만 반복할 수도 있음
   ```python
   a = torch.tensor([[1, 2], [3, 4]])
   b = a.repeat(1, 2)  # 결과는 shape (2, 4)
   print(b)
   # tensor([[1, 2, 1, 2],
   #         [3, 4, 3, 4]])
   ```

3. 메모리 사용: repeat()는 새로운 메모리를 할당하므로, 대용량 데이터를 다룰 때는 주의가 필요함
4. 복잡한 패턴: 여러 차원에 대해 다양한 반복 패턴을 만들 수 있음
   ```python
   c = torch.tensor([1, 2])
   d = c.repeat(2, 1, 3)  # 결과는 shape (2, 1, 6)
   print(d)
   # tensor([[[1, 2, 1, 2, 1, 2]],
   #         [[1, 2, 1, 2, 1, 2]]])
   ```

5. 원본 유지: repeat()는 원본 텐서를 변경하지 않고 새로운 텐서를 반환함

주의사항:
- repeat() 인자의 개수가 원본 텐서의 차원보다 많으면, 새로운 차원이 앞쪽에 추가됨
- 메모리 사용량이 급증할 수 있으므로, 큰 텐서를 여러 번 반복할 때는 주의가 필요
# 텐서 연산 메서드
## 산술 연산
### `add(), sub(), mul(), div()`

```python
a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])

print(a.add(b))      # tensor([5, 7, 9])
print(a.sub(b))      # tensor([-3, -3, -3])
print(a.mul(b))      # tensor([4, 10, 18])
print(a.div(b))      # tensor([0.2500, 0.4000, 0.5000])
```
- 브로드캐스팅 규칙을 따름. 크기가 다른 텐서 간 연산 시 주의 필요함.
- 나눗셈 시 분모가 0인 경우 inf 또는 nan 발생 가능함.
- 인플레이스 연산(add_(), sub_() 등)을 사용하여 메모리 효율 개선 가능함.
- torch.add(a, b)와 같은 함수형 API도 제공됨.

### `pow()`

```python
x = torch.tensor([1, 2, 3])
print(x.pow(2))  # tensor([1, 4, 9])
print(x.pow(0.5))  # tensor([1.0000, 1.4142, 1.7321])
```
- 음수 잘못 건들면 복소수 결과 발생 가능함
- torch.pow(x, 2)와 같은 함수형 사용도 가능함.
- `x**2`와 같은 파이썬 연산자 사용 가능함.

### `sqrt()`

```python
y = torch.tensor([1, 4, 9])
print(y.sqrt())  # tensor([1.0000, 2.0000, 3.0000])
```
- 음수 입력 시 nan 결과 발생함.
- pow(0.5)와 동일한 결과를 제공하나, sqrt()가 더 효율적임.

### `abs()`

```python
z = torch.tensor([-1, 2, -3])
print(z.abs())  # tensor([1, 2, 3])
```

- 복소수 텐서에 대해서는 크기(magnitude)를 반환함.
- torch.abs()와 동일한 기능을 제공함.
- 그래디언트 계산 시 주의가 필요함. 0에서 매끄럽지 않다는거

## 논리연산자

### `logical_and()`

```python
a = torch.tensor([True, False, True])
b = torch.tensor([True, True, False])
print(torch.logical_and(a, b))  # tensor([ True, False, False])
```

- 입력 텐서는 불리언 또는 정수형이어야 함.
- 0은 False, 0이 아닌 값은 True로 해석됨.
- `&`연산자로도 동일한 결과를 얻을 수 있음 (예: `a & b`).

### `logical_or()`

```python
x = torch.tensor([True, False, True])
y = torch.tensor([False, True, False])
print(torch.logical_or(x, y))  # tensor([ True,  True,  True])
```

- 입력 텐서의 형태가 동일해야 함. 아니면 브로드캐스팅 규칙이 적용됨
- `|` 연산자로도 동일한 결과를 얻을 수 있음 (예: `x | y`)

### `logical_xor()`

```python
p = torch.tensor([True, False, True, False])
q = torch.tensor([True, True, False, False])
print(torch.logical_xor(p, q))  # tensor([False,  True,  True, False])
```

- XOR은 입력값이 서로 다를 때만 True를 반환함.
- PyTorch에서 ^ 연산자는 비트 XOR을 수행하므로 주의가 필요함.
```python
a = torch.tensor([1, 0, 3, 4])
b = torch.tensor([1, 1, 0, 5])

print(torch.logical_xor(a, b))  # tensor([False,  True,  True,  False])
print(a ^ b)  # tensor([0, 1, 3, 1])
```

### `logical_not()`

```python
z = torch.tensor([True, False, True])
print(torch.logical_not(z))  # tensor([False,  True, False])
```

- 스칼라 값에 대해서도 사용 가능함.
- ~ 연산자는 비트 NOT을 수행하므로 logical_not과는 다른 결과를 낼 수 있음.
```python
x = torch.tensor([0, 1, -1, 2])

print(torch.logical_not(x))  # tensor([ True, False, False, False])
print(~x)  # tensor([-1, -2,  0, -3])
```

# 텐서 간 거리
## 거리가 뭐냐?
$\forall x, y, z \in U$를 만족하는 $x, y, z$에 대하여($U$는 아마 거리화가능공간..?)
- $d(x, y) \geq 0$
- $d(x, y)=0 \Leftrightarrow x = y$
- $d(x, y) = d(y, x)$
- $d(x,z)\leq d(x,y)+d(y,z)$
이 $4$가지 조건을 만족하는 함수 $d$를 거리함수라 한다.

## 위에껄 왜 썼냐?
- 거리는 익히 잘 아는거라 복습할 필요가 없어서 안적고 넘어갈거라고 시위한것.

## Norm 구하는방법
### `torch.dist(input, other, p=2)`
   - 두 텐서 간의 p-norm 거리를 계산
   - p=1 (맨해튼 거리), p=2 (유클리드 거리, 기본값)

### `torch.norm(input, p='fro', dim=None)`
   - 텐서의 p-norm을 계산. 두 텐서의 차이에 적용하여 거리를 구할 수 있음

### `F.pairwise_distance(x1, x2, p=2)`
   - 두 텐서 집합 간의 pairwise 거리를 계산

### `torch.cdist(x1, x2, p=2)`
   - 두 텐서 집합 간의 pairwise 거리 행렬을 계산

### `(x1 - x2).norm(p=2)`
   - 두 텐서의 차이에 대해 직접 norm을 계산

```python
import torch

a = torch.randn(3, 4)
b = torch.randn(3, 4)

# 유클리드 거리
distance = torch.dist(a, b)

# 맨해튼 거리
manhattan_distance = torch.dist(a, b, p=1)

# 프로베니우스 norm
frobenius_norm = torch.norm(a - b)
```

### `torch.linalg.norm()`
PyTorch의 선형 대수 모듈에 포함된 함수로, 벡터, 행렬, 또는 텐서의 norm을 계산하는 데 사용됨. 이 함수는 `torch.norm`보다 더 다양한 옵션과 정밀한 계산을 제공함.

#### 특징
1. 다양한 norm 타입 지원: Frobenius norm, nuclear norm, 다양한 벡터 및 행렬 norms
2. 여러 차원에 대한 계산 가능
3. 행렬 norm 계산 시 더 안정적이고 정확한 알고리즘 사용

#### 예시
```python
import torch

# 벡터 norm
v = torch.tensor([3.0, 4.0])
vector_norm = torch.linalg.norm(v)  # 기본값은 2-norm (유클리드 norm)

# 행렬 norm
m = torch.randn(3, 3)
matrix_norm = torch.linalg.norm(m)  # 기본값은 Frobenius norm

# 특정 차원에 대한 norm
tensor = torch.randn(2, 3, 4)
dim_norm = torch.linalg.norm(tensor, dim=(1, 2))
```

`ord` 매개변수를 사용하여 다양한 종류의 norm을 계산할 수 있음.

```python
# 1-norm (맨해튼 norm)
norm_1 = torch.linalg.norm(v, ord=1)

# 무한대 norm (최대 절대값)
norm_inf = torch.linalg.norm(v, ord=float('inf'))

# 행렬의 스펙트럴 norm (최대 특이값)
spectral_norm = torch.linalg.norm(m, ord=2)
```

`torch.linalg.norm`은 특히 행렬 연산이나 고차원 텐서를 다룰 때 유용하며, 수치적으로 더 안정적인 계산을 제공한다고 함.

## 유사도

맨하튼 유사도, 유클리드 유사도에서 주의할 점
$$\frac{1}{d(x,y)+1}$$
이렇게 분모에 +1 해줘야함. 안하면 $\infty$떠서 그런듯?


### 맨하튼 유사도 (L1 거리)

```python
import torch
import torch.nn.functional as F

x1 = torch.tensor([[1, 2, 3], [4, 5, 6]])
x2 = torch.tensor([[1, 1, 1], [4, 4, 4]])
manhattan_distance = F.pairwise_distance(x1, x2, p=1)
```
- 음수 값을 포함한 데이터에 대해서도 사용 가능
- 스케일에 민감하므로 필요시 정규화 필요

주요 사용 사례:
- 고차원 데이터의 유사도 측정
- 이상치에 덜 민감한 거리 측정이 필요할 때
- 도시 블록 거리 계산 등 실제 물리적 거리를 다룰 때

### 유클리드 유사도 (L2 거리)

```python
euclidean_distance = F.pairwise_distance(x1, x2, p=2)
```
- 기본값으로 p=2를 사용
- 스케일에 민감하므로 필요시 정규화 필요

주요 사용 사례:
- 일반적인 거리 측정
- 이미지 처리, 컴퓨터 비전
- 클러스터링 알고리즘 (예: K-means)

### 코사인 유사도
```python
cosine_sim = F.cosine_similarity(x1, x2, dim=1)
```
- 벡터의 방향만 고려하고 크기는 무시
- 결과값은 -1에서 1 사이
- 0으로만 구성된 벡터에 대해 계산 시 주의 필요 (분모가 0이 될 수 있음)

주요 사용 사례:
- 텍스트 분석, 문서 유사도 측정
- 추천 시스템
- 고차원 데이터의 유사도 측정 (특히 방향이 중요할 때)

# 텐서의 행렬곱
## `torch.matmul()`
2차원 행렬뿐만 아니라 더 높은 차원의 텐서에 대해서도 사용할 수 있음

```python
import torch

A = torch.randn(3, 4)
B = torch.randn(4, 2)

C = torch.matmul(A, B)
# 또는
C = A @ B  # Python 3.5 이상에서 지원되는 행렬곱 연산자
```

## `torch.mm()`
오직 2차원 행렬에 대해서만 작동함. 높은 차원의 텐서에는 사용할 수 없지만, 2차원 행렬에 대해서는 `torch.matmul()`보다 약간 더 빠를 수 있어서 장단점이 있음.

```python
C = torch.mm(A, B)
```

## `torch.bmm()`
배치 행렬곱(batch matrix multiplication)을 수행함. 3차원 텐서에 대해 작동하며, 첫 번째 차원을 배치 차원으로 취급함

```python
A = torch.randn(32, 3, 4)  # 32개의 3x4 행렬
B = torch.randn(32, 4, 2)  # 32개의 4x2 행렬
C = torch.bmm(A, B)  # 결과: 32개의 3x2 행렬
```

## `torch.einsum()`
아인슈타인 합규약(Einstein summation convention)을 사용하여 텐서 연산을 수행함. 행렬곱 외에도 다양한 텐서 연산을 표현할 수 있음.

```python
C = torch.einsum('ij,jk->ik', A, B)
```