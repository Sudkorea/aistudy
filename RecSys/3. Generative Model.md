# 생성 모델 개요

## Generative Models

생성 모델은 데이터의 분포를 학습하고 새로운 샘플을 생성하는 모델임. 주요 특징은 다음과 같음:

* 데이터: (x,y) 또는 (x) 형태로 주어짐. x는 입력 데이터, y는 레이블을 의미함
* 목표: 관측된 샘플들로부터 데이터의 분포를 표현하는 모델을 학습하는 것임
* 밀도 추정: 데이터의 확률 밀도 함수를 추정함
* 샘플 생성: 학습된 분포로부터 새로운 데이터를 생성할 수 있음
* 역설: "생성할 수 있는 것을 반드시 이해하는 것은 아님". 모델이 데이터를 생성할 수 있다고 해서 그 데이터의 의미나 구조를 완전히 이해하는 것은 아님

생성 모델은 비지도 학습의 한 형태로, 데이터의 숨겨진 구조를 학습하고 이를 바탕으로 새로운 데이터를 생성할 수 있음. 이는 데이터 압축, 특징 추출, 결측치 처리 등 다양한 응용 분야에서 활용됨.

# 그래피컬 모델 개요

그래피컬 모델은 확률 변수들 간의 의존성을 그래프 구조로 표현한 모델임. 주요 특징은 다음과 같음:

1. 구조적 표현: 노드는 확률 변수를, 엣지는 변수 간 의존성을 나타냄
2. 확률적 추론: 관측된 변수로부터 숨겨진 변수의 확률을 추론할 수 있음
3. 모듈성: 복잡한 시스템을 더 작은 부분으로 분해하여 표현 가능함
4. 효율적 계산: 조건부 독립성을 이용해 복잡한 결합 확률 분포를 효율적으로 계산할 수 있음

그래피컬 모델의 종류로는 베이지안 네트워크, 마르코프 랜덤 필드, 은닉 마르코프 모델 등이 있음. 이들은 자연어 처리, 컴퓨터 비전, 생물정보학 등 다양한 분야에서 활용됨.

# VAE (Variational Autoencoder)

## VAE 개요

VAE는 오토인코더의 구조를 기반으로 한 생성 모델임. 주요 특징은 다음과 같음:

1. 인코더-디코더 구조: 인코더는 입력 데이터를 잠재 공간으로 매핑하고, 디코더는 잠재 변수로부터 원본 데이터를 재구성함
2. 확률적 인코딩: 입력을 특정 잠재 변수 값으로 매핑하는 대신, 잠재 변수의 확률 분포를 학습함
3. 잠재 공간의 정규화: 잠재 변수가 표준 정규 분포를 따르도록 제약을 둠
4. 생성 능력: 학습 후에는 잠재 공간에서 샘플링하여 새로운 데이터를 생성할 수 있음

VAE는 데이터의 복잡한 분포를 학습하면서도 연속적인 잠재 공간을 가진다는 장점이 있음. 복잡한 확률 분포를 학습하고 표현할 수 있는 강력한 도구로, 머신러닝과 인공지능의 다양한 영역에서 중요한 역할을 하고 있음. 
## VAE 목적식

VAE의 목적식은 증거 하한(ELBO, Evidence Lower BOund)을 최대화하는 것임. 수학적으로 다음과 같이 표현됨:

$$ \mathcal{L}(\theta, \phi; x) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p(z)) $$

여기서:
- $\theta$와 $\phi$는 각각 디코더와 인코더의 파라미터
- $q_\phi(z|x)$는 인코더가 학습한 근사 사후 분포
- $p_\theta(x|z)$는 디코더가 학습한 생성 모델
- $p(z)$는 잠재 변수의 사전 분포 (보통 표준 정규 분포)
- $D_{KL}$은 쿨백-라이블러 발산

목적식의 첫 번째 항은 재구성 오차를 최소화하고, 두 번째 항은 잠재 공간을 정규화하는 역할을 함.

## VAE 활용 방안

VAE는 다양한 분야에서 활용될 수 있음:

1. 데이터 생성: 새로운 이미지, 텍스트, 음악 등을 생성할 수 있음
2. 특징 추출: 잠재 공간이 데이터의 중요한 특징을 포착할 수 있어, 차원 축소나 특징 추출에 활용됨
3. 이상 탐지: 정상 데이터로 학습된 VAE는 비정상 데이터에 대해 높은 재구성 오차를 보이므로, 이상 탐지에 사용될 수 있음
4. 데이터 압축: 효율적인 잠재 표현을 학습하므로 데이터 압축에 활용 가능함
5. 조건부 생성: 레이블이나 속성을 조건으로 주어 특정 조건을 만족하는 데이터를 생성할 수 있음
6. 결측치 처리: 부분적으로 관측된 데이터의 나머지 부분을 생성하여 결측치를 처리할 수 있음

# 가우시안 혼합 모델 (Gaussian Mixture Model, GMM)

가우시안 혼합 모델은 여러 개의 가우시안 분포를 가중 합으로 조합하여 복잡한 확률 분포를 표현하는 모델임.

## 수학적 정의와 이론

GMM의 확률 밀도 함수는 다음과 같이 정의됨:

$$ p(x) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k) $$

여기서 $\sum_{k=1}^K \pi_k = 1$이며, $\pi_k \geq 0$임.

각 가우시안 성분 $\mathcal{N}(x|\mu_k, \Sigma_k)$는 다음과 같이 정의됨:

$$ \mathcal{N}(x|\mu_k, \Sigma_k) = \frac{1}{(2\pi)^{D/2}|\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)\right) $$

여기서 D는 x의 차원임.

## EM 알고리즘

GMM의 파라미터는 주로 EM(Expectation-Maximization) 알고리즘을 통해 학습됨. EM 알고리즘은 다음 두 단계를 반복함:

1. E-step: 각 데이터 포인트 $x_i$에 대해 각 가우시안 성분에 속할 확률(책임)을 계산함:

   $$ \gamma_{ik} = \frac{\pi_k \mathcal{N}(x_i|\mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(x_i|\mu_j, \Sigma_j)} $$

2. M-step: 계산된 책임을 바탕으로 파라미터를 업데이트함:

   $$ \mu_k^{new} = \frac{\sum_{i=1}^N \gamma_{ik}x_i}{\sum_{i=1}^N \gamma_{ik}} $$
   
   $$ \Sigma_k^{new} = \frac{\sum_{i=1}^N \gamma_{ik}(x_i - \mu_k^{new})(x_i - \mu_k^{new})^T}{\sum_{i=1}^N \gamma_{ik}} $$
   
   $$ \pi_k^{new} = \frac{1}{N}\sum_{i=1}^N \gamma_{ik} $$

여기서 N은 전체 데이터 포인트의 수임.

## 특징

1. 유연성: 복잡한 다봉 분포를 모델링할 수 있음
2. 해석 가능성: 각 가우시안 성분이 데이터의 특정 클러스터나 하위 그룹을 나타낼 수 있음
3. 생성 모델: 학습된 모델로부터 새로운 샘플을 생성할 수 있음
4. EM 알고리즘: 주로 Expectation-Maximization 알고리즘을 통해 모델 파라미터를 학습함

## 생성 모델에서의 활용

1. 밀도 추정: 복잡한 데이터 분포를 근사하는 데 사용됨
2. 클러스터링: 각 가우시안 성분이 하나의 클러스터를 나타낼 수 있음
3. 이상 탐지: 학습된 모델에서 낮은 확률을 갖는 데이터 포인트를 이상치로 간주할 수 있음
4. 데이터 생성: 학습된 모델로부터 새로운 샘플을 생성할 수 있음
5. 차원 축소: 고차원 데이터의 주요 성분을 포착하는 데 사용될 수 있음

# KL Divergence (Kullback-Leibler Divergence)

KL Divergence는 두 확률 분포 간의 차이를 측정하는 방법임.

## 수학적 정의와 성질

연속 확률 변수에 대한 KL Divergence:

$$ D_{KL}(P||Q) = \int p(x) \log\frac{p(x)}{q(x)} dx = \mathbb{E}_{x \sim P}\left[\log\frac{p(x)}{q(x)}\right] $$

이산 확률 변수의 경우:

$$ D_{KL}(P||Q) = \sum_x p(x) \log\frac{p(x)}{q(x)} $$

## 주요 성질

1. 비음수성: Jensen's 부등식에 의해 증명됨
   
   $$ D_{KL}(P||Q) \geq 0 $$

2. 동일성: $P = Q$ 일 때만 $D_{KL}(P||Q) = 0$

3. 비대칭성: 일반적으로 $D_{KL}(P||Q) \neq D_{KL}(Q||P)$

4. 볼록성: $D_{KL}(P||Q)$는 P와 Q에 대해 각각 볼록 함수임

## 정보 이론적 해석

KL Divergence는 상대 엔트로피로도 알려져 있으며, 다음과 같이 해석할 수 있음:

$$ D_{KL}(P||Q) = H(P,Q) - H(P) $$

여기서 $H(P,Q)$는 교차 엔트로피, $H(P)$는 P의 엔트로피임.

## 특징

1. 비대칭성: $D_{KL}(P||Q) \neq D_{KL}(Q||P)$
2. 비음수성: 항상 0 이상의 값을 가짐
3. 동일성: 두 분포가 완전히 같을 때만 0이 됨
4. 정보 이론적 해석: P에 대한 정보를 Q로 인코딩할 때 필요한 추가 비트 수로 해석 가능

## 생성 모델에서의 활용

1. 모델 학습: 많은 생성 모델의 손실 함수에 KL Divergence가 포함됨 (예: VAE)
2. 분포 매칭: 생성된 데이터 분포와 실제 데이터 분포 간의 차이를 최소화하는 데 사용됨
3. 정규화: 모델의 복잡성을 제어하거나 특정 제약을 부과하는 데 활용됨
4. 변분 추론: 복잡한 사후 분포를 근사하는 데 사용됨
5. 모델 평가: 생성된 샘플의 품질을 평가하는 데 활용될 수 있음

# 생성 모델에서의 활용

1. VAE에서의 활용:
   - VAE의 목적 함수에 KL Divergence 항이 포함됨
   - 잠재 변수의 사후 분포를 표준 정규 분포에 가깝게 만드는 역할을 함

2. GAN (Generative Adversarial Network)에서의 활용:
   - 판별자의 학습에 KL Divergence의 변형인 JS Divergence가 사용됨
   - 생성된 데이터 분포와 실제 데이터 분포 간의 차이를 최소화하는 데 활용됨

3. 확률적 생성 모델:
   - GMM은 그 자체로 확률적 생성 모델임
   - 복잡한 데이터 분포를 여러 가우시안의 조합으로 표현할 수 있음

4. 변분 추론:
   - 복잡한 사후 분포를 근사하는 데 KL Divergence가 사용됨
   - 근사 분포와 실제 사후 분포 간의 차이를 최소화하는 방식으로 학습이 진행됨

5. 모델 압축 및 지식 증류:
   - 큰 모델(교사)의 지식을 작은 모델(학생)로 전달할 때 KL Divergence가 사용될 수 있음

6. 이상 탐지:
   - GMM으로 학습된 정상 데이터의 분포에서 크게 벗어난 데이터를 이상치로 탐지할 수 있음

7. 잠재 공간 학습:
   - VAE나 다른 생성 모델에서 의미 있는 잠재 표현을 학습하는 데 KL Divergence가 중요한 역할을 함



