# VI (Variational Inference)

VI는 복잡한 확률 모델의 사후 분포를 근사하는 방법임. 베이지안 추론에서 중요한 역할을 하며, 계산이 어려운 사후 분포를 다루기 쉬운 분포로 근사함.

## 기본 원리

1. 목표: 관측 데이터 X가 주어졌을 때, 숨겨진 변수 Z의 사후 분포 p(Z|X)를 근사하는 것임
2. 방법: 파라미터화된 분포 족 q(Z;θ)에서 실제 사후 분포 p(Z|X)에 가장 가까운 분포를 찾음
3. 최적화: KL Divergence를 최소화하는 파라미터 θ를 찾음

## 수학적 표현

VI는 다음 최적화 문제를 풀게 됨:

$$ q^*(Z) = \arg\min_{q(Z) \in Q} D_{KL}(q(Z) || p(Z|X)) $$

여기서 Q는 가능한 근사 분포의 집합임.

## 변분 하한 (ELBO)

VI는 다음의 변분 하한(ELBO: Evidence Lower BOund)을 최대화하는 것과 동일함:

$$ \text{ELBO}(q) = \mathbb{E}_{q(Z)}[\log p(X,Z)] - \mathbb{E}_{q(Z)}[\log q(Z)] $$

이는 다음과 같이 분해될 수 있음:

$$ \text{ELBO}(q) = \log p(X) - D_{KL}(q(Z)||p(Z|X)) $$

여기서 $\log p(X)$는 관측 데이터의 로그 증거임.

## 최적화 과정

VI는 다음 최적화 문제를 풀게 됨:

$$ q^*(Z) = \arg\max_{q(Z) \in Q} \text{ELBO}(q) $$

이는 $D_{KL}(q(Z)||p(Z|X))$를 최소화하는 것과 동일함.

## 특징

1. 계산 효율성: 복잡한 적분 대신 최적화 문제로 변환됨
2. 확장성: 대규모 데이터셋에 적용 가능함
3. 유연성: 다양한 모델과 분포에 적용 가능함
4. 근사적 방법: 정확한 사후 분포 대신 근사를 제공함

## 이걸 왜 하냐?
'진짜' 분포 p(x,z)가 최댓값을 갖도록 하기 위해 식을 세우면, log p(x)의 하한이 q(z|x)와 관련된 식으로 나옴. 이때, p(x,z)는 우리가 실제로 모르는 경우가 거의 대다수이므로, 이를 근사하여 추론하는 분포 q(z|x)를 잘 조작하여 하한을 최대한 높여, 생성 모델 p(x)의 로그 가능도(log-likelihood)가 최대가 되도록 하는 어떠한 추론 모델 q(z|x)를 구하는 과정임. 

이때, 하한값이 바로 ELBO이며, ELBO는 $E[\log p(x,z)] - E[\log q(z|x)]$로 표현됨. 여기서 중요한 점은 ELBO를 최대화하는 것이 실제로 KL divergence $KL(q(z|x) || p(z|x))$를 최소화하는 것과 동일하다는 것임. 즉, 우리가 찾고자 하는 q(z|x)를 실제 posterior p(z|x)에 가능한 한 가깝게 만드는 과정임.

이 ELBO 값이 최대가 되도록 하는 값을 찾기 위해 변분법을 사용함. 우리는 q(z|x)를 바꿔가며 하한의 최댓값을 찾는 것이 목적이기 때문에, 고전적 의미의 미분 대신 변분이라는 개념을 사용하여 하한을 찾고자 함. 이때, 오일러-라그랑주 equation 덕분에 ELBO의 정적분 안의 함수 F에 대하여 $\frac{δF}{δq}$ = 0 을 만족하는 q에 대해 변분이 0, 즉 하한이 극값을 갖는다는 사실이 증명되어 있음.

실제 최적화 과정에서는 보통 확률적 경사 상승법(stochastic gradient ascent)을 사용하여 ELBO를 최대화하는 q(z|x)의 파라미터를 반복적으로 업데이트함. 이 과정을 통해 우리는 복잡한 posterior 분포를 더 단순하고 다루기 쉬운 형태로 근사할 수 있으며, 이는 추천 시스템이나 다른 확률적 모델링 작업에서 매우 유용하게 활용될 수 있음.

# MFVI (Mean-Field Variational Inference)

MFVI는 VI의 한 형태로, 근사 분포 q(Z)가 각 잠재 변수에 대해 독립적이라고 가정함.

## 기본 가정

근사 분포 q(Z)를 다음과 같이 분해함:

$$ q(Z) = \prod_{i=1}^m q_i(Z_i) $$

여기서 m은 잠재 변수의 수임.

## 최적화 과정

1. 각 q_i(Z_i)에 대해 다른 모든 q_j(Z_j) (j ≠ i)를 고정하고 최적화함
2. 이 과정을 모든 i에 대해 반복함
3. 수렴할 때까지 전체 과정을 반복함

## 수학적 표현

MFVI의 최적화 문제는 다음과 같이 표현됨:

$$ q_i^*(Z_i) \propto \exp(\mathbb{E}_{j \neq i}[\log p(X, Z)]) $$

여기서 기대값은 i를 제외한 모든 j에 대해 계산됨.

## 수학적 정의

MFVI에서 근사 분포는 다음과 같이 분해됨:

$$ q(Z) = \prod_{i=1}^m q_i(Z_i) $$

## 최적화 과정

각 $q_i(Z_i)$에 대한 최적 해는 다음과 같이 주어짐:

$$ \log q_i^*(Z_i) = \mathbb{E}_{j \neq i}[\log p(X, Z)] + \text{const} $$

여기서 기대값은 i를 제외한 모든 j에 대해 계산됨.

## 좌표 상승법 (Coordinate Ascent)

MFVI는 일반적으로 다음 과정을 반복하는 좌표 상승법을 통해 최적화됨:

1. 각 i에 대해 $q_i(Z_i)$를 최적화:
   $$ q_i^{t+1}(Z_i) \propto \exp(\mathbb{E}_{q_{-i}^t}[\log p(X, Z)]) $$

2. t를 증가시키고 수렴할 때까지 반복

여기서 $q_{-i}^t$는 i를 제외한 모든 변수에 대한 t 시점의 근사 분포를 나타냄.

## 특징

1. 단순성: 복잡한 의존성을 무시하고 각 변수를 독립적으로 처리함
2. 계산 효율성: 고차원 문제를 더 작은 하위 문제들로 분해함
3. 병렬화 가능: 각 q_i(Z_i)를 동시에 최적화할 수 있음
4. 근사의 한계: 변수 간 의존성을 무시하므로 정확도가 떨어질 수 있음

# 생성 모델에서의 활용

1. 잠재 변수 모델:
   - VAE, LDA (Latent Dirichlet Allocation) 등의 모델에서 잠재 변수의 사후 분포를 근사하는 데 사용됨
   - 복잡한 사후 분포를 다루기 쉬운 형태로 근사하여 학습과 추론을 가능하게 함

2. 베이지안 신경망:
   - 신경망 가중치의 사후 분포를 근사하는 데 VI를 사용할 수 있음
   - 모델의 불확실성을 추정하고 과적합을 줄이는 데 도움이 됨

3. 확률적 프로그래밍:
   - 복잡한 확률 모델의 추론에 VI를 사용하여 계산 효율성을 높임

4. 강화학습:
   - 정책 최적화나 모델 기반 강화학습에서 복잡한 확률 분포를 다루는 데 활용됨

5. 이미지 생성:
   - VAE나 다른 생성 모델에서 이미지의 잠재 표현을 학습하는 데 VI 기법이 사용됨

6. 토픽 모델링:
   - LDA와 같은 토픽 모델에서 문서-토픽 분포와 단어-토픽 분포를 추론하는 데 MFVI가 널리 사용됨

7. 추천 시스템:
   - 사용자 선호도와 아이템 특성의 잠재 표현을 학습하는 데 VI 기법을 활용할 수 있음
   - 협업 필터링 모델의 베이지안 버전에서 모델 파라미터의 불확실성을 추정하는 데 사용됨

VI와 MFVI는 복잡한 확률 모델을 다루는 데 필수적인 도구로, 특히 대규모 데이터셋에 적용 가능한 확장성 있는 베이지안 추론을 가능하게 함. 이를 통해 생성 모델의 학습과 추론이 더욱 효율적이고 강건해질 수 있음. 다만, MFVI의 경우 변수 간 독립성 가정으로 인한 한계가 있으므로, 더 복잡한 의존성을 포착해야 하는 경우에는 다른 VI 기법을 고려해야 할 수 있음.

# 생성 모델에서의 수학적 응용

1. VAE (Variational Autoencoder)의 목적 함수:

   $$ \mathcal{L}(\theta, \phi; x) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p(z)) $$

   여기서 $q_\phi(z|x)$는 인코더, $p_\theta(x|z)$는 디코더를 나타냄.

2. GAN (Generative Adversarial Network)의 목적 함수:

   $$ \min_G \max_D V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1-D(G(z)))] $$

   여기서 G는 생성자, D는 판별자를 나타냄.

3. LDA (Latent Dirichlet Allocation)의 변분 추론:

   문서-토픽 분포 $\theta_d$와 단어-토픽 분포 $\phi_k$에 대한 Dirichlet 사전 분포를 가정하고, 이들의 사후 분포를 근사하기 위해 MFVI를 사용함.

4. 베이지안 신경망의 변분 추론:

   신경망 가중치 W의 사후 분포 $p(W|D)$를 근사하기 위해 가우시안 분포 $q(W|\theta)$를 사용하고, KL Divergence $D_{KL}(q(W|\theta)||p(W|D))$를 최소화함.

