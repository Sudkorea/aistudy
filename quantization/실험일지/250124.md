## 내가 지금까지 범한 실수

1. 대체 회사가 우리한테 뭘 바라고 과제를 던져줬나
	- 뭐해야하지? 평가 지표가 뭐지?
	- 이런 것에 대한 이해가 없음
2. Baseline 코드가 어떻게 생겼지?
	- 어떻게 돌아가지?

지금까지 1과 2를 동시에 생각하려니 task가 잘 돌아가지 않았음.

1에 대해 1시반까지 살펴보고, 정리한 다음 2로 넘어가자

## 대체 회사가 우리한테 뭘 바라고 있나
핵심 목표:
- 작은 VRAM 환경에서도 효율적으로 작동하는 경량 오디오 언어 모델 개발
- 베이스라인 모델의 성능은 유지하면서 더 작고 빠른 모델 구현

주요 평가 기준 (가중치 반영):
1. ASR(음성인식) 성능 - WER(Word Error Rate) ↓
2. AAC(오디오 캡셔닝) 성능 - SPIDEr ↑ 
3. 메모리 사용량 (2배 가중치) ↓
4. 지연시간 (2배 가중치) ↓
5. Hidden task(나중에 공개)

특별히 중요한 제약사항:
1. 제공된 데이터셋만 사용 가능 (외부 데이터 사용 금지)
2. ASR과 AAC는 반드시 동일한 모델로 측정
3. 최종 제출할 모델은 1개

이 기업이 원하는 것은:
1. 실제 디바이스 환경을 고려한 경량화 - 메모리와 지연시간에 2배의 가중치를 둔 것으로 보아 실제 서비스 적용을 고려
2. 다목적성 - 하나의 모델로 ASR과 AAC 두 가지 태스크를 모두 처리할 수 있어야 함
3. 효율적인 성능 - 베이스라인 수준의 정확도를 유지하면서도 리소스 사용은 최소화

해커톤의 핵심 : "실용적인 경량 오디오 언어 모델" 개발

## 데이터는 어떻게 생겼나
데이터셋은 크게 학습용(stage1, stage2)과 평가용(ASR, AAC) 데이터로 구분

학습 데이터 구조:
- path: 오디오 파일 경로
- text: 정답 텍스트
- task: 수행할 태스크 유형

평가 데이터 구조:
- testset_id: 테스트 샘플 ID
- path: 오디오 파일 경로
- task: 수행할 태스크 유형

데이터 출처:
1. ASR 관련:
- Librispeech: 오디오북 기반 음성인식 데이터
- GigaSpeech: 10,000시간 분량의 다양한 도메인 음성 데이터

2. 오디오 캡셔닝 관련:
- Clotho: 오디오 캡셔닝 전용 데이터셋
- WavCaps: ChatGPT 활용 약지도 오디오 캡셔닝 데이터
- AudioCaps: 일반 오디오에 대한 캡션 데이터

3. 음악 관련:
- MusicNet: 실시간 배경음악 감지 데이터

----
이제 Baseline Code 뜯어보자

## SALMONN
목적:
- 통합된 오디오-언어 모델 개발
- 단일 모델로 ASR, 음악 이해, 오디오 캡셔닝 등 다중 태스크 처리

주요 특징:
1. 모델 구조
- LLaMA를 기반으로 한 디코더 전용 아키텍처
- 오디오 인코더로 AudioMAE, MusicLM, WavLM 활용
- 멀티모달 입력을 위한 프롬프트 튜닝 전략 적용

2. 학습 방법
- Stage 1: 단일 오디오 인코더로 기본 학습
- Stage 2: 다중 오디오 인코더 통합
- 프롬프트 기반 학습으로 태스크별 지시 처리

성능:
- ASR: LibriSpeech test-clean WER 4.3%
- 음악 이해: MusicCaps 캡셔닝 SPIDEr 0.485
- 환경음 캡셔닝: AudioCaps SPIDEr 0.443

장점:
- 다양한 오디오 도메인 통합 처리
- 제로샷 성능 우수
- 유연한 프롬프트 기반 태스크 처리
### Structure

1. 입력 처리
- 텍스트 입력: 
  - 프롬프트와 지시문이 토크나이저를 통해 토큰화
  - 토큰은 임베딩 레이어를 통해 벡터로 변환

- 오디오 입력:
  - 오디오 시그널은 3가지 다른 인코더를 통과
  - AudioMAE: 일반적인 오디오/음성 처리
  - WavLM: 음성 특화 처리
  - MusicLM: 음악 특화 처리
  - 각 인코더의 출력은 프로젝션 레이어를 통해 동일한 차원으로 매핑

2. 토큰 시퀀스 구성
```
[텍스트 토큰] + [오디오 토큰] + [EOS 토큰]
```
- 텍스트 토큰: 프롬프트와 지시문
- 오디오 토큰: 인코더들의 출력을 프로젝션한 결과
- 모든 토큰은 동일한 임베딩 공간에 매핑됨

3. LLaMA 기반 디코더 처리
- 입력된 토큰 시퀀스는 LLaMA 트랜스포머 레이어들을 통과
- Self-attention과 Feed-forward 네트워크로 구성
- 각 레이어는 입력 시퀀스의 문맥을 점진적으로 처리

4. 출력 생성
- 디코더는 자기회귀적으로 토큰을 생성
- 생성된 토큰들은 다시 텍스트로 디코딩되어 최종 출력

특징적인 부분:
1. 멀티모달 프롬프트 처리
- 텍스트와 오디오를 자연스럽게 결합
- 태스크별 프롬프트 형식 지원

2. 다중 인코더 활용
- 각 도메인에 특화된 인코더 사용
- 인코더 출력을 적절히 통합

3. 2단계 학습
- Stage 1: 단일 인코더로 기본 학습
- Stage 2: 다중 인코더 통합 학습

4. Zero-shot 능력
- 명시적인 프롬프트만으로 다양한 태스크 수행 가능
- 새로운 태스크에 대한 적응력 우수
