## **1. 데이터 유형별 계보**

### **1.1 텍스트 데이터**

텍스트 데이터는 자연어 처리를 중심으로 발전했어.

- **Word Embedding**
    - 핵심 아이디어: 단어를 벡터로 표현.
    - 발전 흐름:
        - Count-based Methods (TF-IDF, Bag of Words)
        - Word2Vec (Skip-gram, CBOW)
        - GloVe
- **시퀀스 모델링**
    - 핵심 아이디어: 단어의 순서와 문맥을 모델링.
    - 발전 흐름:
        - RNN → LSTM/GRU → seq2seq 모델 → Transformer → GPT, BERT, T5
- **대규모 사전 학습**
    - 핵심 아이디어: 대규모 코퍼스를 활용한 언어 모델 학습.
    - 발전 흐름:
        - GPT (Generative Pre-trained Transformer)
        - BERT (Bidirectional Encoder Representations)
        - ChatGPT, GPT-4, PaLM 등

---

### **1.2 이미지 데이터**

이미지 데이터는 합성곱 신경망(CNN)을 중심으로 발전.

- **초기 모델**
    - LeNet: 숫자 인식을 위한 초기 CNN.
    - AlexNet: 딥러닝 시대를 연 이미지 분류 모델.
- **네트워크 심화**
    - VGG: 네트워크를 더 깊게 확장.
    - ResNet: Residual Connection으로 학습 안정성 개선.
- **새로운 접근**
    - Vision Transformer (ViT): 이미지 데이터를 패치로 나눠 Transformer 방식으로 처리.

---

### **1.3 시계열 데이터**

시계열 데이터는 순차적인 시간 의존성을 다룸.

- **초기 모델**
    - ARIMA: 통계 기반 시계열 예측.
    - HMM (Hidden Markov Model): 확률적 시퀀스 모델링.
- **딥러닝 접근**
    - RNN → LSTM/GRU: 장기 종속성 문제 해결.
    - Temporal Convolutional Network (TCN): CNN 기반 시계열 처리.
    - Transformer: 시퀀스 데이터에 Self-Attention 적용.

---

### **1.4 그래프 데이터**

그래프 데이터는 노드와 엣지 간의 관계를 학습.

- **초기 모델**
    - Random Walk 기반: DeepWalk, Node2Vec.
    - Matrix Factorization 기반: LINE, Graph Factorization.
- **그래프 신경망 (GNN)**
    - 발전 흐름:
        - GCN (Graph Convolutional Network): 그래프에서 합성곱 연산.
        - GAT (Graph Attention Network): Attention 적용.
        - NGCF → LightGCN: 협업 필터링 기반의 그래프 모델.

---

### **1.5 표 형태 데이터 (Tabular Data)**

표 데이터는 구조적이고 정형화된 데이터를 처리.

- **전통적인 머신러닝**
    - Logistic Regression, Decision Trees, Random Forest.
    - Gradient Boosting Machines (GBM): XGBoost, LightGBM, CatBoost.
- **딥러닝 접근**
    - TabNet: Attention 기반의 Tabular 데이터 처리 모델.
    - TabTransformer: Transformer 구조를 활용.

---

## **2. 데이터 접근 방식**

이제 데이터를 처리하는 방식을 나열해볼게.

### **2.1 데이터 접근 방식**

- **시퀀스 기반**: 데이터를 시간적/순서적 흐름으로 다룸 (RNN, Transformer).
- **계층적 접근**: 데이터의 계층적 구조를 모델링 (트리, Decision Trees).
- **그래프 기반**: 데이터 간 관계를 노드-엣지 형태로 표현 (GNN 계열).
- **패치 기반**: 데이터를 작은 조각으로 나누어 처리 (CNN, ViT).
- **확률적 접근**: 데이터의 확률적 특성을 모델링 (HMM, Bayesian Networks).

---

## **3. 데이터 유형과 접근 방식의 융합**

이제 데이터 유형과 접근 방식을 엮어보자.

### **텍스트 데이터**

- 시퀀스 기반: RNN, LSTM, seq2seq, Transformer.
- 그래프 기반: 단어/문장 간 관계를 그래프로 모델링 (Text GCN).

### **이미지 데이터**

- 패치 기반: CNN, Vision Transformer.
- 그래프 기반: 이미지의 픽셀 관계를 그래프로 표현 (Graph-based Image Models).

### **시계열 데이터**

- 시퀀스 기반: RNN, LSTM, Transformer.
- 그래프 기반: 시간 간 의존성을 그래프로 모델링 (Temporal Graph Networks).

### **그래프 데이터**

- 그래프 기반: GCN, GAT, NGCF, LightGCN.
- 시퀀스 기반: 그래프의 경로를 시퀀스로 변환 (Random Walk + RNN).

### **표 데이터**

- 트리 기반: Random Forest, Gradient Boosting Machines.
- 딥러닝 기반: TabNet, TabTransformer.

---

### **4. 분석 방향**

- 데이터 유형과 접근 방식이 어떻게 융합되었는지 탐구.
- 특정 데이터 유형에서 새로운 접근 방식이 도입된 사례 분석.
- 서로 다른 접근 방식이 동일한 데이터 유형에서 어떤 장단점을 가졌는지 비교.

---

각 발전 흐름에서 **패러다임의 전환**을 초점으로 분석하면, 기술적 도약이 왜 일어났고 어떤 문제를 해결했는지 명확히 이해할 수 있어. 아래는 주요 데이터 유형별 흐름에서의 **패러다임 전환**을 정리한 내용이야.

---

## **1. 텍스트 데이터**

### **1.1 Word Embedding의 도입**

- **전환 이전**: 단어를 정수 인덱스나 One-Hot 벡터로 표현 → 희소 벡터 문제.
- **패러다임 전환**: Word2Vec, GloVe가 **분산 표현(distributed representation)**을 도입.
    - 단어 간 의미적 유사성을 벡터 공간에서 나타냄.
    - `King - Man + Woman ≈ Queen` 같은 관계 학습 가능.

---

### **1.2 seq2seq와 Attention 도입**

- **전환 이전**: RNN, LSTM으로 시퀀스 학습 → 장기 종속성 문제와 계산 병목.
- **패러다임 전환**: seq2seq 모델에 **Attention 메커니즘** 추가.
    - 입력 시퀀스 전체를 동적으로 참조.
    - 번역, 요약 등에서 큰 성능 향상.

---

### **1.3 Transformer의 등장**

- **전환 이전**: RNN, LSTM은 순차적 계산 필요 → 병렬화 불가능.
- **패러다임 전환**: **Self-Attention**을 기반으로 한 Transformer 도입.
    - 순차적 계산 제거 → 병렬화 가능.
    - 대규모 모델 학습의 기반 마련 (GPT, BERT).

---

### **1.4 대규모 사전 학습 언어 모델**

- **전환 이전**: task-specific 학습 (문제별로 새로운 모델 학습).
- **패러다임 전환**: 대규모 **Pretraining + Fine-Tuning** 패러다임.
    - Pretrained Language Model (GPT, BERT)로 일반적 언어 이해 능력 확보.
    - 다양한 task에 효율적으로 활용 가능.

---

## **2. 이미지 데이터**

### **2.1 CNN의 등장**

- **전환 이전**: 이미지의 픽셀을 전역적으로 처리 → 고차원 데이터로 인한 과적합.
- **패러다임 전환**: **합성곱 연산**으로 국소적 특징 학습 가능.
    - 공간적 구조를 효율적으로 처리.
    - LeNet, AlexNet으로 이미지 분류 정확도 급상승.

---

### **2.2 Residual Networks (ResNet)의 등장**

- **전환 이전**: 네트워크가 깊어질수록 **기울기 소실(vanishing gradient)** 문제.
- **패러다임 전환**: **Residual Connection**으로 학습 안정성 개선.
    - 더 깊은 네트워크 학습 가능 (152-layer ResNet).

---

### **2.3 Vision Transformer (ViT)의 등장**

- **전환 이전**: CNN은 국소적 특징 학습에 강하지만, 전역적 관계 학습이 약함.
- **패러다임 전환**: Transformer를 이미지 처리에 적용.
    - 이미지를 패치로 분할 → Self-Attention으로 전역적 관계 학습.

---

## **3. 시계열 데이터**

### **3.1 RNN의 등장**

- **전환 이전**: ARIMA, HMM 등 통계적/확률적 접근.
- **패러다임 전환**: RNN이 **학습 기반 순차 모델링** 도입.
    - 데이터의 시간적 종속성을 학습.

---

### **3.2 LSTM/GRU의 등장**

- **전환 이전**: RNN은 장기 종속성(Long-term Dependency) 학습이 어려움.
- **패러다임 전환**: **게이트 메커니즘** 도입 (LSTM, GRU).
    - 멀리 떨어진 시점 간 관계 학습 가능.

---

### **3.3 Transformer로 시계열 처리**

- **전환 이전**: 순차적 특성을 강하게 이용 → 병렬 처리 어려움.
- **패러다임 전환**: Transformer 기반 모델로 **병렬 처리와 전역적 학습** 가능.

---

## **4. 그래프 데이터**

### **4.1 GCN의 등장**

- **전환 이전**: 그래프 데이터는 주로 임베딩 기반 접근 (DeepWalk, Node2Vec).
- **패러다임 전환**: **그래프 구조를 직접 활용**하는 GCN 도입.
    - 노드와 엣지의 관계를 학습.
    - 합성곱 연산을 그래프 형태로 확장.

---

### **4.2 GAT의 등장**

- **전환 이전**: 모든 이웃 노드를 동일한 중요도로 처리.
- **패러다임 전환**: **Attention 메커니즘**으로 노드 중요도 학습.
    - 유사 노드 간 가중치를 학습해 성능 개선.

---

### **4.3 NGCF와 LightGCN의 등장**

- **전환 이전**: 협업 필터링에서 단순 행렬 분해 접근.
- **패러다임 전환**:
    - NGCF: GCN을 협업 필터링에 적용 → 유저-아이템 관계 학습.
    - LightGCN: 비선형 활성화 제거로 계산 효율성과 성능 개선.

---

## **5. 표 형태 데이터**

### **5.1 Gradient Boosting Machines의 등장**

- **전환 이전**: Decision Tree, Random Forest로 분류/회귀.
- **패러다임 전환**: **Boosting**으로 순차적 모델 개선.
    - XGBoost, LightGBM 등으로 높은 성능.

---

### **5.2 딥러닝 기반 Tabular 모델**

- **전환 이전**: GBM 계열이 표 데이터의 표준.
- **패러다임 전환**: Attention 기반 TabNet, TabTransformer.
    - 딥러닝을 표 데이터에 적합하게 변형.

