![[[NLP 이론] (10강) Decoding을 통한 자연어 생성.pdf]]

# 자연어 생성 과정으로서 Decoding의 수학적 정의 
결국 확률 P를 어떻게 극대화해서, 좋은 답변 뽑을건지 고찰이 필요하단거임.
# Greedy decoding의 동작 방식 
매 시간별 가장 높은 확률값 갖는 Token 선택하는거

## 단점
이상한거 뽑으면 제대로 우르르 무너져서 확률이 낮아짐. 근시안적이야

## Decoding 언제 끝남?
EOS 토큰 택할때까지
# Beam search를 통한 문장 생성 
Greedy 보완한 방식

각 시간마다 k개의 후보 단어 생성함.(k는 bean-size)

그럼 수형도 k^t개쯤 나오지 않나? -> 아 이거 그냥 문장은 K개만 쓰고, 각 스탭에서 k개 구하고 그 중 가장 좋은거만 쓰는거인듯?

score를 아까 구한 확률 P에 로그씌운거로 다룸 -> 1과 가까워질수록 로그는 0과 가까워지므로, p 다 곱한거 = logP 다 더한거니까 0이랑 가까운 문장을 쓰는거임

## 언제끝남? 
생성 끝나면 멈추고, Beam Size만큼 후보 유지하면서 계속 생성
EOS 생성 이전에 특정 시간 정해두고 넘으면 종료, 문장이 특정 개수 도달하면 종료
-> 자연스레 따라오는 의문 : 이거 그러면 문장 길이 길면 불리한거 아닌가?
-> A : 정규화 시켜줘야지

## 단점
- 반복에 매우 취약
- 사람이라는게 확률 높은쪽으로만 말하지 않음

## Sampling 기반 생성 방법과 Temperature의 효과 
위에 단점 보이지? 사람이 확률로 이야기하면 그게 기계라서, 확률 분포에 따라 랜덤하게 단어 샘플링하는거임.
이때, 그 랜덤한걸 많이 뽑을거냐, 아니면 적게 뽑을거냐? 이거 정해주는게 Temperature $\tau$

## Case of Temperature
### $i\ )\ \tau>1$
출력 확률분포가 평탄해짐 -> 막 고르게됨. 
$\tau\rightarrow\infty$면 진짜 아무거나 뽑음

### $ii\ )\ \tau<1$ 
출력 확률분포가 뾰족해짐 -> 원래 뽑혀야할게 잘 뽑힘
$\tau\rightarrow 0$ 이면 greedy랑 똑같아짐

## Top-k sampling, Top-p sampling의 개념과 활용
## Top k sampling
진짜 아무거나 택하면 문제가 있으니까, 확률 높은 k개중에서 뽑겠다는거임. ai 단어들 한번 잘 파고들어보면 진짜 뭔가 별거없음. 수학같은 삼삼한 매력이 있음

근데 이러면 중요한 단어가 2~3개인데 이상한 10개 가져와서 똑같은 문제 생길수도 있겠지?
## Top-p sampling
그래서 누적 확률 합 p 이상이면 멈추도록 만드는게 top-p sampling
Nucleus sampling이라고도 부름.