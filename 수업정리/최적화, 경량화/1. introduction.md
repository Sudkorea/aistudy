![[[모델최적화및경량화] (1강) Introduction.pdf]]
# **AI 모델 경량화 및 최적화 개요**

---

## **1. AI 모델 경량화 (AI Model Lightweighting)**

💡 **이 개념이 왜 생겼을까?**  
대형 AI 모델은 높은 연산 비용과 메모리 사용량을 요구하며, 특히 **엣지 디바이스**나 **실시간 응용**에서는 실행이 어려움. 이를 해결하기 위해 **모델 경량화 기법**이 개발됨.
- 실시간 처리
- 저자원 환경(온디바이스)
- 에너지 효율성

📌 **🔎 생각해보기**

- AI 모델 크기와 연산 속도 사이의 관계를 수식으로 표현할 수 있을까?
	- 빅O노테이션 쓰면 되겠지
- 모바일 디바이스에서 AI 모델을 효율적으로 실행하려면 어떤 기법이 필요할까?
	- 모델 사이즈 줄여야하니까 pruning - Quantization 쓰지않을지

📝 **💭 나의 메모**  
아 저사진 진짜 찰떡인데 저작권있을까봐 여기 못가져오겠네


---

### **1.1 Pruning (가지치기)**

✅ **핵심 아이디어**  
모델이 필요 없는 가중치(weight)나 뉴런을 제거하여 연산량을 줄이는 방법.

✅ **왜 중요한가?**

- 불필요한 뉴런을 제거하여 모델 크기 감소.
- 연산 속도 향상 및 메모리 절약.
- 정확도를 크게 손상시키지 않으면서 경량화 가능.

🧠 **📌 예습용 Task**  
🔹 **실험하기**: ResNet 모델에서 가지치기 전후의 파라미터 수 비교.  
🔹 **그래프 그리기**: Pruning 비율과 정확도 감소 간의 관계를 그래프로 표현해보자.

📝 **💭 나의 메모**  
(이 부분에 자신의 생각을 적어보자!)

---

### **1.2 Knowledge Distillation (지식 증류)**

✅ **핵심 아이디어**  
큰 모델(Teacher)이 작은 모델(Student)에게 학습된 지식을 전달하여 **모델 크기를 줄이면서도 성능을 유지하는 기법**.

✅ **왜 중요한가?**

- 기존의 Teacher 모델을 활용하여 효율적으로 학습 가능.
- Student 모델이 Teacher 모델의 성능을 최대한 유지하면서 경량화됨.

🧠 **📌 예습용 Task**  
🔹 **비교 분석**: Knowledge Distillation과 Transfer Learning의 차이점 분석.  
🔹 **수식 유도**: KL Divergence Loss를 이용한 Distillation Loss 수식을 유도해보자.

📝 **💭 나의 메모**  
(이 부분에 자신의 생각을 적어보자!)

---

### **1.3 Quantization (양자화)**

✅ **핵심 아이디어**  
모델의 가중치를 **저해상도 데이터 타입**으로 변환하여 연산 속도를 증가시키는 기법.

✅ **왜 중요한가?**

- 8-bit 연산으로 변환하면 속도가 증가하고 메모리 사용량이 줄어듦.
- 일부 하드웨어(예: Edge TPU)에서 양자화된 모델만 실행 가능.

🧠 **📌 예습용 Task**  
🔹 **실험하기**: 동일한 모델을 FP32와 INT8에서 실행했을 때 속도 차이 측정.  
🔹 **적용 사례 분석**: Quantization이 적용된 실제 논문을 찾아보고 요약.

📝 **💭 나의 메모**  
(이 부분에 자신의 생각을 적어보자!)

---

## **2. AI 모델 경량화 재학습 (AI Model Lightweight Re-training)**

💡 **이 개념이 왜 필요할까?**

- 기존 모델을 직접 줄이는 방식 외에도, 경량화된 모델을 **다시 학습**하여 성능을 보완하는 방법이 존재함.
- 일부 구조만 변경하여 **원래 모델과의 호환성을 유지하면서도** 연산 효율을 높일 수 있음.

📌 **🔎 생각해보기**

- AI 모델을 경량화할 때 학습 데이터는 원본과 동일해야 할까?
- 미세 조정(fine-tuning)과 경량화 재학습은 어떤 차이가 있을까?

📝 **💭 나의 메모**  
파라미터 효율적인 전이 학습 **(Parameter-Efficient Fine-Tuning - PEFT)**
- PEFT 기법은 훈련된 모델을 자원 효율적인 방식으로 재학습하는 방법을 통칭함

---

### **2.1 Adapter (어댑터)**

✅ **핵심 아이디어**  
기존 모델을 유지하면서 특정 레이어만 추가 학습하여 경량화된 모델을 생성하는 기법.

✅ **왜 중요한가?**

- 원래 모델의 구조를 크게 변경하지 않으면서 적응 가능.
- 새로운 데이터에 맞춰 가볍게 조정 가능.
- Transfer Learning과 결합하여 효과적인 학습 가능.

🧠 **📌 예습용 Task**  
🔹 **비교 실험**: Adapter 적용 모델과 기존 모델의 학습 시간 비교.  
🔹 **적용 사례 찾기**: Adapter 기반 모델이 활용된 논문을 조사.

📝 **💭 나의 메모**  
기존의 3.6%만 사용해도 비슷한 성능에 도달한다는데, 그러면 학습시간 못해도 1/25 줄이는건가

---

### **2.2 Low-Rank Adaptation (LoRA)**

✅ **핵심 아이디어**  
모델의 가중치를 완전히 변경하는 대신, **저차원 행렬(Low-Rank Matrix)**을 사용하여 미세 조정하는 방식.

✅ **왜 중요한가?**

- 기존 모델보다 훨씬 적은 파라미터만 업데이트 가능.
- GPU 메모리 사용량이 크게 줄어듦.
- 큰 모델을 작은 GPU에서도 효과적으로 fine-tuning 가능.

🧠 **📌 예습용 Task**  
🔹 **수식 분석**: LoRA의 핵심 아이디어인 저차원 행렬 분해를 수식으로 정리.  
🔹 **실험하기**: 동일한 모델에 LoRA 적용 전후의 학습 속도 비교.

📝 **💭 나의 메모**  
(이 부분에 자신의 생각을 적어보자!)

---

## **3. AI 모델 메모리 최적화 (AI Model Memory Optimization)**

💡 **이 개념이 왜 필요할까?**

- AI 모델을 경량화하더라도, **훈련 및 추론 과정에서의 메모리 사용량을 최적화하는 것이 필수적**임.
- 메모리 최적화를 통해 모델을 더 빠르고 효율적으로 실행할 수 있음.

📌 **🔎 생각해보기**

- 병렬 연산이 메모리 사용량에 어떤 영향을 미칠까?
- 모델 구조를 변경하지 않고 메모리 사용량을 줄이는 방법이 있을까?

📝 **💭 나의 메모**  
(이 부분에 자신의 생각을 적어보자!)

---

### **3.1 Parallel Computation (병렬 연산)**

✅ **핵심 아이디어**  
모델 연산을 여러 GPU나 CPU에서 **동시에 처리**하여 연산 속도를 증가시키는 방법.

✅ **왜 중요한가?**

- 대형 모델을 훈련할 때 메모리 부족 문제를 완화.
- 추론 속도를 대폭 향상 가능.
- 분산 학습 환경에서 필수적인 기술.

🧠 **📌 예습용 Task**  
🔹 **실험하기**: 단일 GPU vs. 멀티 GPU에서 동일 모델 학습 속도 비교.  
🔹 **적용 사례 분석**: 대형 모델(예: GPT)에서 병렬 연산이 어떻게 활용되는지 조사.

📝 **💭 나의 메모**  
(이 부분에 자신의 생각을 적어보자!)

---

### **3.2 Efficient Architecture (효율적인 모델 구조)**

✅ **핵심 아이디어**  
기존의 깊고 복잡한 모델 구조를 단순화하여 **효율적인 연산이 가능하도록 최적화**하는 방법.

✅ **왜 중요한가?**

- 불필요한 연산을 줄여 메모리 사용량 절감.
- 연산 속도 증가 및 배터리 소비 절감(특히 모바일 환경).
- 최근 연구에서 Transformer의 대안으로 다양한 새로운 구조 제안됨.

🧠 **📌 예습용 Task**  
🔹 **비교 분석**: Transformer와 Efficient Transformer(Sparse Attention) 구조 비교.  
🔹 **논문 분석**: 최근 Efficient Architecture 논문 요약 및 개념 정리.

📝 **💭 나의 메모**  
(이 부분에 자신의 생각을 적어보자!)
