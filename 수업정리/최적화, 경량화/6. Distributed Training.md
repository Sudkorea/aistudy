(병렬컴퓨팅)

## Data Parallelism
큰 데이터를 여러 GPU에 분할해서 동시에 처리함.

이론적으로 학습 시간 1/n이 됨. 근데,각 GPU 위에 올라간 데이터를 복제한 모델들에 넣어서 돌리는거라, 메모리 사용량이 증가함

### DDP

팀장이 없고 AllReduce 써서 모든 GPU 동시에 업뎃


![[[모델최적화및경량화] (10강) Distributed Training (2) & Wrap-Up.pdf]]


## Model Parallelism

### Tensor Parallelism
모델의 텐서를 포뜨는거

결국 행렬을 어떻게 쪼개느냐

### Pipeline Parallelism
모델의 각 층을 뭉텅이로 써는거

데이터 Batch -> Micro Batch들로 자름 -> batch 들어가고 이전거 끝나면 다음꺼로 넘기고..

#### Sync

#### Async