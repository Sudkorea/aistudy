모든 optimizer들은 특정 학습률을 필요로 함.

- 학습률이 높은 경우:
  - Loss가 급증할 수 있음.
  - 최적점을 넘어서 발산할 위험이 있음.
  - 

$$\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t)$$

여기서 $\eta$가 큰 경우, $\theta_{t+1}$이 크게 변화함.

- 학습률이 낮은 경우:
  - Loss는 줄어들지만 매우 천천히 수렴함.
  - 지역 최소값에 갇힐 수 있음.
  - 같은 식에서 $\eta$가 작은 경우, $\theta_{t+1}$이 작게 변화함.

일반적으로 큰 초기 학습률을 사용하고, 이후 최적해에 근접하기 위해 작은 학습률을 사용함.

더 큰 미니 배치는 그래디언트 추정의 노이즈가 적어 큰 초기 학습률을 사용할 수 있음.

주요 Learning Rate Decay 방식:

1. Step Decay:
   - 특정 에폭마다 학습률을 감소시킴.
   - 수식: $$\eta_t = \eta_0 \cdot \gamma^{\lfloor t/k \rfloor}$$
     여기서 $\gamma$는 감소 비율, $k$는 스텝 크기임.

2. Exponential Decay:
   - 지수 함수적으로 학습률을 감소시킴.
   - 수식: $$\eta_t = \eta_0 \cdot e^{-kt}$$
     여기서 $k$는 감소 속도를 제어하는 하이퍼파라미터임.

3. Cosine Annealing:
   - 코사인 함수를 사용하여 학습률을 조절함.
   - 수식: $$\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{t\pi}{T}))$$
     여기서 $T$는 총 학습 스텝 수임.

4. Linear Decay:
   - 선형적으로 학습률을 감소시킴.
   - 수식: $$\eta_t = \eta_0(1 - \frac{t}{T})$$
     여기서 $T$는 총 학습 스텝 수임.

5. Inverse Square Root Decay:
   - 역제곱근 함수를 사용하여 학습률을 감소시킴.
   - 수식: $$\eta_t = \frac{\eta_0}{\sqrt{t}}$$

