맨 처음 모델을 돌리기 위해 가중치를 임의의 값으로 맞춰줘야 하고, 그 값이 전부 같거나 하는 등의 경우 안장점, 고립 특이점 등 뭔가 이상하게 나올 수 있으므로 랜덤으로 초기화해주는것
추가적으로,
- 가중치를 모두 0으로 초기화하면 모든 뉴런이 같은 출력을 내어 네트워크가 제대로 학습하지 못함.
- 랜덤 초기화는 대칭성 문제를 해결하고, 네트워크가 다양한 특징을 학습할 수 있게 함.
- 적절한 초기화는 학습 초기 단계에서 그래디언트 소실/폭발 문제를 완화하는 데 도움을 줌.
# Small Gaussian Random

- 가중치를 평균 0, 표준편차 0.01의 가우시안 분포에서 초기화함.
- 수식

$$W \sim \mathcal{N}(0, 0.01)$$

- 장점: 구현이 간단함. 초기에 작은 값으로 시작하여 학습의 안정성을 높일 수 있음.
- 단점: 깊은 네트워크에서 그래디언트 소실 문제가 발생할 수 있음. 활성화 값이 0에 가까워져 학습이 느려질 수 있음.

# Large Gaussian Random

- 가중치를 평균 0, 표준편차 1의 가우시안 분포에서 초기화함.
- 수식

 $$W \sim \mathcal{N}(0, 1)$$
 
- 장점: 더 큰 초기값으로 인해 그래디언트 소실 문제를 일부 완화할 수 있음.
- 단점: 깊은 네트워크에서 그래디언트 폭발 문제가 발생할 수 있음. 활성화 값이 포화 상태에 빠질 수 있음.

# Xavier Initialization

- 입력과 출력 뉴런 수를 고려하여 가중치를 초기화함.
- 수식
  - 균등 분포

$$W \sim \mathcal{U}(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}})$$

  - 정규 분포

$$W \sim \mathcal{N}(0, \sqrt{\frac{2}{n_{in} + n_{out}}})$$

- $n_{in}$: 입력 뉴런 수, $n_{out}$: 출력 뉴런 수,  $\mathcal{N}$은 정규 분포, $\mathcal{U}$는 균등 분포
- 장점: 
  - 각 층의 출력 분산을 일정하게 유지하여 그래디언트 소실/폭발 문제를 완화함.
  - sigmoid나 tanh 활성화 함수와 함께 사용할 때 특히 효과적임.
- 단점: ReLU 활성화 함수와 함께 사용할 때는 최적이 아닐 수 있음.

# He Initialization

- ReLU 활성화 함수를 위해 설계된 초기화 방법임.
- 수식: 
  - 균등 분포

$$W \sim \mathcal{U}(-\sqrt{\frac{6}{n_{in}}}, \sqrt{\frac{6}{n_{in}}})$$

  - 정규 분포

$$W \sim \mathcal{N}(0, \sqrt{\frac{2}{n_{in}}})$$

- $n_{in}$: 입력 뉴런 수,  $\mathcal{N}$은 정규 분포, $\mathcal{U}$는 균등 분포
- 장점: 
  - ReLU 및 그 변형 활성화 함수와 함께 사용할 때 특히 효과적임.
  - 깊은 네트워크에서도 안정적인 학습을 가능하게 함.
- 단점: sigmoid나 tanh 활성화 함수와 함께 사용할 때는 최적이 아닐 수 있음.


그래디언트 소실/폭발 문제는 역전파 과정 등 그래디언트를 사용하여 가중치를 조정하는 과정에서 일어남. 
- 그래디언트 소실의 경우, 그래디언트 값이 너무 작거나 0 등 다른 그래디언트값과 차이점을 찾기 힘들게 나와서, 가중치를 조절할 때 사용할 수 없게 됨
- 그래디언트 폭발의 경우, 그래디언트값이 너무 커져서 가중치가 인플레이션이 오나..? 대충 그런 문제가 생김.