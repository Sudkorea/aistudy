
## 역전파를 왜 쓰는걸까

신경망은 입력층, 출력층, 그리고 하나 이상의 은닉층으로 구성됨. 

입력과 출력 사이의 관계는 은닉층, 활성화 함수, 가중치 등으로 인해 비선형적일 수 있음.

학습 과정에서 훈련 데이터를 사용해 입력에 대한 출력을 계산하고, 그 결과를 바탕으로 가중치와 편향을 조정함. 이 조정에는 여러 방법이 있는데, 손실 함수를 계산하고 경사 하강법 등의 최적화 알고리즘을 적용하는 것이 대표적임.

역전파는 다층 신경망에서 출력층부터 입력층 방향으로 각 층의 가중치를 효율적으로 조정하는 알고리즘임. 이는 연쇄 법칙을 이용해 각 가중치가 최종 오차에 미치는 영향을 계산하고, 이를 바탕으로 가중치를 업데이트함.

## 그래디언트가 왜 중요할까?

ML에서 그래디언트가 되게 중요하게 다루어진다는 사실을 느끼고 있는데, 아직 그 이유가 잘 실감이 안남. 내 방식대로 그 이유를 추론해보겠음.

그래디언트가 중요한 이유 : 각 변수에 따른 편미분값이 그래디언트임. 당장 선형 모델만 생각해도, 기울기가 곧 가중치 W임. 비선형의 경우에도, 우리는 '어떤 함수를 따라갈것이다'라는 것을 맨 처음 가정하고, 각 변수의 가중치 W1, W2,... 등을 학습을 통해 구하는 것이 목적임. 따라서, train data를 통해 변수에 따른 출력값과, 그 변화량(미분값)을 따져서 가중치를 업데이트하는 과정은 필연적으로 있어야 하고, 가중치 업데이트에 그래디언트가 무조건 쓰이기 때문임.
 
몇 가지 보완할 점을 추가해 정리함:

1. 최적화 방향 제시: 그래디언트는 함수값이 가장 빠르게 증가하는 방향을 가리킴. 손실 함수를 최소화하는 과정에서 그래디언트의 반대 방향으로 이동하면 가장 효율적으로 최적점에 도달할 수 있음.

2. 효율적인 학습: 그래디언트를 이용한 경사 하강법은 계산 효율성이 높음. 특히 고차원 공간에서 최적화할 때 유용함.

3. 비선형 문제 해결: 비선형 모델에서도 그래디언트를 통해 국소적으로 선형 근사를 할 수 있어, 복잡한 함수의 최적화에 적용 가능함.

4. 역전파 알고리즘: 신경망에서 그래디언트는 역전파의 핵심임. 각 층의 가중치가 최종 오차에 미치는 영향을 계산하는 데 사용됨.

5. 학습률 조정: 그래디언트의 크기는 학습률 조정에 중요한 정보를 제공함. 이를 통해 적응적 학습률 방법들이 개발됨.

그래디언트는 단순히 가중치 업데이트에만 쓰이는 게 아니라, ML 알고리즘의 학습 과정 전반에 걸쳐 중요한 역할을 함.

# Attention
네, attention model의 기본적인 작동 원리를 잘 이해하고 계십니다. 그러나 몇 가지 세부사항을 추가하여 더 정확하게 설명드리겠습니다.

1. 인코더-디코더 구조:
   - Attention model은 대개 seq2seq와 유사한 인코더-디코더 구조를 기반으로 합니다.
   - 인코더는 입력 시퀀스를 처리하고, 디코더는 출력 시퀀스를 생성합니다.

2. 인코더 처리:
   - 입력 시퀀스 S = (s₁, s₂, ..., sₙ)가 인코더에 입력됩니다.
   - 인코더는 각 입력 토큰에 대해 hidden state를 생성합니다: H = (h₁, h₂, ..., hₙ)

3. 디코더와 Attention 메커니즘:
   - 디코더는 각 시간 단계 t에서 출력을 생성합니다.
   - 이 때 attention 메커니즘이 작동합니다:
     a) 현재 디코더 상태 sₜ와 모든 인코더 hidden states H를 사용하여 attention scores를 계산합니다.
     b) 이 scores를 정규화하여 attention weights를 얻습니다.
     c) attention weights와 인코더 hidden states의 가중합을 계산하여 context vector cₜ를 생성합니다.

4. 출력 생성:
   - context vector cₜ와 현재 디코더 상태 sₜ를 결합(concatenate)하여 출력 단어의 확률 분포를 계산합니다.
   - 이 분포에서 가장 높은 확률을 가진 단어를 선택하거나 sampling하여 출력 aₜ를 생성합니다.

5. 다음 상태로의 전이:
   - 출력 aₜ, 이전 상태 sₜ, context vector cₜ를 사용하여 다음 디코더 상태 sₜ₊₁을 계산합니다.

6. 손실 함수 계산:
   - 각 시간 단계에서 생성된 출력과 실제 목표 출력 간의 차이를 측정하여 손실을 계산합니다.
   - 일반적으로 cross-entropy loss를 사용합니다.

7. 학습:
   - 계산된 손실을 바탕으로 역전파를 수행하여 모델의 파라미터를 업데이트합니다.

주요 차이점:
- 전통적인 seq2seq 모델과 달리, attention model은 디코더가 출력을 생성할 때 인코더의 모든 hidden states에 "주의를 기울일" 수 있습니다.
- 이를 통해 모델은 입력 시퀀스의 다른 부분에 동적으로 집중할 수 있어, 특히 긴 시퀀스를 처리할 때 성능이 향상됩니다.

Attention model의 이러한 메커니즘은 기계 번역, 요약, 질문 답변 등 다양한 자연어 처리 작업에서 큰 성공을 거두었으며, Transformer 아키텍처의 기반이 되었습니다.