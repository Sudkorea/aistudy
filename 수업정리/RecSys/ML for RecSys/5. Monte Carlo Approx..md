![[[MLforRecSys] (5강) 마르코프 연쇄 몬테카를로 방법 I.pdf]]
확률적 샘플링을 통해 복잡한 수학적 문제를 근사적으로 해결하는 기법

이걸 왜하냐? 10차원 공간 적분같은거 생각하면, 푸는데 진짜 힘듦. 근데 그거 굳이 적분식 생각 안하고, 샘플링해서 적분하면 해결됨.


아래 너무 멋부려서 정작 필요한 내용이 없는듯. 내식대로 다시 정리함. 왜 교육 끝나고나서야 같은 나쁜말은 ㄴㄴ

일반론적인 MC sampling의 한계 : ai 분야에서 uniform sampling을 쓰면 문제 해결에 에러사항이 있음. 샘플링이 어렵다거나 하는 등. 따라서, 주로 쓰이는게 Rejection sampling, Importance Sampling임

## Rejection Sampling

이거 어디서봤지?

## Importance Sampling


## Markov Chains

rejection sampling을 잘 생각해보면, 샘플을 어떻게 잡을지 잘 생각해봐야함. 이때, 저번 시행에서 다음 시행을 갖다주는거ㅣㅇㅁ

marginal probability
invariant(or stationary)

## Simple Ergodic Theorem
### irreducible
어떤 상태에서 출발하더라도 임의의 상태에 도달해야함

### Aperiodic
어떤 상태에서 출발했을 때 특정 주기로만 돌아오지 않아야함

### Stationary Distribution(정상 분포)
Markov chain이 충분히 오래 반복하면 어느 특정 분포에 수렴하는 경우가 있음(이게 결국 시간에 불변하는 상태라는거같음)

# 챗쌤의 한마디

### **1. Monte Carlo Approximation이 뭔가요?**

Monte Carlo Approximation은 **확률적 샘플링을 이용해서 어떤 복잡한 값을 근사적으로 계산하는 방법**이야.  
특히, **적분을 풀거나 확률 분포의 기대값을 계산할 때** 유용해.

예를 들어보자.  
만약 네가 다음과 같은 **복잡한 적분을 풀어야 한다고 하자.**

I=∫abf(x)dxI = \int_a^b f(x) dx

이걸 해석적으로 풀 수 있으면 좋겠지만, 대부분의 경우 현실적으로 어려워.

Monte Carlo 방법은 이 적분을 **샘플링을 통해 근사적으로 계산하는 전략**이야.  
즉, 무작위 샘플을 뽑아서 적분값을 추정하는 거지.

---

### **2. 왜 Sampling을 하는가?**

Monte Carlo 방법의 핵심은 **"샘플을 뽑아서 계산하는 것이 복잡한 수식보다 더 효율적일 수 있다"**는 점이야.  
몇 가지 중요한 이유를 살펴보자.

#### **1) 복잡한 적분 문제 해결**

Monte Carlo는 고차원 적분에서도 강력해.  
예를 들어, 10차원 이상 되는 다중적분을 해석적으로 풀려면 매우 어렵지만, Monte Carlo 방법을 쓰면 **랜덤 샘플을 뽑아서 평균을 내는 방식으로 근사**할 수 있어.  
즉, **"샘플링만 잘 하면 고차원 문제도 해결 가능"**하다는 거야.

#### **2) 확률적 기대값 계산**

확률분포에서 기대값을 구하려면 보통 이렇게 적분을 해야 해.

E[f(X)]=∫f(x)p(x)dxE[f(X)] = \int f(x) p(x) dx

하지만 p(x)p(x) 가 복잡한 분포라면 직접 계산하기 어렵겠지?  
그래서 **X∼p(x)X \sim p(x) 에서 샘플을 여러 개 뽑아서 평균을 내면 기대값을 근사적으로 구할 수 있어**:

E[f(X)]≈1N∑i=1Nf(xi)E[f(X)] \approx \frac{1}{N} \sum_{i=1}^{N} f(x_i)

즉, 샘플을 뽑아서 평균을 구하는 것이 Monte Carlo Sampling의 핵심이야.

#### **3) 최적화 문제 해결**

Gradient Descent(경사 하강법)처럼 최적화 문제에서도 샘플링이 중요해.  
특히, **Reinforcement Learning(강화학습)에서는 에이전트가 환경에서 샘플링하면서 정책을 최적화**하지.  
Monte Carlo 방법을 이용해서 **미래 보상을 예측하는 것**도 샘플링을 기반으로 해.

---

### **3. Monte Carlo 샘플링의 핵심 아이디어**

Monte Carlo 방법에서 샘플링을 하는 이유는 결국, **복잡한 적분이나 확률적 계산을 효율적으로 풀기 위해서**야.

1. **랜덤 샘플을 뽑고**
2. **샘플의 평균을 내서 근사값을 만든다.**

이게 Monte Carlo Sampling의 기본 원리야.

---

### **4. 직관적인 예제**

직관적으로 이해하려면, **원 안에 점을 찍어서 원주율 π\pi 를 근사하는 Monte Carlo 방법**을 생각해 보자.

1. 1×1 크기의 정사각형 안에 반지름 1인 원이 들어있다고 하자.
2. 이 안에 무작위로 점을 찍는다.
3. 원 안에 들어간 점의 비율을 이용하면 π\pi 를 근사할 수 있다.

π≈4×원 안에 들어간 점 개수전체 점 개수\pi \approx 4 \times \frac{\text{원 안에 들어간 점 개수}}{\text{전체 점 개수}}

이처럼 복잡한 값을 쉽게 근사할 수 있기 때문에 샘플링을 활용하는 거야.

---

### **5. 결론**

- Monte Carlo 방법은 **복잡한 적분이나 확률적 기대값을 샘플링을 통해 근사하는 기법**이야.
- 샘플링을 하는 이유는 **수식적으로 해결하기 어려운 문제를 확률적으로 풀기 위해서**야.
- Monte Carlo 샘플링은 **최적화, 강화학습, 그래픽스, 금융 등 다양한 AI 분야에서 활용**되고 있어.

---

## **1. Monte Carlo Sampling의 기본 한계**

Monte Carlo 방법은 보통 **어떤 분포에서 샘플을 추출하여 근사값을 구하는 기법**인데,  
일반적인 **Uniform(균등) 샘플링**만으로는 현실적인 AI 문제를 해결하기 어려운 경우가 많아.

예를 들어, 우리가 **어떤 복잡한 확률 분포 p(x)p(x) 에서 샘플을 뽑아야 한다고 해보자.**  
그런데 직접 샘플링하기 어려운 경우가 많아.

이제, 이런 한계를 해결하기 위한 **Rejection Sampling**과 **Importance Sampling**을 차례대로 살펴볼게.

---

## **2. Rejection Sampling (거절 샘플링)**

### **📌 개념**

Rejection Sampling은 **직접 샘플링이 어려운 분포에서 샘플을 생성하는 기법**이야.  
**쉽게 샘플링할 수 있는 분포 q(x)q(x) 를 설정하고, 필요 없는 샘플을 거절하면서 p(x)p(x) 에 맞는 샘플을 만든다**는 아이디어야.

### **📌 왜 필요할까?**

- 우리가 원하는 분포 p(x)p(x) 가 복잡해서 직접 샘플링이 어려울 때 유용해.
- 대신, 샘플링이 쉬운 분포 q(x)q(x) (예: 정규 분포, 균등 분포)에서 샘플을 뽑고, 필요 없는 샘플을 걸러내는 방식.

### **📌 과정**

4. **쉽게 샘플링 가능한 분포 q(x)q(x) 에서 샘플 xx 를 하나 뽑는다.**
5. **확률적으로 샘플을 거절하거나 채택한다.**
    - p(x)p(x) 가 너무 복잡해서 직접 샘플링하기 어렵다면, q(x)q(x) 를 이용해 샘플을 만든 후
    - M⋅q(x)M \cdot q(x) (어떤 상수 MM 을 곱한 값)보다 p(x)p(x) 가 작으면 샘플을 버림 (rejection).
    - 확률적으로 유지할 샘플을 결정하면서 p(x)p(x) 에 가까운 샘플들을 유지함.

### **📌 직관적 예시**

- **다트 던지기 방식**으로 설명할 수 있어.
    - 원래 우리가 원하는 분포 p(x)p(x) 를 정확히 알지 못하지만,
    - q(x)q(x) 라는 비교적 단순한 분포를 이용해서 **많이 샘플을 던져놓고, 필요 없는 샘플을 거절하는 방식**.

---

## **3. Importance Sampling (중요도 샘플링)**

### **📌 개념**

Importance Sampling은 **Rejection Sampling보다 더 효율적인 방식으로 샘플을 조정하는 기법**이야.  
즉, 무작정 샘플을 거절하는 대신, **각 샘플의 기여도를 조정하여 계산하는 방식**이야.

### **📌 왜 필요할까?**

Rejection Sampling은 많은 샘플을 거절하기 때문에 **비효율적일 수 있어**.  
Importance Sampling은 **각 샘플의 중요도를 조정하여 보다 효율적인 근사값을 만들 수 있게 해**.

### **📌 과정**

6. 샘플링이 쉬운 분포 q(x)q(x) 에서 샘플을 뽑는다.
    
7. 샘플의 중요도를 **가중치(weight) w(x)w(x)** 로 조정한다.
    
    w(x)=p(x)q(x)w(x) = \frac{p(x)}{q(x)}
8. 가중치를 적용하여 기대값을 계산한다.
    
    Ep[f(x)]≈∑i=1Nw(xi)f(xi)E_p[f(x)] \approx \sum_{i=1}^{N} w(x_i) f(x_i)

즉, **q(x)q(x) 에서 샘플을 뽑되, 원래 우리가 원했던 분포 p(x)p(x) 와 비교하여 가중치를 부여함으로써 조정하는 방식**이야.

### **📌 직관적 예시**

- 예를 들어, **정확한 뉴스 기사 샘플을 모으고 싶다고 하자.**
    - p(x)p(x) 는 우리가 원하는 "신뢰도 높은 뉴스 분포"야.
    - 하지만 우리가 직접 그런 기사를 찾기 어렵다면, 대략적인 뉴스 사이트 q(x)q(x) 에서 샘플을 뽑아온 뒤,
    - 각 기사마다 신뢰도 점수를 부여하고 중요도가 높은 기사만 남기는 방식.

---

## **4. AI 분야에서의 활용**

Rejection Sampling과 Importance Sampling은 AI에서 많이 활용되는데, 대표적인 적용 사례를 몇 가지 소개할게.

### **📌 1) 강화학습 (Reinforcement Learning)**

- 강화학습에서는 **미래 보상을 추정할 때 샘플링이 중요한데**, 여기서 Importance Sampling이 활용돼.
- 예를 들어, 강화학습에서 정책을 업데이트할 때 **다른 분포에서 샘플을 뽑아 보상을 추정할 필요가 있어**.
- 일반적인 방법으로는 학습 속도가 느려지지만, Importance Sampling을 활용하면 **더 좋은 행동을 더 높은 가중치로 학습할 수 있어**.

### **📌 2) Variational Inference (변분 추론)**

- **베이지안 확률 모델에서 직접 샘플링이 어려운 경우**, Importance Sampling을 이용해 근사적으로 샘플을 뽑을 수 있어.
- 특히, **딥러닝에서 베이지안 네트워크를 학습할 때** 사용돼.

### **📌 3) Monte Carlo Tree Search (MCTS)**

- AI가 **체스, 바둑 같은 게임을 할 때 최적의 수를 찾는 방법**이야.
- 여기서 Importance Sampling을 활용하면 **게임의 특정 시뮬레이션을 더 현실적인 가중치로 조정**할 수 있어.

### **📌 4) Generative Models (생성 모델)**

- GAN(생성적 적대 신경망)이나 VAE(변분 오토인코더)에서 샘플링 기법이 많이 사용돼.
- 특히, **분포 변환을 할 때 Importance Sampling을 적용하면 더 현실적인 샘플을 만들 수 있어**.

---

## **5. 결론**

- **Rejection Sampling**: 쉽게 샘플링할 수 있는 분포에서 샘플을 만들고, 필요 없는 샘플을 거절하면서 우리가 원하는 분포를 근사하는 방식.
- **Importance Sampling**: 샘플을 직접 거절하는 대신, **각 샘플의 중요도를 가중치로 조정하여 원하는 분포에 맞추는 방식**.
- **AI에서 활용**: 강화학습, 베이지안 추론, MCTS, 생성 모델 등 다양한 분야에서 사용됨.

---

# 예전에 적얻ㄴ거

## 기본 원리

어떤 함수 $f(x)$의 기댓값 $E[f(x)]$를 계산하고자 할 때, x의 분포 $p(x)$에서 N개의 샘플을 추출하여 근사할 수 있음:

$$ E[f(x)] \approx \frac{1}{N} \sum_{i=1}^N f(x_i), \quad x_i \sim p(x) $$

이 근사의 정확도는 대수의 법칙(Law of Large Numbers)에 의해 보장되며, 중심극한정리(Central Limit Theorem)에 의해 오차의 분포를 알 수 있음.

## 분산 감소 기법

Monte Carlo 추정의 분산을 줄이기 위한 여러 기법이 있음:

9. 대조 변량(Control Variates): 

   $$ \hat{\mu} = \frac{1}{N} \sum_{i=1}^N f(x_i) - c(\frac{1}{N} \sum_{i=1}^N g(x_i) - E[g(x)]) $$
   
   여기서 $g(x)$는 $E[g(x)]$를 알고 있는 함수임.

10. 층화 샘플링(Stratified Sampling):
   전체 영역을 여러 층으로 나누고, 각 층에서 독립적으로 샘플링함.

11. 상관 샘플링(Correlated Sampling):
   유사한 문제들 간에 동일한 난수 시퀀스를 사용하여 분산을 줄임.

# Basic Sampling Algorithms

## 역변환 샘플링 (Inverse Transform Sampling)

연속 확률 변수 X의 누적 분포 함수(CDF) $F(x)$가 주어졌을 때:

12. U ~ Uniform(0, 1)에서 샘플 u를 추출
13. $x = F^{-1}(u)$를 계산

이렇게 얻은 x는 원하는 분포를 따름.

## 박스-뮬러 변환 (Box-Muller Transform)

표준 정규 분포 N(0, 1)에서 샘플을 생성하는 방법:

14. U1, U2 ~ Uniform(0, 1) 추출
15. $R = \sqrt{-2 \ln{U1}}, θ = 2π U2$
16. $X = R \cosθ, Y = R \sinθ$

X와 Y는 독립적인 표준 정규 분포를 따름.

# Rejection Sampling

목표 분포 p(x)에서 직접 샘플링하기 어려울 때 사용하는 방법임.

## 알고리즘

17. 제안 분포 q(x)와 상수 M을 선택 (M ≥ max(p(x)/q(x)))
18. q(x)에서 샘플 x를 추출
19. Uniform(0, Mq(x))에서 u를 추출
20. u ≤ p(x)이면 x를 받아들이고, 그렇지 않으면 거부
21. 2-4 단계를 원하는 수의 샘플을 얻을 때까지 반복

## 수학적 정당성

제안된 x가 받아들여질 확률:

$$ P(\text{accept}) = \int \frac{p(x)}{Mq(x)} q(x) dx = \frac{1}{M} $$

따라서 받아들여진 샘플의 분포:

$$ p(\text{accepted } x) = \frac{p(x)}{MP(\text{accept})} = p(x) $$

## 효율성

효율성은 1/M으로, M이 클수록 거부율이 높아짐. 따라서 p(x)와 q(x)가 유사할수록 효율적임.

# Importance Sampling

직접 샘플링이 어려운 분포 p(x)에서 기댓값을 계산하기 위한 방법임.

## 기본 원리

$$ E_{p(x)}[f(x)] = \int f(x)p(x)dx = \int f(x)\frac{p(x)}{q(x)}q(x)dx = E_{q(x)}[f(x)\frac{p(x)}{q(x)}] $$

여기서 q(x)는 제안 분포임.

## 알고리즘

22. 제안 분포 q(x)에서 N개의 샘플 {x_i}를 추출
23. 각 샘플에 대해 중요도 가중치 w_i = p(x_i)/q(x_i) 계산
24. 기댓값을 다음과 같이 추정:
   $$ \hat{E}[f(x)] = \frac{\displaystyle\sum_{i=1}^N w_i f(x_i)}{\displaystyle\sum_{i=1}^N w_i} $$

## 분산 감소

Self-normalized Importance Sampling:

$$ \hat{E}[f(x)] = \frac{\displaystyle\sum_{i=1}^N w_i f(x_i)}{\displaystyle\sum_{i=1}^N w_i}, \quad w_i = \frac{p(x_i)}{q(x_i)} $$

이 방법은 정규화 상수를 모르는 경우에도 사용 가능함.

## 효과적인 제안 분포 선택

이상적인 제안 분포: q*(x) ∝ |f(x)|p(x)
실제로는 p(x)와 |f(x)|의 곱과 유사한 분포를 선택하는 것이 좋음.

# 생성 모델에서의 응용

25. VAE (Variational Autoencoder):
   - 잠재 변수 z의 사후 분포 p(z|x)를 근사하기 위해 중요도 가중치 샘플링 사용
   - ELBO의 Monte Carlo 추정:
     $$ \mathcal{L} \approx \frac{1}{N} \sum_{i=1}^N [\log p_\theta(x|z_i) + \log p(z_i) - \log q_\phi(z_i|x)] $$
     여기서 z_i ~ q_φ(z|x)

26. MCMC (Markov Chain Monte Carlo):
   - 복잡한 사후 분포에서 샘플링하기 위해 사용
   - Metropolis-Hastings 알고리즘은 rejection sampling의 일종

27. 파티클 필터 (Particle Filter):
   - 시계열 데이터의 상태 추정에 importance sampling 사용
   - 각 파티클에 중요도 가중치 할당

28. GAN (Generative Adversarial Network):
   - 판별자 학습 시 중요도 샘플링을 통해 효율성 향상 가능

29. 베이지안 신경망:
   - 가중치의 사후 분포에서 샘플링하여 예측 불확실성 추정

# Markov Chain Monte Carlo (MCMC)

MCMC는 복잡한 확률 분포에서 샘플을 생성하는 알고리즘 계열임. 특히 고차원 분포나 정규화 상수를 모르는 분포에서 유용함.

## 기본 원리

30. 목표 분포 π(x)에 수렴하는 마르코프 체인을 구성함
31. 이 체인을 충분히 긴 시간 동안 실행하여 샘플을 얻음

## 이론적 기반

32. 마르코프 체인의 정상 분포 (Stationary Distribution):
   전이 확률 P에 대해, π(x)P(x|y) = π(y)P(y|x) 를 만족하는 π(x)

33. 에르고딕성 (Ergodicity):
   충분히 긴 시간이 지나면 초기 상태와 무관하게 정상 분포에 수렴

34. 상세 균형 조건 (Detailed Balance):
   π(x)P(y|x) = π(y)P(x|y) 를 만족하면 π(x)가 정상 분포임

## Metropolis-Hastings 알고리즘

MCMC의 대표적인 알고리즘 중 하나임.

35. 현재 상태 x에서 제안 분포 q(y|x)를 사용해 후보 상태 y를 생성
36. 수용 확률 α를 계산:
   $$ α = min(1, \frac{π(y)q(x|y)}{π(x)q(y|x)}) $$
37. α의 확률로 y를 수용하고, (1-α)의 확률로 x를 유지

이 과정을 반복하여 π(x)에서의 샘플을 얻음.

## 수렴 진단

38. Gelman-Rubin 통계량: 여러 체인 간의 분산을 비교
39. 자기상관 분석: 샘플 간의 독립성 확인
40. Trace plot: 샘플의 시계열 플롯을 통한 시각적 검사

# Gibbs Sampling

Gibbs Sampling은 MCMC의 특별한 경우로, 다변량 분포에서 샘플링할 때 유용함.

## 기본 원리

각 변수를 다른 모든 변수가 주어졌을 때의 조건부 분포에서 순차적으로 샘플링함.

## 알고리즘

$x = (x_1, ..., x_n)$인 n차원 확률 변수에 대해:

41. 초기값 $x^0 = ({x_1}^0, ..., {x_n}^0)$ 선택
42. 각 반복 t에 대해:
   - ${x_1}^t \sim p(x_1|{x_2}^{t-1}, ..., {x_n}^{t-1})$
   - ${x_2}^t \sim p(x_2|{x_1}^{t},{x_3}^{t-1}, ..., {x_n}^{t-1})$
   - ...
   - ${x_n}^t \sim p(x_n|{x_1}^{t}, ..., {x_{n-1}}^{t})$

## 수학적 정당성

Gibbs Sampling은 다음의 전이 커널을 가진 Metropolis-Hastings 알고리즘의 특수한 경우로 볼 수 있음:

$$ T(x'|x) = p(x'_i|x_{-i}) $$

여기서 x_{-i}는 x_i를 제외한 모든 변수를 의미함. 이 전이 커널은 항상 수용되므로 (α = 1) Metropolis-Hastings 단계가 필요 없음.

## 수렴 특성

43. 에르고딕성: 모든 조건부 분포가 양의 확률을 가지면 에르고딕함
44. 수렴 속도: 변수 간 상관관계가 높을 경우 수렴이 느릴 수 있음
45. 차원의 저주: 고차원에서는 수렴이 느려질 수 있음

## Collapsed Gibbs Sampling

일부 변수를 분석적으로 적분하여 제거한 후 나머지 변수에 대해 Gibbs Sampling을 수행하는 방법. 수렴 속도를 향상시킬 수 있음.

# 생성 모델에서의 응용

46. LDA (Latent Dirichlet Allocation):
   - 토픽 모델링에서 Collapsed Gibbs Sampling 사용
   - 문서-단어 할당을 샘플링:
     $$ p(z_i = k | z_{-i}, w) ∝ (n_{d_i,k,-i} + α_k) · \frac{n_{k,w_i,-i} + β}{n_{k,-i} + Wβ} $$
     여기서 n_{d_i,k,-i}는 현재 단어를 제외한 문서 d_i의 토픽 k 할당 수, n_{k,w_i,-i}는 단어 w_i의 토픽 k 할당 수

47. 베이지안 신경망:
   - 가중치의 사후 분포에서 샘플링:
     $$ p(w_i | w_{-i}, D) ∝ p(D|w)p(w_i|w_{-i}) $$

48. 은닉 마르코프 모델 (HMM):
   - Forward-Backward 알고리즘과 결합하여 상태 시퀀스 샘플링

49. 베이지안 가우시안 혼합 모델:
   - 클러스터 할당과 파라미터를 번갈아가며 샘플링:
     $$ p(z_i = k | z_{-i}, x, μ, Σ) ∝ π_k N(x_i | μ_k, Σ_k) $$
     $$ p(μ_k, Σ_k | z, x) ∝ N(μ_k | m_k, (κ_k Λ_k)^{-1}) W(Λ_k | W_k, ν_k) $$

50. 조건부 임의장 (Conditional Random Fields):
   - 구조화된 예측 문제에서 MCMC를 사용하여 레이블 시퀀스 샘플링
