1. Position Embedding

- Absolute vs Relative position embedding의 차이
- Sinusoidal position embedding의 수학적 원리
- Position embedding이 필요한 이유와 구현 방식
- Positional encoding의 주기성이 모델에 미치는 영향

1. Attention Mechanism

- Scaled dot-product attention의 수학적 원리
- Self-attention vs Cross-attention
- Multi-head attention의 장점과 구현 방식
- Attention score 계산 과정과 softmax의 역할

1. 구현 레벨

- PyTorch로 transformer block 구현
- Attention mask 구현 방식
- 서브 컴포넌트(LayerNorm, Feed Forward Network 등) 구현