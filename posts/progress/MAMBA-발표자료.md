# 1번 슬라이드
안녕하세요, MAMBA 아키텍쳐란 무엇인가 발표를 맡은 RecSys 8조 수학과 김영찬입니다. 자료 조사에 같은 조원 신승훈, 박광진 학우, 발표 준비 도움에 신승훈 학우께서 도와주셨습니다.

# 2번 슬라이드
목차입니다. 우선, 발표 시간상 원래 계획했던 제목인 '머신러닝 및 딥러닝의 계보' 부분을, 시계열 데이터에서만 살펴보는 것으로 대폭 축소하였습니다. 이 점 양해 부탁드립니다.
1 머신러닝 연대기
2 시계열 데이터의 접근 방식에서 두 모델을 살펴보고
3 MAMBA를 이해하기 위한 backbone인 ssm에 대해 공부하고
4 MAMBA가 그래서 뭔지 살펴보겠습니다.

# 4번 슬라이드
태초의 머신러닝은 통계적 모델에서 시작합니다.

Linear Regression : 가우스, 르장드르가 선형회귀 방식을 제안함

천문학 계산에 도구로 쓰려고 개발했는데, 이게 1950년에 머신러닝맥락에 들어옵니다.

K-NN : 1951 미 공군 항공 의학 연구소에서 작성됨. 군 기밀이라 논문 나온 시점에서 이거 읽을 수 없었음. 1967년에서야 머신러닝에서 핫해지기 시작함.

Naïve Bayes : 왜 나이브하냐? – 독립가정이 비현실적이라는 비판적인 의미임

18세기 수학인데 머신러닝으로 1970년대에 들어옴



# 5번 슬라이드
1980년대로 넘어오면, 본격적으로 머신러닝 모델들이 여러 가지 개발되게 됩니다. SVM은 1960년대 후반 소련에서 이론적 기초를 개발.

1990년대 초 미국으로 이주한 Vapnik이 SVM을 완성하고 널리 보급.

이외에도, 우리가 흔히 아는 모델들이 다 이 시기에 개발되었습니다.

--
>
>원래는 핵심 연구가 냉전 시절 소련에서 진행되었지만, 이후 서구권에서 발전.
Boser, B.E., Guyon, I.M., & Vapnik, V.N. "A Training Algorithm for Optimal Margin Classifiers.“에서 이론적 기초가 나와서 1992 써둠
"SVM은 1992년 Vapnik과 동료들이 처음 제안한 후, 1995년 Cortes와 Vapnik의 논문을 통해 대중적으로 확산되었습니다."

# 6번 슬라이드
발표용비화)

•AlexNet : Geoffrey Hinton의 제자 Alex Krizhevsky가 학위 연구로 개발.

•당시 NVIDIA의 GPU를 적극 활용하여 ImageNet 대회에서 압도적 우승.

•학위 프로젝트가 딥러닝의 상징적 사건으로 자리잡음.


# 7번 슬라이드
(발표용) 2019년 이후로 모델이 없는 이유는, 생성형으로 트렌드가 넘어갔기 때문이 아닐지

Transformer의 등장 이후로, 모든 이가 Attention에 매료됨. 그래프에 도입해보고, 양방향으로 써보고 등등..
그 중에서 LightGCN, 재밌지요. 딥러닝의 세계에서 머신러닝으로 한 획을 그은, 이 시대의 마지막 낭만입니다. 넘어가곘습니다

# 9번 슬라이드
시계열 데이터 처리의 조상님 RNN과 가장 핫한 Transformer의 장단점을 한번 살펴보도록 하겠습니다.




# 13번 슬라이드
저주의 삼각형이죠. RNN의 장기 기억 의존 문제를 해결하고 성능을 높인 트랜스포머는 연산량이라는 벽에 부딪혔습니다.
Transformer : 
train : O(n^2)
추론 : O(n)
으로 오래 걸림. 따라서, 최근 연구에서 경량화, 최적화 등 이번강의에서 배운 것들을 만들어냈음

RNN
train : O(n)
추론 : O(n)

LLM 분야에서, 장기 기억 문제를 해결하려 압축하려다보니 정보 손실이 있음. 이때문에 성능이 낮음.
- 따라서, 연구 동향 분야는 Hidden State를 어떻게든 Finite수준으로 끌어내려야 한다.

-> 그렇다면, RNN과 Transformer의 장점을 섞어서 만들면 좋지 않을까?를 연구자들이 연구하다가
RNN의 장점(빠른 연산)을 활용하기 위해 1960년대부터 연구된 제어 이론의 State Space equation을 빌려와서 state space model이라는 개념을 사용하게 됩니다. 
이때, Transformer의 장점인 문맥추론을 하기 위해, input 중 필요한 부분만 취사선택하는, attention과 매우 유사한 기능을 넣게 되기까지가 뒤에 나올 내용입니다. 이를 차근차근 살펴보도록 하겠습니다.

# 14번 슬라이드
우선, RNN의 단점은 장기 기억 문제였죠? 이걸 해결하기 위해 연구한 결과, 수학적으로 '시간에 따른 정보의 중요도'를 모델링하게 됩니다. 모델링 연산이 용이하도록 하기 위해 왼쪽과 같은 임의의 데이터를 오른쪽과 같이 다항함수 꼴로 projection 합니다.(테일러 근사죠?)
계산이 용이해진 데이터에 시간에 따른 중요도를 곱하여 데이터를 분석하는 방법에 대해 연구한 것이 오른쪽에 있는 하마 히포입니다.

하지만, 단점이 있습니다. 우선, high-order polynomial이다 보니까, 뭐 잘못 건드리면 데이터가 확 튀겠죠?(차수가 높으니)
또한, 그림에서 데이터는 연속함수잖아요? 하지만, 컴퓨터 세계는 이산공간이죠? 이 차이 때문에, 히포를 잘 사용하기란 매우 까다롭습니다. 따라서, 이를 이산공간에 끌어오도록 하는 것이 필요하겠죠?

# 15번 슬라이드
그래서 나온 개념이 LSSL입니다. 여기서 이제 상태 공간 모델, 즉 State Space Model이 나오게 됩니다. 이 내용에 대해 한번 알아보겠습니다.

# 16번 슬라이드
상태 공간 방정식, 앞서 설명드렸듯 제어 이론에서 나온 개념입니다.
x를 직접 관측하기 어려울 때, 관측 가능한 데이터 y와 입력 u에 대한 식을 세워 x를 추론하는 과정입니다. 예시를 들겠습니다.
가령, 자동차의 위치 x에요. 이는 GPS라는 정보로 어떻게 관측 가능하긴 한데, 이 GPS도 관측이 부정확할 수 있습니다.  완벽한 위치를 구하기 위해, y와 input에 어떠한 값을 대입하여, x값을 추론해내는 과정이라고 보시면 됩니다.

이때, 이 개념을 차용하되, 컴퓨터는 이산공간이므로 식은 이렇게 변할 수 있습니다. 이때, 오른쪽 그림 어디선가 본 적 있지 않나요?


# 17번 슬라이드
그렇습니다, 바로 RNN과 비슷하게 생겼어요. 인풋 시퀀스 0, 1, 2를 대입하여 아웃풋을 뽑아내는 과정이 꽤 비슷하게 생겼습니다. 하지만, 여기에는 중요한 차이점이 하나 있습니다. RNN의 경우, W값이 앞서 말했듯이 각 단계별로 update됩니다. 따라서 병렬 계산을 실행할 수 없죠


# 18번 슬라이드
하지만, SSM의 경우, 식 6과 같이 각 단계에서 A 행렬이 변하지 않습니다. 따라서, 계산이 일종의 다항식 계산처럼 되기 때문에, 병렬 계산 가능하고, 계산 자체도 쉬워집니다.

하지만, 여기에도 여전히 함정이 있습니다. A는 행렬이잖아요? 행렬의 곱이므로, LSSL도 여전히 메모리 문제, 계산량 문제가 있습니다. 이를 해결하기 위해 나온 모델이 바로

# 19번 슬라이드
S4, Structured State Spaces for Sequence Modeling 입니다.
여기서 짚고 갈 부분은 크게 두 가지인데요,
첫번째로, 앞서 보여드린 HiPPO의 개념을 다시 가져옵니다. HiPPO matrix는 이렇게 생겼는데요, 이와 같이 시간이 k보다 '앞'이면 선형보다 조금 느리게 증가하는 식으로 행렬 A를 초기화합니다. 왜 한다고 했죠 앞에서? 시간에 따른 정보 중요도를 모델링하기 위함이다~


두번째로, 상태 전이 행렬 A를 대각화합니다. 행렬을 대각화하면 아무래도 연산 시간이 획기적으로 줄어들겠죠? 하지만 행렬 대각화는 항상 되는게 아니죠? 따라서, 대수학의 기본정리를 사용하러 복소수 차원으로 가서 이걸 해냅니다. 실수계수 n차 다항식은 항상 복소수 해가 n개 있으니까요. 저는 진짜 이거 보고 울었어요 감동이라, 궁금하시면 한번 찾아보세요
이 두 개념을 섞기 위해 NPLR을 사용하고 하는데, 시간 관계상 생략하겠습니다.

-- 이 개념 : 연산 간단한 행렬로 쪼개가지고 따로 계산해버리는 스킬임

# 20번 슬라이드
이제 SSM의 단점을 한번 살펴보겠습니다. 앞서 식 6번 기억나시죠? 이처럼, 모든 시퀀스의 시점 0, 1, .. k에 대해서 모두 같은 행렬 A를 곱해요. 이걸 시간 불변성이라고 하는데, 
이러면, k번째 output을 도출할 때 k-1번까지 모든 데이터를 사용하겠죠? 또한, 모두 같은 행렬을 곱해주기 때문에, 아무래도 유연성이 떨어지겠죠?
그렇다면, 여기서 input중 몇개를 취사선택하는 기술이 만약 접목된다면, 어텐션 구조랑 매우 비슷해지겠죠?

# 21번 슬라이드
그렇게, 각 시간별로 행렬을 조금 다르게 주는 것이 Selective의 개념입니다.  이러면 RNN과 다를게 없지 않나? 싶지만, 여기에 좀 트릭이 있습니다.

# 22번 슬라이드
아무리 생각해도 이게 최선이라, 제 필기로 대체하겠습니다.
State space equation에서, selective를 주는 것은 결국 delta를 추가하는 것입니다.
이때, 아래 1번과 같이 이 ODE의 일반해는 exp 꼴이라 연산량이 n^3 정도이나, 이게 만약 대각행렬이면 연산량이 n으로 확 떨어지겠죠? 
어렵지만, 결국 핵심은 대각입니다. 

# 23번 슬라이드
앞서 S4에서 A를 대각행렬로 만드는 것을 살펴봤습니다. 연속된 알고리즘(앞서 본 미분방정식)을 이산화시키는 과정에서 ZOH를 사용하면, delta는 각 시간별 차이가 되기 때문에, 상수 꼴로 떨어지는 대각행렬이 만들어집니다. 대각화가능행렬 A와 대각행렬 delta를 설정하여, 연산을 n으로 줄여버린 거죠. 


# 24번 슬라이드

```
기존 모델의 경우, dram, sram를 왕복하며 메모리 복사가 일어나기 때문에, 시간이 걸림. 맘바에서는, 중요한 상태를 빠른 메모리에 유지하기 위해 커널 퓨전을 사용하여 빠르게 돕니다. 이를 스캔 알고리즘이라 한대요

정리하자면, Selective Space State Model을 구현하는 과정에서 단순히 수학적·알고리즘적 접근뿐 아니라, DRAM·SRAM 등 하드웨어 구조를 함께 고려해야 큰 성능 개선을 얻을 수 있습니다. MAMBA는 바로 이러한 하드웨어 인식(Hardware-aware) 설계를 토대로 높은 계산 효율을 달성하는 모듈입니다
```

# 25번 슬라이드
그래서, 결론은 맘바가 이렇게 생겼다!
저희가 지금까지 본 Selective State Space Model은, 이 구조의 하나의 뇌로써 작동하는 것이죠. 아름답지 않습니까?
이 뇌에 Projection, Convolution, Activation을 하나로 통합시킨 모듈입니다.

# 26번 슬라이드
